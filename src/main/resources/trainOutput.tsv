true	1611.06624v3.pdf	Video Generation; UCF-101 16 frames, Unconditional, Single GPU; Inception Score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Video Generation; UCF-101 16 frames, 64x64, Unconditional; Inception Score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Video Generation; UCF-101 16 frames, Unconditional, Single GPU; Inception Score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Video Generation; UCF-101 16 frames, 64x64, Unconditional; Inception Score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Question Answering; YahooCQA; P@1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Few-Shot Image Classification; Caltech-256 5-way (1-shot); Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Atari Games; Atari 2600 Private Eye; Score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Semi-Supervised Video Object Segmentation; YouTube; mIoU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Skeleton Based Action Recognition; JHMDB Pose Tracking; PCK@0.1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Skeleton Based Action Recognition; JHMDB Pose Tracking; PCK@0.3	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Skeleton Based Action Recognition; JHMDB Pose Tracking; PCK@0.2	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Skeleton Based Action Recognition; JHMDB Pose Tracking; PCK@0.5	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Skeleton Based Action Recognition; JHMDB Pose Tracking; PCK@0.4	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Keypoint Detection; COCO test-challenge; ARL	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Keypoint Detection; COCO test-challenge; ARM	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Cross-Lingual Document Classification; MLDoc Zero-Shot English-to-Chinese; Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Semi-Supervised Video Object Segmentation; DAVIS 2017 (test-dev); Jaccard (Decay)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Question Answering; WikiQA; MRR	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Fine-Grained Image Classification; FGVC Aircraft; FLOPS	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Real-time Instance Segmentation; MSCOCO; Frame (fps)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Image-to-Image Translation; Cityscapes Labels-to-Photo; FID	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Neural Architecture Search; ImageNet; Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Few-Shot Image Classification; FC100 5-way (5-shot); Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Atari Games; Atari 2600 Pooyan; Score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Cross-Lingual NER; CoNLL Dutch; F1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Activity Prediction; ActEV; mAP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Video Super-Resolution; Ultra Video Group HD - 4x upscaling; Average PSNR	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Human Pose Forecasting; Human3.6M; MAR, walking, 400ms	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Face Detection; WIDER Face (Medium); AP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Scene Text Detection; Total-Text; Precision	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Link Prediction; YAGO3-10; Hits@1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Heterogeneous Node Classification; DBLP (PACT) 14k; Macro-F1 (60% training data)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Word Sense Disambiguation; SemEval 2007 Task 17; F1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Neural Architecture Search; CIFAR-10; Search Time (GPU days)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	3D Object Detection; KITTI Pedestrians Hard; AP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Few-Shot Image Classification; CUB-200-2011 - 0-Shot; Top-1 Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Neural Architecture Search; Oxford 102 Flowers; PARAMS	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Visual Dialog; VisDial v0.9 val; R@10	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Image Clustering; Tiny-ImageNet; Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Skeleton Based Action Recognition; SHREC 2017 track on 3D Hand Gesture Recognition; Speed  (FPS)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Link Prediction; WN18RR; MR	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Visual Question Answering; VQA-CP; Score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Node Classification; USA Air-Traffic; Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Table-to-Text Generation; WikiBio; BLEU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Multi-Label Text Classification; EUR-Lex; P@5	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Action Segmentation; GTEA; Edit	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Weakly Supervised Action Localization; ActivityNet-1.3; mAP@0.5	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Skeleton Based Action Recognition; Florence 3D; Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Question Answering; WikiHop; Test	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Panoptic Segmentation; Cityscapes test; PQ	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Colorectal Gland Segmentation:; CRAG; Dice	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Keypoint Detection; COCO test-challenge; APL	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Image Super-Resolution; Set14 - 4x upscaling; MOS	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Semantic Segmentation; NYU Depth v2; Mean IoU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1910.10750v1.pdf	unknow	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Question Answering; YahooCQA; P@1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Few-Shot Image Classification; Caltech-256 5-way (1-shot); Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Atari Games; Atari 2600 Private Eye; Score	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Semi-Supervised Video Object Segmentation; YouTube; mIoU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Skeleton Based Action Recognition; JHMDB Pose Tracking; PCK@0.1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Skeleton Based Action Recognition; JHMDB Pose Tracking; PCK@0.3	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Skeleton Based Action Recognition; JHMDB Pose Tracking; PCK@0.2	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Skeleton Based Action Recognition; JHMDB Pose Tracking; PCK@0.5	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Skeleton Based Action Recognition; JHMDB Pose Tracking; PCK@0.4	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Keypoint Detection; COCO test-challenge; ARL	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Keypoint Detection; COCO test-challenge; ARM	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Cross-Lingual Document Classification; MLDoc Zero-Shot English-to-Chinese; Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Semi-Supervised Video Object Segmentation; DAVIS 2017 (test-dev); Jaccard (Decay)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Question Answering; WikiQA; MRR	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Fine-Grained Image Classification; FGVC Aircraft; FLOPS	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Real-time Instance Segmentation; MSCOCO; Frame (fps)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Image-to-Image Translation; Cityscapes Labels-to-Photo; FID	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Neural Architecture Search; ImageNet; Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Few-Shot Image Classification; FC100 5-way (5-shot); Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Atari Games; Atari 2600 Pooyan; Score	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Cross-Lingual NER; CoNLL Dutch; F1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Activity Prediction; ActEV; mAP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Video Super-Resolution; Ultra Video Group HD - 4x upscaling; Average PSNR	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Human Pose Forecasting; Human3.6M; MAR, walking, 400ms	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Face Detection; WIDER Face (Medium); AP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Scene Text Detection; Total-Text; Precision	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Link Prediction; YAGO3-10; Hits@1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Heterogeneous Node Classification; DBLP (PACT) 14k; Macro-F1 (60% training data)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Word Sense Disambiguation; SemEval 2007 Task 17; F1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Neural Architecture Search; CIFAR-10; Search Time (GPU days)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	3D Object Detection; KITTI Pedestrians Hard; AP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Few-Shot Image Classification; CUB-200-2011 - 0-Shot; Top-1 Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Neural Architecture Search; Oxford 102 Flowers; PARAMS	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Visual Dialog; VisDial v0.9 val; R@10	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Image Clustering; Tiny-ImageNet; Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Skeleton Based Action Recognition; SHREC 2017 track on 3D Hand Gesture Recognition; Speed  (FPS)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Link Prediction; WN18RR; MR	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Visual Question Answering; VQA-CP; Score	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Node Classification; USA Air-Traffic; Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Table-to-Text Generation; WikiBio; BLEU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Multi-Label Text Classification; EUR-Lex; P@5	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Action Segmentation; GTEA; Edit	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Weakly Supervised Action Localization; ActivityNet-1.3; mAP@0.5	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Skeleton Based Action Recognition; Florence 3D; Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Question Answering; WikiHop; Test	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Panoptic Segmentation; Cityscapes test; PQ	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Colorectal Gland Segmentation:; CRAG; Dice	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Keypoint Detection; COCO test-challenge; APL	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Image Super-Resolution; Set14 - 4x upscaling; MOS	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Semantic Segmentation; NYU Depth v2; Mean IoU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation; DAVIS 2016; F-measure (Decay)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation; DAVIS 2016; F-measure (Mean)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation; DAVIS 2016; F-measure (Recall)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation; DAVIS 2016; J&F	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation; DAVIS 2016; Jaccard (Decay)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation; DAVIS 2016; Jaccard (Mean)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation; DAVIS 2016; Jaccard (Recall)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation; DAVIS 2016; F-measure (Decay)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation; DAVIS 2016; F-measure (Mean)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation; DAVIS 2016; F-measure (Recall)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation; DAVIS 2016; J&F	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation; DAVIS 2016; Jaccard (Decay)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation; DAVIS 2016; Jaccard (Mean)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation; DAVIS 2016; Jaccard (Recall)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation; DAVIS 2016; F-measure (Decay)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation; DAVIS 2016; F-measure (Mean)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation; DAVIS 2016; F-measure (Recall)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation; DAVIS 2016; J&F	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation; DAVIS 2016; Jaccard (Decay)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation; DAVIS 2016; Jaccard (Mean)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation; DAVIS 2016; Jaccard (Recall)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Question Answering; YahooCQA; P@1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Few-Shot Image Classification; Caltech-256 5-way (1-shot); Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Atari Games; Atari 2600 Private Eye; Score	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation; YouTube; mIoU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Skeleton Based Action Recognition; JHMDB Pose Tracking; PCK@0.1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Skeleton Based Action Recognition; JHMDB Pose Tracking; PCK@0.3	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Skeleton Based Action Recognition; JHMDB Pose Tracking; PCK@0.2	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Skeleton Based Action Recognition; JHMDB Pose Tracking; PCK@0.5	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Skeleton Based Action Recognition; JHMDB Pose Tracking; PCK@0.4	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Keypoint Detection; COCO test-challenge; ARL	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Keypoint Detection; COCO test-challenge; ARM	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Cross-Lingual Document Classification; MLDoc Zero-Shot English-to-Chinese; Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation; DAVIS 2017 (test-dev); Jaccard (Decay)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Question Answering; WikiQA; MRR	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Fine-Grained Image Classification; FGVC Aircraft; FLOPS	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Real-time Instance Segmentation; MSCOCO; Frame (fps)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Image-to-Image Translation; Cityscapes Labels-to-Photo; FID	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Neural Architecture Search; ImageNet; Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Few-Shot Image Classification; FC100 5-way (5-shot); Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Atari Games; Atari 2600 Pooyan; Score	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Cross-Lingual NER; CoNLL Dutch; F1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Activity Prediction; ActEV; mAP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Video Super-Resolution; Ultra Video Group HD - 4x upscaling; Average PSNR	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Human Pose Forecasting; Human3.6M; MAR, walking, 400ms	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Face Detection; WIDER Face (Medium); AP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Scene Text Detection; Total-Text; Precision	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Link Prediction; YAGO3-10; Hits@1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Heterogeneous Node Classification; DBLP (PACT) 14k; Macro-F1 (60% training data)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Word Sense Disambiguation; SemEval 2007 Task 17; F1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Neural Architecture Search; CIFAR-10; Search Time (GPU days)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	3D Object Detection; KITTI Pedestrians Hard; AP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Few-Shot Image Classification; CUB-200-2011 - 0-Shot; Top-1 Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Neural Architecture Search; Oxford 102 Flowers; PARAMS	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Visual Dialog; VisDial v0.9 val; R@10	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Image Clustering; Tiny-ImageNet; Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Skeleton Based Action Recognition; SHREC 2017 track on 3D Hand Gesture Recognition; Speed  (FPS)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Link Prediction; WN18RR; MR	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Visual Question Answering; VQA-CP; Score	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Node Classification; USA Air-Traffic; Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Table-to-Text Generation; WikiBio; BLEU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Multi-Label Text Classification; EUR-Lex; P@5	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Action Segmentation; GTEA; Edit	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Weakly Supervised Action Localization; ActivityNet-1.3; mAP@0.5	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Skeleton Based Action Recognition; Florence 3D; Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Question Answering; WikiHop; Test	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Panoptic Segmentation; Cityscapes test; PQ	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Colorectal Gland Segmentation:; CRAG; Dice	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Keypoint Detection; COCO test-challenge; APL	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Image Super-Resolution; Set14 - 4x upscaling; MOS	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Semantic Segmentation; NYU Depth v2; Mean IoU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2005.12661v1.pdf	unknow	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Question Answering; YahooCQA; P@1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Few-Shot Image Classification; Caltech-256 5-way (1-shot); Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Atari Games; Atari 2600 Private Eye; Score	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Semi-Supervised Video Object Segmentation; YouTube; mIoU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Skeleton Based Action Recognition; JHMDB Pose Tracking; PCK@0.1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Skeleton Based Action Recognition; JHMDB Pose Tracking; PCK@0.3	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Skeleton Based Action Recognition; JHMDB Pose Tracking; PCK@0.2	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Skeleton Based Action Recognition; JHMDB Pose Tracking; PCK@0.5	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Skeleton Based Action Recognition; JHMDB Pose Tracking; PCK@0.4	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Keypoint Detection; COCO test-challenge; ARL	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Keypoint Detection; COCO test-challenge; ARM	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Cross-Lingual Document Classification; MLDoc Zero-Shot English-to-Chinese; Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Semi-Supervised Video Object Segmentation; DAVIS 2017 (test-dev); Jaccard (Decay)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Question Answering; WikiQA; MRR	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Fine-Grained Image Classification; FGVC Aircraft; FLOPS	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Real-time Instance Segmentation; MSCOCO; Frame (fps)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Image-to-Image Translation; Cityscapes Labels-to-Photo; FID	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Neural Architecture Search; ImageNet; Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Few-Shot Image Classification; FC100 5-way (5-shot); Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Atari Games; Atari 2600 Pooyan; Score	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Cross-Lingual NER; CoNLL Dutch; F1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Activity Prediction; ActEV; mAP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Video Super-Resolution; Ultra Video Group HD - 4x upscaling; Average PSNR	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Human Pose Forecasting; Human3.6M; MAR, walking, 400ms	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Face Detection; WIDER Face (Medium); AP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Scene Text Detection; Total-Text; Precision	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Link Prediction; YAGO3-10; Hits@1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Heterogeneous Node Classification; DBLP (PACT) 14k; Macro-F1 (60% training data)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Word Sense Disambiguation; SemEval 2007 Task 17; F1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Neural Architecture Search; CIFAR-10; Search Time (GPU days)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	3D Object Detection; KITTI Pedestrians Hard; AP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Few-Shot Image Classification; CUB-200-2011 - 0-Shot; Top-1 Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Neural Architecture Search; Oxford 102 Flowers; PARAMS	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Visual Dialog; VisDial v0.9 val; R@10	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Image Clustering; Tiny-ImageNet; Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Skeleton Based Action Recognition; SHREC 2017 track on 3D Hand Gesture Recognition; Speed  (FPS)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Link Prediction; WN18RR; MR	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Visual Question Answering; VQA-CP; Score	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Node Classification; USA Air-Traffic; Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Table-to-Text Generation; WikiBio; BLEU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Multi-Label Text Classification; EUR-Lex; P@5	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Action Segmentation; GTEA; Edit	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Weakly Supervised Action Localization; ActivityNet-1.3; mAP@0.5	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Skeleton Based Action Recognition; Florence 3D; Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Question Answering; WikiHop; Test	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Panoptic Segmentation; Cityscapes test; PQ	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Colorectal Gland Segmentation:; CRAG; Dice	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Keypoint Detection; COCO test-challenge; APL	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Image Super-Resolution; Set14 - 4x upscaling; MOS	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Semantic Segmentation; NYU Depth v2; Mean IoU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
