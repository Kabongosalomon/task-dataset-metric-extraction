true	1611.06624v3.pdf	Video Generation#UCF-101 16 frames, Unconditional, Single GPU#Inception Score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Question Answering#SQuAD1.1#F1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Human-Object Interaction Detection#HICO#mAP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Node Classification#Wikipedia#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Question Answering#SQuAD1.1#EM	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Visual Question Answering#VQA v2 test-std#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Semantic Segmentation#PASCAL VOC 2012 test#Mean IoU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Question Answering#YahooCQA#P@1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Semantic Segmentation#ShapeNet#Mean IoU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Pose Estimation#UPenn Action#Mean PCK@0.2	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Person Re-Identification#PRID2011#Rank-5	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Neural Architecture Search#ImageNet#Top-1 Error Rate	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Person Re-Identification#PRID2011#Rank-1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Image Classification#MNIST#Percentage error	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Question Answering#WikiQA#MRR	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Code Generation#WikiSQL#Execution Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#Jaccard (Decay)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Question Answering#CoQA#In-domain	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Temporal Action Localization#ActivityNet-1.3#mAP IOU@0.5	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Text Classification#IMDb#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Image Super-Resolution#Urban100 - 8x upscaling#SSIM	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Action Detection#UCF101-24#Video-mAP 0.2	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Language Modelling#One Billion Word#Number of params	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Neural Architecture Search#ImageNet#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Image Classification#Clothing1M#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Image Classification#CIFAR-100#Percentage correct	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Machine Translation#WMT2014 German-English#BLEU score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Object Counting#Pascal VOC 2007 count-test#m-reIRMSE-nz	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Instance Segmentation#Cityscapes test#Average Precision	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Action Classification#Charades#MAP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Scene Text Detection#Total-Text#Precision	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Link Prediction#YAGO3-10#Hits@3	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Link Prediction#YAGO3-10#Hits@1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Incremental Learning#CIFAR-100 - 50 classes + 10 steps of 5 classes#Average Incremental Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Trajectory Prediction#ETH/UCY#ADE-8/12	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Language Modelling#WikiText-2#Number of params	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Scene Text Detection#MSRA-TD500#F-Measure	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Neural Architecture Search#CIFAR-10#Search Time (GPU days)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Action Recognition#UCF101#3-fold Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Image Clustering#CIFAR-10#NMI	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Scene Text Detection#ICDAR 2013#Precision	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Node Classification#Pubmed#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Breast Tumour Classification#PCam#AUC	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Recommendation Systems#MovieLens 1M#nDCG@10	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Line Segment Detection#York Urban Dataset#sAP10	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Visual Question Answering#VQA-CP#Score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Machine Translation#IWSLT2015 English-Vietnamese#BLEU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	3D Reconstruction#Data3DâR2N2#3DIoU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Question Answering#WikiHop#Test	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Data-to-Text Generation#Rotowire (Content Selection)#Recall	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
False	1611.06624v3.pdf	Atari Games#Atari 2600 Freeway#Score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1910.10750v1.pdf	unknow	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Question Answering#SQuAD1.1#F1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Human-Object Interaction Detection#HICO#mAP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Node Classification#Wikipedia#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Question Answering#SQuAD1.1#EM	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Visual Question Answering#VQA v2 test-std#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Semantic Segmentation#PASCAL VOC 2012 test#Mean IoU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Question Answering#YahooCQA#P@1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Semantic Segmentation#ShapeNet#Mean IoU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Pose Estimation#UPenn Action#Mean PCK@0.2	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Person Re-Identification#PRID2011#Rank-5	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Neural Architecture Search#ImageNet#Top-1 Error Rate	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Person Re-Identification#PRID2011#Rank-1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Image Classification#MNIST#Percentage error	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Question Answering#WikiQA#MRR	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Code Generation#WikiSQL#Execution Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#Jaccard (Decay)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Question Answering#CoQA#In-domain	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Temporal Action Localization#ActivityNet-1.3#mAP IOU@0.5	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Text Classification#IMDb#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Image Super-Resolution#Urban100 - 8x upscaling#SSIM	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Action Detection#UCF101-24#Video-mAP 0.2	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Language Modelling#One Billion Word#Number of params	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Neural Architecture Search#ImageNet#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Image Classification#Clothing1M#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Image Classification#CIFAR-100#Percentage correct	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Machine Translation#WMT2014 German-English#BLEU score	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Object Counting#Pascal VOC 2007 count-test#m-reIRMSE-nz	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Instance Segmentation#Cityscapes test#Average Precision	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Action Classification#Charades#MAP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Scene Text Detection#Total-Text#Precision	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Link Prediction#YAGO3-10#Hits@3	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Link Prediction#YAGO3-10#Hits@1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Incremental Learning#CIFAR-100 - 50 classes + 10 steps of 5 classes#Average Incremental Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Trajectory Prediction#ETH/UCY#ADE-8/12	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Language Modelling#WikiText-2#Number of params	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Scene Text Detection#MSRA-TD500#F-Measure	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Neural Architecture Search#CIFAR-10#Search Time (GPU days)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Action Recognition#UCF101#3-fold Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Image Clustering#CIFAR-10#NMI	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Scene Text Detection#ICDAR 2013#Precision	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Node Classification#Pubmed#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Breast Tumour Classification#PCam#AUC	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Recommendation Systems#MovieLens 1M#nDCG@10	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Line Segment Detection#York Urban Dataset#sAP10	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Visual Question Answering#VQA-CP#Score	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Machine Translation#IWSLT2015 English-Vietnamese#BLEU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	3D Reconstruction#Data3DâR2N2#3DIoU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Question Answering#WikiHop#Test	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Data-to-Text Generation#Rotowire (Content Selection)#Recall	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
False	1910.10750v1.pdf	Atari Games#Atari 2600 Freeway#Score	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#F-measure (Decay)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#F-measure (Mean)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#F-measure (Recall)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#J&F	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#Jaccard (Decay)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#Jaccard (Mean)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#Jaccard (Recall)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Question Answering#SQuAD1.1#F1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Human-Object Interaction Detection#HICO#mAP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Node Classification#Wikipedia#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Question Answering#SQuAD1.1#EM	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Visual Question Answering#VQA v2 test-std#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Semantic Segmentation#PASCAL VOC 2012 test#Mean IoU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Question Answering#YahooCQA#P@1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Semantic Segmentation#ShapeNet#Mean IoU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Pose Estimation#UPenn Action#Mean PCK@0.2	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Person Re-Identification#PRID2011#Rank-5	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Neural Architecture Search#ImageNet#Top-1 Error Rate	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Person Re-Identification#PRID2011#Rank-1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Image Classification#MNIST#Percentage error	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Question Answering#WikiQA#MRR	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Code Generation#WikiSQL#Execution Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#Jaccard (Decay)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Question Answering#CoQA#In-domain	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Temporal Action Localization#ActivityNet-1.3#mAP IOU@0.5	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Text Classification#IMDb#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Image Super-Resolution#Urban100 - 8x upscaling#SSIM	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Action Detection#UCF101-24#Video-mAP 0.2	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Language Modelling#One Billion Word#Number of params	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Neural Architecture Search#ImageNet#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Image Classification#Clothing1M#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Image Classification#CIFAR-100#Percentage correct	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Machine Translation#WMT2014 German-English#BLEU score	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Object Counting#Pascal VOC 2007 count-test#m-reIRMSE-nz	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Instance Segmentation#Cityscapes test#Average Precision	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Action Classification#Charades#MAP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Scene Text Detection#Total-Text#Precision	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Link Prediction#YAGO3-10#Hits@3	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Link Prediction#YAGO3-10#Hits@1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Incremental Learning#CIFAR-100 - 50 classes + 10 steps of 5 classes#Average Incremental Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Trajectory Prediction#ETH/UCY#ADE-8/12	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Language Modelling#WikiText-2#Number of params	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Scene Text Detection#MSRA-TD500#F-Measure	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Neural Architecture Search#CIFAR-10#Search Time (GPU days)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Action Recognition#UCF101#3-fold Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Image Clustering#CIFAR-10#NMI	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Scene Text Detection#ICDAR 2013#Precision	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Node Classification#Pubmed#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Breast Tumour Classification#PCam#AUC	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Recommendation Systems#MovieLens 1M#nDCG@10	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Line Segment Detection#York Urban Dataset#sAP10	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Visual Question Answering#VQA-CP#Score	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Machine Translation#IWSLT2015 English-Vietnamese#BLEU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	3D Reconstruction#Data3DâR2N2#3DIoU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Question Answering#WikiHop#Test	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Data-to-Text Generation#Rotowire (Content Selection)#Recall	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
False	2002.03651v4.pdf	Atari Games#Atari 2600 Freeway#Score	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2005.12661v1.pdf	unknow	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Question Answering#SQuAD1.1#F1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Human-Object Interaction Detection#HICO#mAP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Node Classification#Wikipedia#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Question Answering#SQuAD1.1#EM	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Visual Question Answering#VQA v2 test-std#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Semantic Segmentation#PASCAL VOC 2012 test#Mean IoU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Question Answering#YahooCQA#P@1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Semantic Segmentation#ShapeNet#Mean IoU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Pose Estimation#UPenn Action#Mean PCK@0.2	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Person Re-Identification#PRID2011#Rank-5	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Neural Architecture Search#ImageNet#Top-1 Error Rate	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Person Re-Identification#PRID2011#Rank-1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Image Classification#MNIST#Percentage error	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Question Answering#WikiQA#MRR	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Code Generation#WikiSQL#Execution Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#Jaccard (Decay)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Question Answering#CoQA#In-domain	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Temporal Action Localization#ActivityNet-1.3#mAP IOU@0.5	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Text Classification#IMDb#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Image Super-Resolution#Urban100 - 8x upscaling#SSIM	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Action Detection#UCF101-24#Video-mAP 0.2	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Language Modelling#One Billion Word#Number of params	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Neural Architecture Search#ImageNet#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Image Classification#Clothing1M#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Image Classification#CIFAR-100#Percentage correct	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Machine Translation#WMT2014 German-English#BLEU score	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Object Counting#Pascal VOC 2007 count-test#m-reIRMSE-nz	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Instance Segmentation#Cityscapes test#Average Precision	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Action Classification#Charades#MAP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Scene Text Detection#Total-Text#Precision	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Link Prediction#YAGO3-10#Hits@3	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Link Prediction#YAGO3-10#Hits@1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Incremental Learning#CIFAR-100 - 50 classes + 10 steps of 5 classes#Average Incremental Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Trajectory Prediction#ETH/UCY#ADE-8/12	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Language Modelling#WikiText-2#Number of params	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Scene Text Detection#MSRA-TD500#F-Measure	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Neural Architecture Search#CIFAR-10#Search Time (GPU days)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Action Recognition#UCF101#3-fold Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Image Clustering#CIFAR-10#NMI	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Scene Text Detection#ICDAR 2013#Precision	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Node Classification#Pubmed#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Breast Tumour Classification#PCam#AUC	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Recommendation Systems#MovieLens 1M#nDCG@10	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Line Segment Detection#York Urban Dataset#sAP10	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Visual Question Answering#VQA-CP#Score	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Machine Translation#IWSLT2015 English-Vietnamese#BLEU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	3D Reconstruction#Data3DâR2N2#3DIoU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Question Answering#WikiHop#Test	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Data-to-Text Generation#Rotowire (Content Selection)#Recall	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
False	2005.12661v1.pdf	Atari Games#Atari 2600 Freeway#Score	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
