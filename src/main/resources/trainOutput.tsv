true	1611.06624v3.pdf	Video Generation; UCF-101 16 frames, Unconditional, Single GPU; Inception Score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
