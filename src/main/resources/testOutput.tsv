true	1611.06624v3.pdf	Question Answering#SQuAD1.1#F1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Human-Object Interaction Detection#HICO#mAP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Node Classification#Wikipedia#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#SQuAD1.1#EM	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Visual Question Answering#VQA v2 test-std#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semantic Segmentation#PASCAL VOC 2012 test#Mean IoU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#YahooCQA#P@1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semantic Segmentation#ShapeNet#Mean IoU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Pose Estimation#UPenn Action#Mean PCK@0.2	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Person Re-Identification#PRID2011#Rank-5	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Neural Architecture Search#ImageNet#Top-1 Error Rate	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Person Re-Identification#PRID2011#Rank-1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Classification#MNIST#Percentage error	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#WikiQA#MRR	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Code Generation#WikiSQL#Execution Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#Jaccard (Decay)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#CoQA#In-domain	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Temporal Action Localization#ActivityNet-1.3#mAP IOU@0.5	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Text Classification#IMDb#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Super-Resolution#Urban100 - 8x upscaling#SSIM	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Action Detection#UCF101-24#Video-mAP 0.2	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Language Modelling#One Billion Word#Number of params	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Neural Architecture Search#ImageNet#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Classification#Clothing1M#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Classification#CIFAR-100#Percentage correct	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Machine Translation#WMT2014 German-English#BLEU score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Object Counting#Pascal VOC 2007 count-test#m-reIRMSE-nz	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Instance Segmentation#Cityscapes test#Average Precision	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Action Classification#Charades#MAP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Scene Text Detection#Total-Text#Precision	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Link Prediction#YAGO3-10#Hits@3	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Link Prediction#YAGO3-10#Hits@1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Incremental Learning#CIFAR-100 - 50 classes + 10 steps of 5 classes#Average Incremental Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Trajectory Prediction#ETH/UCY#ADE-8/12	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Language Modelling#WikiText-2#Number of params	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Scene Text Detection#MSRA-TD500#F-Measure	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Neural Architecture Search#CIFAR-10#Search Time (GPU days)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Action Recognition#UCF101#3-fold Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Clustering#CIFAR-10#NMI	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Scene Text Detection#ICDAR 2013#Precision	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Node Classification#Pubmed#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Breast Tumour Classification#PCam#AUC	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Recommendation Systems#MovieLens 1M#nDCG@10	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Line Segment Detection#York Urban Dataset#sAP10	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Visual Question Answering#VQA-CP#Score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Machine Translation#IWSLT2015 English-Vietnamese#BLEU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	3D Reconstruction#Data3DâR2N2#3DIoU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#WikiHop#Test	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Data-to-Text Generation#Rotowire (Content Selection)#Recall	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Atari Games#Atari 2600 Freeway#Score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semantic Segmentation#NYU Depth v2#Mean IoU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Person Re-Identification#MARS#mAP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Link Prediction#WN18#MRR	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	3D Object Detection#nuScenes#NDS	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Drug Discovery#ToxCast#AUC	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Pose Estimation# ITOP front-view#Mean mAP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Classification#Tiered ImageNet 5-way (5-shot)#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Neural Architecture Search#NAS-Bench-201, ImageNet-16-120#Search time (s)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#YahooCQA#MRR	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Sentiment Analysis#Multi-Domain Sentiment Dataset#Books	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Relation Extraction#TACRED#F1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Instance Segmentation#COCO test-dev#AP75	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Aspect-Based Sentiment Analysis#SemEval 2014 Task 4 Sub Task 2#Restaurant (Acc)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Recommendation Systems#Netflix#Recall@50	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semantic Segmentation#Semantic3D#mIoU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	3D Human Pose Estimation#Human3.6M#Average MPJPE (mm)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Denoising#Darmstadt Noise Dataset#PSNR	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Speech Enhancement#DEMAND#PESQ	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Object Counting#Pascal VOC 2007 count-test#mRMSE	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Recommendation Systems#Netflix#Recall@20	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Pose Estimation#COCO test-dev#APL	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Multi-Label Classification#MS-COCO#mAP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Pose Estimation#COCO test-dev#APM	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Generation#CIFAR-10#Model Entropy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Classification#SVHN#Percentage error	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Machine Translation#WMT2014 English-French#BLEU score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Temporal Action Localization#ActivityNet-1.3#mAP IOU@0.95	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Human Interaction Recognition#UT#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Node Classification#Reddit#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Face Verification#IJB-A#TAR @ FAR=0.01	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	3D Human Pose Estimation#MPI-INF-3DHP#AUC	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Natural Language Inference#SNLI#Parameters	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Depth Completion#KITTI Depth Completion#RMSE	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Lipreading#LRS2#Word Error Rate (WER)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Multi-tissue Nucleus Segmentation#Kumar#Dice	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Link Prediction#WN18#Hits@10	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Person Re-Identification#Market-1501->DukeMTMC-reID#mAP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Visual Question Answering#GQA test-std#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Entity Typing#Ontonotes v5 (English)#Recall	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Action Recognition#THUMOSâ14#mAP@0.5	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Action Recognition#THUMOSâ14#mAP@0.4	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Matting#Composition-1K#MSE	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Action Recognition#THUMOSâ14#mAP@0.3	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Temporal Action Localization#ActivityNet-1.3#mAP IOU@0.75	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Data-to-Text Generation#Rotowire (Content Selection)#Precision	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Atari Games#Atari 2600 Montezuma's Revenge#Score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Action Recognition#Jester#Val	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Few-Shot Image Classification#Mini-ImageNet - 1-Shot Learning#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Classification#CIFAR-100#Percentage error	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Action Recognition#Something-Something V2#Top-1 Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#QASent#MAP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semantic Segmentation#CamVid#Mean IoU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Neural Architecture Search#ImageNet#Params	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Instance Segmentation#COCO test-dev#AP50	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Data-to-Text Generation#E2E NLG Challenge#CIDEr	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Speech Recognition#LibriSpeech test-other#Word Error Rate (WER)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semantic Segmentation#PASCAL Context#mIoU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Graph Classification#PTC#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Classification#MNIST#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Visual Question Answering#COCO Visual Question Answering (VQA) real images 1.0 open ended#Percentage correct	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image-to-Image Translation#Cityscapes Labels-to-Photo#Per-pixel Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Fine-Grained Image Classification# CUB-200-2011#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Visual Question Answering#VQA v2 test-dev#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semantic Segmentation#Cityscapes val#mIoU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Generation#CIFAR-10#Inception score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Click-Through Rate Prediction#MovieLens 20M#AUC	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Super-Resolution#Set5 - 4x upscaling#PSNR	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Automated Theorem Proving#HOList benchmark#Percentage correct	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Text Summarization#arXiv#ROUGE-1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	3D Semantic Segmentation#SemanticKITTI#mIoU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Action Recognition#NTU RGB+D#Accuracy (CV)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Domain Adaptation#UCF-to-HMDBfull#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Lipreading#Lip Reading in the Wild#Top-1 Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Super-Resolution#FFHQ 1024 x 1024 - 4x upscaling#FID	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semantic Segmentation#S3DIS#Mean IoU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semantic Segmentation#ADE20K val#mIoU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Machine Translation#WMT2014 English-German#BLEU score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Relation Extraction#ACE 2004#Entity F1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Face Detection#Annotated Faces in the Wild#AP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Ad-Hoc Information Retrieval#TREC Robust04#nDCG@20	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Scene Text Detection#Total-Text#Recall	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Language Modelling#WikiText-103#Test perplexity	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Continual Learning#CUBS (Fine-grained 6 Tasks)#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	3D Human Pose Estimation#Human3.6M#Multi-View or Monocular	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Visual Question Answering#VQA v1 test-dev#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Link Prediction#WN18#Hits@3	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Word Sense Disambiguation#SemEval 2007 Task 7#F1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Link Prediction#WN18#Hits@1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Retrieval#Flickr30K 1K test#R@5	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Retrieval#Flickr30K 1K test#R@1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#F-measure (Mean)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Fine-Grained Image Classification#FGVC Aircraft#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Music Source Separation#MUSDB18#SDR (vocals)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Super-Resolution#FFHQ 1024 x 1024 - 4x upscaling#MS-SSIM	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	3D Human Pose Estimation#Human3.6M#Using 2D ground-truth joints	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Data-to-Text Generation#E2E NLG Challenge#METEOR	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Link Prediction#FB15k-237#MRR	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Sentiment Analysis#Multi-Domain Sentiment Dataset#Electronics	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Recommendation Systems#MovieLens 1M#HR@10	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Time Series Classification#PhysioNet Challenge 2012#AUC Stdev	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Drug Discovery#QM9#Error ratio	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Face Verification#CASIA NIR-VIS 2.0#TAR @ FAR=0.001	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Action Recognition#NTU RGB+D#Accuracy (CS)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Graph Regression#Lipophilicity #RMSE	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Scene Text Detection#ICDAR 2017 MLT#Precision	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Face Verification#Labeled Faces in the Wild#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Action Recognition#NTU RGB+D 120#Accuracy (Cross-Subject)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	3D Human Pose Estimation#MPI-INF-3DHP#3DPCK	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#Jaccard (Mean)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Meta-Learning#MT50#Average Success Rate	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image-to-Image Translation#COCO-Stuff Labels-to-Photos#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Scene Text Detection#MSRA-TD500#Recall	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Scene Text Detection#ICDAR 2017 MLT#Recall	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Robotic Grasping#Cornell Grasp Dataset#5 fold cross validation	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Temporal Action Localization#ActivityNet-1.3#mAP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Link Prediction#FB15k-237#Hits@10	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	6D Pose Estimation#YCB-Video#ADDS AUC	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	3D Human Pose Estimation#HumanEva-I#Mean Reconstruction Error (mm)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Visual Question Answering#VCR (QA-R) test#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Pose Estimation#COCO test-dev#AP50	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Shadow Detection#SBU#BER	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#F-measure (Mean)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Dehazing#SOTS Indoor#SSIM	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Clustering#Extended Yale-B#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#F-measure (Recall)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Link Prediction#WN18RR#MRR	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Action Recognition#Something-Something V1#Top 5 Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Object Detection#COCO test-dev#AP75	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Generation#LSUN Bedroom 256 x 256#FID	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#SQuAD2.0 dev#F1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Text Generation#EMNLP2017 WMT#BLEU-5	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Text Generation#EMNLP2017 WMT#BLEU-4	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#SQuAD2.0 dev#EM	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Sentiment Analysis#Multi-Domain Sentiment Dataset#Average	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Natural Language Inference#MultiNLI#Matched	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Dense Pixel Correspondence Estimation#HPatches#Viewpoint V AEPE	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Traffic Prediction#METR-LA#MAE @ 12 step	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Click-Through Rate Prediction#Dianping#Log Loss	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Person Re-Identification#PRID2011#Rank-20	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Speech Recognition#LibriSpeech test-clean#Word Error Rate (WER)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Text Generation#EMNLP2017 WMT#BLEU-3	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Text Generation#EMNLP2017 WMT#BLEU-2	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Neural Architecture Search#ImageNet#FLOPS	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Cross-Lingual Bitext Mining#BUCC German-to-English#F1 score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Scene Text Detection#ICDAR 2017 MLT#F-Measure	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Metric Learning#CARS196#R@1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Code Generation#WikiSQL#Exact Match Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Scene Text Detection#Total-Text#F-Measure	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Classification#STL-10#Percentage correct	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Facial Expression Recognition#FER2013#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Graph Classification#PROTEINS#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Neural Architecture Search#CIFAR-10 Image Classification#Percentage error	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#SQuAD1.1 dev#F1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Scene Text Detection#ICDAR 2015#F-Measure	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Speech Recognition#Switchboard + Hub500#Percentage error	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#SQuAD1.1 dev#EM	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Few-Shot Image Classification#Mini-Imagenet 5-way (5-shot)#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Classification#CIFAR-10#Percentage correct	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Neural Architecture Search#CIFAR-10 Image Classification#Params	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	3D Human Pose Estimation#Surreal#MPJPE	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Facial Landmark Detection#300W#NME	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Multi-Object Tracking#MOT16#MOTA	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Language Modelling#Text8#Number of params	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#Jaccard (Decay)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Action Classification#HMDB51#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Reading Comprehension#RACE#Accuracy (High)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Person Re-Identification#Market-1501#Rank-1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#Jaccard (Recall)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Generation#Fashion-MNIST#FID	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Data-to-Text Generation#E2E NLG Challenge#ROUGE-L	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#F-measure (Recall)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Object Detection#COCO minival#APL	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Object Detection#COCO minival#APM	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Generation#CIFAR-10#FID	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Temporal Action Localization#CrossTask#Recall	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Generation#MNIST#bits/dimension	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Pose Estimation#COCO test-dev#AP75	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Temporal Action Localization#J-HMDB-21#Frame-mAP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Object Detection#COCO minival#APS	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Named Entity Recognition#GENIA#F1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Super-Resolution#FFHQ 1024 x 1024 - 4x upscaling#PSNR	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Reading Comprehension#RACE#Accuracy (Middle)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Object Detection#COCO test-dev#AP50	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#Jaccard (Recall)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Conversational Response Selection#Ubuntu Dialogue (v1, Ranking)#R2@1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Person Re-Identification#DukeMTMC-reID#Rank-1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Ad-Hoc Information Retrieval#TREC Robust04#MAP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semantic Role Labeling#CoNLL 2005#F1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Person Re-Identification#DukeMTMC-reID#Rank-5	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Music Source Separation#MUSDB18#SDR (bass)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Language Modelling#Penn Treebank (Word Level)#Params	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Object Detection#PASCAL VOC 2007#MAP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#CNN / Daily Mail#CNN	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#TrecQA#MAP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Common Sense Reasoning#CommonsenseQA#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Retrieval#CARS196#R@1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Sentiment Analysis#Yelp Fine-grained classification#Error	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Action Recognition#NTU RGB+D 120#Accuracy (Cross-Setup)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Link Prediction#WN18#MR	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Salient Object Detection#DUTS-TE#MAE	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Scene Text Detection#ICDAR 2013#F-Measure	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Generation#SQuAD1.1#BLEU-4	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Neural Architecture Search#CIFAR-10#Top-1 Error Rate	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Action Recognition#Sports-1M#Video hit@1 	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Scene Text Detection#ICDAR 2015#Precision	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Object Counting#CARPK#MAE	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	3D Object Reconstruction#Data3DâR2N2#3DIoU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Dependency Parsing#Penn Treebank#LAS	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	3D Object Reconstruction#Data3DâR2N2#Avg F1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Hand Pose Estimation#NYU Hands#Average 3D Error	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Conversational Response Selection#Ubuntu Dialogue (v1, Ranking)#R10@5	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#CoQA#Out-of-domain	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Conversational Response Selection#Ubuntu Dialogue (v1, Ranking)#R10@1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Conversational Response Selection#Ubuntu Dialogue (v1, Ranking)#R10@2	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#F-measure (Decay)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Optical Flow Estimation#KITTI 2012#Average End-Point Error	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Domain Adaptation#ImageCLEF-DA#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Aspect-Based Sentiment Analysis#SemEval 2014 Task 4 Sub Task 2#Mean Acc (Restaurant + Laptop)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#CoQA#Overall	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Text Generation#Yahoo Questions#KL	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Line Segment Detection#York Urban Dataset#sAP5	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image-to-Image Translation#Cityscapes Labels-to-Photo#Class IOU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Part-Of-Speech Tagging#Penn Treebank#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#J&F	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Object Counting#Pascal VOC 2007 count-test#m-relRMSE	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semantic Segmentation#Cityscapes test#Mean IoU (class)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Natural Language Inference#SNLI#% Train Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Person Re-Identification#MARS#Rank-5	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Clustering#MNIST-test#NMI	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Sentiment Analysis#SST-2 Binary classification#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Link Prediction#FB15k-237#MR	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Person Re-Identification#MARS#Rank-1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Human-Object Interaction Detection#HICO-DET#MAP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Text Classification#R8#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Person Re-Identification#Market-1501->DukeMTMC-reID#Rank-1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Time Series Classification#PhysioNet Challenge 2012#AUC	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Node Classification#Wikipedia#Macro-F1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Facial Expression Recognition#AffectNet#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#Quora Question Pairs#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Atari Games#Atari 2600 Q*Bert#Score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Data-to-Text Generation#MULTIWOZ 2.1#BLEU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Speech Synthesis#North American English#Mean Opinion Score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Text Summarization#GigaWord#ROUGE-L	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Retrieval#Flickr30K 1K test#R@10	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Reconstruction#Edge-to-Handbags#FID	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Natural Language Inference#SNLI#% Test Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Text Summarization#GigaWord#ROUGE-1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Text Summarization#GigaWord#ROUGE-2	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Entity Typing#Ontonotes v5 (English)#F1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Click-Through Rate Prediction#Criteo#Log Loss	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Named Entity Recognition#Ontonotes v5 (English)#F1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Dependency Parsing#Penn Treebank#POS	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Domain Adaptation#USPS-to-MNIST#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Object Detection#COCO test-dev#APS	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Optical Flow Estimation#Sintel-clean#Average End-Point Error	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Link Prediction#YAGO3-10#MRR	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Object Detection#KITTI Cars Moderate#AP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#bAbi#Mean Error Rate	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Click-Through Rate Prediction#Criteo#AUC	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Scene Text Detection#ICDAR 2015#Recall	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Entity Typing#Ontonotes v5 (English)#Precision	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Lipreading#GRID corpus (mixed-speech)#Word Error Rate (WER)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#F-measure (Decay)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Relation Extraction#CoNLL04#Relation F1 	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Clustering#MNIST-test#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Generation#Stanford Dogs#Inception score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Dense Pixel Correspondence Estimation#HPatches#Viewpoint IV AEPE	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Subjectivity Analysis#SUBJ#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Classification#ImageNet#Number of params	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#bAbi#Accuracy (trained on 10k)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	3D Human Pose Estimation#MPI-INF-3DHP#MJPE	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Speech Recognition#TIMIT#Percentage error	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Part-Of-Speech Tagging#UD#Avg accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Classification#ImageNet#Top 5 Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Clustering#CIFAR-10#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Object Detection#COCO test-dev#APM	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Object Detection#COCO test-dev#APL	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#MS MARCO#Rouge-L	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Graph Classification#NEURON-BINARY#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Language Modelling#Penn Treebank (Word Level)#Test perplexity	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Pose Tracking#PoseTrack2017#MOTA	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image-to-Image Translation#SYNTHIA-to-Cityscapes#mIoU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Visual Question Answering#MSVD-QA#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Relation Extraction#CoNLL04#Entity F1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Motion Segmentation#Hopkins155#Classification Error	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Person Re-Identification#DukeMTMC-reID#MAP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Pose Estimation#MPII Human Pose#PCKh-0.5	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Graph Classification#NCI1#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Named Entity Recognition#SciERC#F1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Scene Text Detection#MSRA-TD500#Precision	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	3D Human Pose Estimation#3DPW#PA-MPJPE	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Classification#Kuzushiji-MNIST#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Language Modelling#Text8#Bit per Character (BPC)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image-to-Image Translation#COCO-Stuff Labels-to-Photos#FID	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Super-Resolution#Urban100 - 8x upscaling#PSNR	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semantic Segmentation#LIP val#mIoU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Data-to-Text Generation#E2E NLG Challenge#NIST	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Sentiment Analysis#Multi-Domain Sentiment Dataset#DVD	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Domain Adaptation#Office-Caltech#Average Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Relation Extraction#DocRED#F1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#MS MARCO#BLEU-1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Music Source Separation#MUSDB18#SDR (drums)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Machine Translation#WMT2016 English-German#BLEU score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Face Verification#CK+#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Language Modelling#Penn Treebank (Word Level)#Validation perplexity	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#J&F	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Relation Extraction#NYT#F1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Generation#ImageNet 64x64#Bits per dim	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semantic Segmentation#ScanNet#3DIoU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Dense Pixel Correspondence Estimation#HPatches#Viewpoint I AEPE	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Few-Shot Image Classification#Mini-ImageNet-CUB 5-way (5-shot)#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Language Modelling#WikiText-103#Validation perplexity	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Neural Architecture Search#NAS-Bench-201, ImageNet-16-120#Accuracy (Test)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Object Detection#COCO minival#AP50	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Few-Shot Image Classification#CIFAR-FS 5-way (5-shot)#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Object Counting#Pascal VOC 2007 count-test#mRMSE-nz	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Action Classification#Kinetics-400#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	3D Object Detection#ScanNetV2#mAP@0.5	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Instance Segmentation#COCO test-dev#mask AP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Named Entity Recognition#ACE 2005#F1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Clustering#MNIST-full#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Object Detection#COCO test-dev#box AP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image-to-Image Translation#RaFD#Classification Error	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Sentiment Analysis#IMDb#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#WikiQA#MAP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image-to-Image Translation#COCO-Stuff Labels-to-Photos#mIoU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Node Classification#Cora with Public Split: fixed 20 nodes per class#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Instance Segmentation#COCO test-dev#APS	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Generation#Binarized MNIST#nats	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Instance Segmentation#COCO test-dev#APM	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Instance Segmentation#COCO test-dev#APL	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Passage Re-Ranking#MS MARCO#MRR	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Entity Alignment#DBP15k zh-en#Hits@1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Object Detection#COCO minival#AP75	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	3D Object Detection#ScanNetV2#mAP@0.25	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Causal Inference#IDHP#Average Treatment Effect Error	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Text Summarization#DUC 2004 Task 1#ROUGE-1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Text Summarization#DUC 2004 Task 1#ROUGE-2	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Link Prediction#YAGO3-10#Hits@10	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Pose Estimation#COCO test-dev#AR	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Pose Estimation#COCO test-dev#AP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#Children's Book Test#Accuracy-NE	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Dense Pixel Correspondence Estimation#HPatches#Viewpoint III AEPE	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Action Recognition#Sports-1M#Video hit@5	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Natural Language Inference#MultiNLI#Mismatched	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Dependency Parsing#Penn Treebank#UAS	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Text Summarization#DUC 2004 Task 1#ROUGE-L	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Human Part Segmentation#PASCAL-Person-Part#mIoU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Generation#LSUN Churches 256 x 256#FID	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Action Recognition#HMDB-51#Average accuracy of 3 splits	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Relation Extraction#ACE 2004#Entity+Relation F1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Language Modelling#WikiText-2#Validation perplexity	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image-to-Image Translation#Cityscapes Labels-to-Photo#Per-class Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Text Generation#Yahoo Questions#Perplexity	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Machine Translation#IWSLT2014 German-English#BLEU score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Dehazing#SOTS Indoor#PSNR	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Recommendation Systems#Netflix#nDCG@100	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Classification#ImageNet#Top 1 Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Data-to-Text Generation#E2E NLG Challenge#BLEU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Ad-Hoc Information Retrieval#TREC Robust04#P@20	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Graph Classification#NCI109#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Natural Language Inference#SciTail#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Drug Discovery#Tox21#AUC	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Fake News Detection#FNC-1#Weighted Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#Jaccard (Mean)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Face Verification#CFP-FP#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Scene Text Detection#SCUT-CTW1500#F-Measure	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Named Entity Recognition#CoNLL 2003 (English)#F1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Action Recognition#Something-Something V1#Top 1 Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Optical Character Recognition#FSNS - Test#Sequence error	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Link Prediction#FB15k-237#Hits@3	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Link Prediction#FB15k-237#Hits@1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image-to-Image Translation#Cityscapes-to-Foggy Cityscapes#mAP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#TrecQA#MRR	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Generation#CIFAR-10#bits/dimension	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Horizon Line Estimation#York Urban Dataset#AUC (horizon error)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	3D Object Detection#KITTI Pedestrians Moderate#AP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Speech Recognition#swb_hub_500 WER fullSWBCH#Percentage error	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Neural Architecture Search#NAS-Bench-201, ImageNet-16-120#Accuracy (val)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Depth Completion#KITTI Depth Completion#MAE	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Machine Translation#IWSLT2015 German-English#BLEU score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Person Re-Identification#Market-1501#MAP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Classification#EMNIST-Letters#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Face Detection#WIDER Face (Hard)#AP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Text Style Transfer#Yelp Review Dataset (Small)#G-Score (BLEU, Accuracy)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Extractive Document Summarization#CNN / Daily Mail#ROUGE-2	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Super-Resolution#PIRM-test#NIQE	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Extractive Document Summarization#CNN / Daily Mail#ROUGE-1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Link Prediction#Cora#AP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image-to-Image Translation#GTAV-to-Cityscapes Labels#mIoU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#QASent#MRR	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Aspect-Based Sentiment Analysis#SemEval 2014 Task 4 Sub Task 2#Laptop (Acc)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#WebQuestions#F1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Few-Shot Image Classification#OMNIGLOT - 1-Shot, 5-way#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Few-Shot Semantic Segmentation#FSS-1000#Mean IoU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Scene Text Detection#ICDAR 2013#Recall	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Lipreading#LRW-1000#Top-1 Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Language Modelling#WikiText-2#Test perplexity	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Scene Text Detection#SCUT-CTW1500#Recall	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Graph Classification#IMDb-B#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	3D Human Pose Estimation#Total Capture#Average MPJPE (mm)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Salient Object Detection#DUTS-TE#F-measure	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.6	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.5	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Multi-tissue Nucleus Segmentation#Kumar#Hausdorff Distance (mm)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.7	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.2	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.4	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.3	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Super-Resolution#FFHQ 1024 x 1024 - 4x upscaling#SSIM	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Face Verification#YouTube Faces DB#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Link Prediction#Cora#AUC	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Document Image Classification#RVL-CDIP#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Semantic Segmentation#PASCAL VOC 2012 val#mIoU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Atari Games#Atari 2600 Breakout#Score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Sentiment Analysis#Multi-Domain Sentiment Dataset#Kitchen	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Super-Resolution#VggFace2 - 8x upscaling#PSNR	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Click-Through Rate Prediction#Dianping#AUC	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#bAbi#Accuracy (trained on 1k)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Object Counting#CARPK#RMSE	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Retrieval#Oxf5k#MAP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#Children's Book Test#Accuracy-CN	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Super-Resolution#Set5 - 4x upscaling#SSIM	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Recommendation Systems#MovieLens 1M#RMSE	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Sentiment Analysis#MR#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Video Generation#UCF-101 16 frames, Unconditional, Single GPU#Inception Score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Atari Games#Atari 2600 Fishing Derby#Score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Dehazing#O-Haze#PSNR	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Language Modelling#enwik8#Number of params	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Dense Pixel Correspondence Estimation#HPatches#Viewpoint II AEPE	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Machine Translation#WMT2014 English-German#SacreBLEU	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Human action generation#Human3.6M#MMD	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Generation#Stanford Dogs#FID	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Domain Adaptation#Office-31#Average Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Clustering#MNIST-full#NMI	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Constituency Parsing#Penn Treebank#F1 score	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Image Reconstruction#Edge-to-Handbags#LPIPS	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Language Modelling#enwik8#Bit per Character (BPC)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Reading Comprehension#RACE#Accuracy	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Text Generation#Yahoo Questions#NLL	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Object Detection#COCO minival#box AP	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Anomaly Detection#CIFAR-10 model detecting CIFAR-10#AUPR	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Music Source Separation#MUSDB18#SDR (other)	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Scene Text Detection#SCUT-CTW1500#Precision	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#NewsQA#EM	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Speech Separation#wsj0-2mix#SI-SDRi	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Language Modelling#One Billion Word#PPL	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Coreference Resolution#OntoNotes#F1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	Question Answering#NewsQA#F1	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1611.06624v3.pdf	unknown	Temporal Generative Adversarial Nets with Singular Value Clipping In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods. address this problem by decomposing it into background generation and foreground generation, this approach has a drawback that it cannot generate a scene with dynamic background due to the static background assumption [44]. To the best of our knowledge, there is no study that tackles video generation without such assumption and generates diversified videos like natural videos. Although a simple approach is to use 3D convolutional layers for representing the generating process of a video, it implies that images along x-t plane and y-t plane besides x-y plane are considered equally, where x and y denote the spatial dimensions and t denotes the time dimension. We believe that the nature of time dimension is essentially different from the spatial dimensions in the case of videos so that such approach has difficulty on the video generation problem. The relevance of this assumption has been also discussed in some recent studies [33, 24, 46] that have shown good performance on the video recognition task. We performed experiments with the following datasets Moving MNIST To investigate the properties of our models, we trained the models on the moving MNIST dataset, in which there are 10,000 clips each of which has 20 frames and consists of two digits moving inside a 64 × 64 patch In our experiments, we randomly extracted 16 frames from these clips and used them as a training dataset UCF-101 UCF-101 is a commonly used video dataset that consists of 13,320 videos belonging to 101 different categories such as IceDancing and Baseball Pitch Since the resolution of videos in the dataset is too large for the generative models, we resized all the videos to 85 × 64 pixels, randomly extracted 16 frames, and cropped a center square with 64 pixels We trained our proposed model on the above datasets and visually confirmed the quality of the results shows examples of generated videos by Table 1. Network configuration of the generator. The second row represents the input variables. "linear (·)" is the number of output units in the linear layer. The parameters in the convolutional and the deconvolutional layer are denoted as "conv/deconv ((kernel size), (output channels), (padding), (strides))." ? R 100 Temporal generator Image generator z t Table 2. Proposed methods to satisfy the 1-Lipschitz constraint. · denotes a spectral norm. a represents a fixed parameter of the LeakyReLU layer. ? and ?B are a scaling parameter after the batch normalization and a running mean of a standard deviation of a batch, respectively. clipping . Algorithm 1 WGAN using Singular Value Clipping n D :
true	1910.10750v1.pdf	Question Answering#SQuAD1.1#F1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Human-Object Interaction Detection#HICO#mAP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Node Classification#Wikipedia#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#SQuAD1.1#EM	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Visual Question Answering#VQA v2 test-std#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semantic Segmentation#PASCAL VOC 2012 test#Mean IoU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#YahooCQA#P@1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semantic Segmentation#ShapeNet#Mean IoU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Pose Estimation#UPenn Action#Mean PCK@0.2	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Person Re-Identification#PRID2011#Rank-5	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Neural Architecture Search#ImageNet#Top-1 Error Rate	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Person Re-Identification#PRID2011#Rank-1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Classification#MNIST#Percentage error	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#WikiQA#MRR	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Code Generation#WikiSQL#Execution Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#Jaccard (Decay)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#CoQA#In-domain	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Temporal Action Localization#ActivityNet-1.3#mAP IOU@0.5	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Text Classification#IMDb#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Super-Resolution#Urban100 - 8x upscaling#SSIM	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Action Detection#UCF101-24#Video-mAP 0.2	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Language Modelling#One Billion Word#Number of params	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Neural Architecture Search#ImageNet#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Classification#Clothing1M#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Classification#CIFAR-100#Percentage correct	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Machine Translation#WMT2014 German-English#BLEU score	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Object Counting#Pascal VOC 2007 count-test#m-reIRMSE-nz	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Instance Segmentation#Cityscapes test#Average Precision	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Action Classification#Charades#MAP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Scene Text Detection#Total-Text#Precision	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Link Prediction#YAGO3-10#Hits@3	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Link Prediction#YAGO3-10#Hits@1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Incremental Learning#CIFAR-100 - 50 classes + 10 steps of 5 classes#Average Incremental Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Trajectory Prediction#ETH/UCY#ADE-8/12	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Language Modelling#WikiText-2#Number of params	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Scene Text Detection#MSRA-TD500#F-Measure	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Neural Architecture Search#CIFAR-10#Search Time (GPU days)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Action Recognition#UCF101#3-fold Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Clustering#CIFAR-10#NMI	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Scene Text Detection#ICDAR 2013#Precision	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Node Classification#Pubmed#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Breast Tumour Classification#PCam#AUC	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Recommendation Systems#MovieLens 1M#nDCG@10	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Line Segment Detection#York Urban Dataset#sAP10	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Visual Question Answering#VQA-CP#Score	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Machine Translation#IWSLT2015 English-Vietnamese#BLEU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	3D Reconstruction#Data3DâR2N2#3DIoU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#WikiHop#Test	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Data-to-Text Generation#Rotowire (Content Selection)#Recall	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Atari Games#Atari 2600 Freeway#Score	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semantic Segmentation#NYU Depth v2#Mean IoU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Person Re-Identification#MARS#mAP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Link Prediction#WN18#MRR	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	3D Object Detection#nuScenes#NDS	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Drug Discovery#ToxCast#AUC	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Pose Estimation# ITOP front-view#Mean mAP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Classification#Tiered ImageNet 5-way (5-shot)#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Neural Architecture Search#NAS-Bench-201, ImageNet-16-120#Search time (s)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#YahooCQA#MRR	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Sentiment Analysis#Multi-Domain Sentiment Dataset#Books	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Relation Extraction#TACRED#F1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Instance Segmentation#COCO test-dev#AP75	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Aspect-Based Sentiment Analysis#SemEval 2014 Task 4 Sub Task 2#Restaurant (Acc)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Recommendation Systems#Netflix#Recall@50	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semantic Segmentation#Semantic3D#mIoU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	3D Human Pose Estimation#Human3.6M#Average MPJPE (mm)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Denoising#Darmstadt Noise Dataset#PSNR	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Speech Enhancement#DEMAND#PESQ	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Object Counting#Pascal VOC 2007 count-test#mRMSE	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Recommendation Systems#Netflix#Recall@20	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Pose Estimation#COCO test-dev#APL	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Multi-Label Classification#MS-COCO#mAP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Pose Estimation#COCO test-dev#APM	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Generation#CIFAR-10#Model Entropy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Classification#SVHN#Percentage error	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Machine Translation#WMT2014 English-French#BLEU score	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Temporal Action Localization#ActivityNet-1.3#mAP IOU@0.95	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Human Interaction Recognition#UT#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Node Classification#Reddit#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Face Verification#IJB-A#TAR @ FAR=0.01	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	3D Human Pose Estimation#MPI-INF-3DHP#AUC	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Natural Language Inference#SNLI#Parameters	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Depth Completion#KITTI Depth Completion#RMSE	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Lipreading#LRS2#Word Error Rate (WER)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Multi-tissue Nucleus Segmentation#Kumar#Dice	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Link Prediction#WN18#Hits@10	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Person Re-Identification#Market-1501->DukeMTMC-reID#mAP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Visual Question Answering#GQA test-std#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Entity Typing#Ontonotes v5 (English)#Recall	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Action Recognition#THUMOSâ14#mAP@0.5	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Action Recognition#THUMOSâ14#mAP@0.4	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Matting#Composition-1K#MSE	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Action Recognition#THUMOSâ14#mAP@0.3	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Temporal Action Localization#ActivityNet-1.3#mAP IOU@0.75	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Data-to-Text Generation#Rotowire (Content Selection)#Precision	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Atari Games#Atari 2600 Montezuma's Revenge#Score	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Action Recognition#Jester#Val	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Few-Shot Image Classification#Mini-ImageNet - 1-Shot Learning#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Classification#CIFAR-100#Percentage error	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Action Recognition#Something-Something V2#Top-1 Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#QASent#MAP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semantic Segmentation#CamVid#Mean IoU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Neural Architecture Search#ImageNet#Params	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Instance Segmentation#COCO test-dev#AP50	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Data-to-Text Generation#E2E NLG Challenge#CIDEr	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Speech Recognition#LibriSpeech test-other#Word Error Rate (WER)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semantic Segmentation#PASCAL Context#mIoU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Graph Classification#PTC#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Classification#MNIST#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Visual Question Answering#COCO Visual Question Answering (VQA) real images 1.0 open ended#Percentage correct	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image-to-Image Translation#Cityscapes Labels-to-Photo#Per-pixel Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Fine-Grained Image Classification# CUB-200-2011#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Visual Question Answering#VQA v2 test-dev#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semantic Segmentation#Cityscapes val#mIoU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Generation#CIFAR-10#Inception score	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Click-Through Rate Prediction#MovieLens 20M#AUC	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Super-Resolution#Set5 - 4x upscaling#PSNR	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Automated Theorem Proving#HOList benchmark#Percentage correct	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Text Summarization#arXiv#ROUGE-1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	3D Semantic Segmentation#SemanticKITTI#mIoU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Action Recognition#NTU RGB+D#Accuracy (CV)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Domain Adaptation#UCF-to-HMDBfull#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Lipreading#Lip Reading in the Wild#Top-1 Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Super-Resolution#FFHQ 1024 x 1024 - 4x upscaling#FID	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semantic Segmentation#S3DIS#Mean IoU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semantic Segmentation#ADE20K val#mIoU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Machine Translation#WMT2014 English-German#BLEU score	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Relation Extraction#ACE 2004#Entity F1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Face Detection#Annotated Faces in the Wild#AP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Ad-Hoc Information Retrieval#TREC Robust04#nDCG@20	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Scene Text Detection#Total-Text#Recall	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Language Modelling#WikiText-103#Test perplexity	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Continual Learning#CUBS (Fine-grained 6 Tasks)#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	3D Human Pose Estimation#Human3.6M#Multi-View or Monocular	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Visual Question Answering#VQA v1 test-dev#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Link Prediction#WN18#Hits@3	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Word Sense Disambiguation#SemEval 2007 Task 7#F1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Link Prediction#WN18#Hits@1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Retrieval#Flickr30K 1K test#R@5	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Retrieval#Flickr30K 1K test#R@1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#F-measure (Mean)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Fine-Grained Image Classification#FGVC Aircraft#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Music Source Separation#MUSDB18#SDR (vocals)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Super-Resolution#FFHQ 1024 x 1024 - 4x upscaling#MS-SSIM	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	3D Human Pose Estimation#Human3.6M#Using 2D ground-truth joints	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Data-to-Text Generation#E2E NLG Challenge#METEOR	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Link Prediction#FB15k-237#MRR	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Sentiment Analysis#Multi-Domain Sentiment Dataset#Electronics	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Recommendation Systems#MovieLens 1M#HR@10	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Time Series Classification#PhysioNet Challenge 2012#AUC Stdev	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Drug Discovery#QM9#Error ratio	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Face Verification#CASIA NIR-VIS 2.0#TAR @ FAR=0.001	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Action Recognition#NTU RGB+D#Accuracy (CS)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Graph Regression#Lipophilicity #RMSE	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Scene Text Detection#ICDAR 2017 MLT#Precision	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Face Verification#Labeled Faces in the Wild#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Action Recognition#NTU RGB+D 120#Accuracy (Cross-Subject)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	3D Human Pose Estimation#MPI-INF-3DHP#3DPCK	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#Jaccard (Mean)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Meta-Learning#MT50#Average Success Rate	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image-to-Image Translation#COCO-Stuff Labels-to-Photos#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Scene Text Detection#MSRA-TD500#Recall	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Scene Text Detection#ICDAR 2017 MLT#Recall	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Robotic Grasping#Cornell Grasp Dataset#5 fold cross validation	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Temporal Action Localization#ActivityNet-1.3#mAP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Link Prediction#FB15k-237#Hits@10	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	6D Pose Estimation#YCB-Video#ADDS AUC	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	3D Human Pose Estimation#HumanEva-I#Mean Reconstruction Error (mm)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Visual Question Answering#VCR (QA-R) test#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Pose Estimation#COCO test-dev#AP50	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Shadow Detection#SBU#BER	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#F-measure (Mean)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Dehazing#SOTS Indoor#SSIM	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Clustering#Extended Yale-B#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#F-measure (Recall)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Link Prediction#WN18RR#MRR	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Action Recognition#Something-Something V1#Top 5 Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Object Detection#COCO test-dev#AP75	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Generation#LSUN Bedroom 256 x 256#FID	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#SQuAD2.0 dev#F1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Text Generation#EMNLP2017 WMT#BLEU-5	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Text Generation#EMNLP2017 WMT#BLEU-4	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#SQuAD2.0 dev#EM	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Sentiment Analysis#Multi-Domain Sentiment Dataset#Average	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Natural Language Inference#MultiNLI#Matched	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Dense Pixel Correspondence Estimation#HPatches#Viewpoint V AEPE	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Traffic Prediction#METR-LA#MAE @ 12 step	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Click-Through Rate Prediction#Dianping#Log Loss	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Person Re-Identification#PRID2011#Rank-20	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Speech Recognition#LibriSpeech test-clean#Word Error Rate (WER)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Text Generation#EMNLP2017 WMT#BLEU-3	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Text Generation#EMNLP2017 WMT#BLEU-2	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Neural Architecture Search#ImageNet#FLOPS	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Cross-Lingual Bitext Mining#BUCC German-to-English#F1 score	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Scene Text Detection#ICDAR 2017 MLT#F-Measure	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Metric Learning#CARS196#R@1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Code Generation#WikiSQL#Exact Match Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Scene Text Detection#Total-Text#F-Measure	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Classification#STL-10#Percentage correct	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Facial Expression Recognition#FER2013#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Graph Classification#PROTEINS#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Neural Architecture Search#CIFAR-10 Image Classification#Percentage error	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#SQuAD1.1 dev#F1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Scene Text Detection#ICDAR 2015#F-Measure	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Speech Recognition#Switchboard + Hub500#Percentage error	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#SQuAD1.1 dev#EM	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Few-Shot Image Classification#Mini-Imagenet 5-way (5-shot)#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Classification#CIFAR-10#Percentage correct	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Neural Architecture Search#CIFAR-10 Image Classification#Params	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	3D Human Pose Estimation#Surreal#MPJPE	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Facial Landmark Detection#300W#NME	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Multi-Object Tracking#MOT16#MOTA	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Language Modelling#Text8#Number of params	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#Jaccard (Decay)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Action Classification#HMDB51#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Reading Comprehension#RACE#Accuracy (High)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Person Re-Identification#Market-1501#Rank-1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#Jaccard (Recall)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Generation#Fashion-MNIST#FID	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Data-to-Text Generation#E2E NLG Challenge#ROUGE-L	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#F-measure (Recall)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Object Detection#COCO minival#APL	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Object Detection#COCO minival#APM	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Generation#CIFAR-10#FID	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Temporal Action Localization#CrossTask#Recall	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Generation#MNIST#bits/dimension	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Pose Estimation#COCO test-dev#AP75	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Temporal Action Localization#J-HMDB-21#Frame-mAP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Object Detection#COCO minival#APS	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Named Entity Recognition#GENIA#F1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Super-Resolution#FFHQ 1024 x 1024 - 4x upscaling#PSNR	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Reading Comprehension#RACE#Accuracy (Middle)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Object Detection#COCO test-dev#AP50	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#Jaccard (Recall)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Conversational Response Selection#Ubuntu Dialogue (v1, Ranking)#R2@1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Person Re-Identification#DukeMTMC-reID#Rank-1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Ad-Hoc Information Retrieval#TREC Robust04#MAP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semantic Role Labeling#CoNLL 2005#F1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Person Re-Identification#DukeMTMC-reID#Rank-5	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Music Source Separation#MUSDB18#SDR (bass)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Language Modelling#Penn Treebank (Word Level)#Params	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Object Detection#PASCAL VOC 2007#MAP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#CNN / Daily Mail#CNN	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#TrecQA#MAP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Common Sense Reasoning#CommonsenseQA#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Retrieval#CARS196#R@1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Sentiment Analysis#Yelp Fine-grained classification#Error	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Action Recognition#NTU RGB+D 120#Accuracy (Cross-Setup)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Link Prediction#WN18#MR	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Salient Object Detection#DUTS-TE#MAE	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Scene Text Detection#ICDAR 2013#F-Measure	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Generation#SQuAD1.1#BLEU-4	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Neural Architecture Search#CIFAR-10#Top-1 Error Rate	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Action Recognition#Sports-1M#Video hit@1 	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Scene Text Detection#ICDAR 2015#Precision	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Object Counting#CARPK#MAE	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	3D Object Reconstruction#Data3DâR2N2#3DIoU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Dependency Parsing#Penn Treebank#LAS	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	3D Object Reconstruction#Data3DâR2N2#Avg F1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Hand Pose Estimation#NYU Hands#Average 3D Error	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Conversational Response Selection#Ubuntu Dialogue (v1, Ranking)#R10@5	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#CoQA#Out-of-domain	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Conversational Response Selection#Ubuntu Dialogue (v1, Ranking)#R10@1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Conversational Response Selection#Ubuntu Dialogue (v1, Ranking)#R10@2	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#F-measure (Decay)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Optical Flow Estimation#KITTI 2012#Average End-Point Error	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Domain Adaptation#ImageCLEF-DA#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Aspect-Based Sentiment Analysis#SemEval 2014 Task 4 Sub Task 2#Mean Acc (Restaurant + Laptop)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#CoQA#Overall	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Text Generation#Yahoo Questions#KL	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Line Segment Detection#York Urban Dataset#sAP5	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image-to-Image Translation#Cityscapes Labels-to-Photo#Class IOU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Part-Of-Speech Tagging#Penn Treebank#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#J&F	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Object Counting#Pascal VOC 2007 count-test#m-relRMSE	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semantic Segmentation#Cityscapes test#Mean IoU (class)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Natural Language Inference#SNLI#% Train Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Person Re-Identification#MARS#Rank-5	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Clustering#MNIST-test#NMI	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Sentiment Analysis#SST-2 Binary classification#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Link Prediction#FB15k-237#MR	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Person Re-Identification#MARS#Rank-1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Human-Object Interaction Detection#HICO-DET#MAP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Text Classification#R8#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Person Re-Identification#Market-1501->DukeMTMC-reID#Rank-1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Time Series Classification#PhysioNet Challenge 2012#AUC	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Node Classification#Wikipedia#Macro-F1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Facial Expression Recognition#AffectNet#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#Quora Question Pairs#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Atari Games#Atari 2600 Q*Bert#Score	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Data-to-Text Generation#MULTIWOZ 2.1#BLEU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Speech Synthesis#North American English#Mean Opinion Score	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Text Summarization#GigaWord#ROUGE-L	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Retrieval#Flickr30K 1K test#R@10	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Reconstruction#Edge-to-Handbags#FID	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Natural Language Inference#SNLI#% Test Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Text Summarization#GigaWord#ROUGE-1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Text Summarization#GigaWord#ROUGE-2	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Entity Typing#Ontonotes v5 (English)#F1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Click-Through Rate Prediction#Criteo#Log Loss	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Named Entity Recognition#Ontonotes v5 (English)#F1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Dependency Parsing#Penn Treebank#POS	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Domain Adaptation#USPS-to-MNIST#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Object Detection#COCO test-dev#APS	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Optical Flow Estimation#Sintel-clean#Average End-Point Error	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Link Prediction#YAGO3-10#MRR	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Object Detection#KITTI Cars Moderate#AP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#bAbi#Mean Error Rate	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Click-Through Rate Prediction#Criteo#AUC	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Scene Text Detection#ICDAR 2015#Recall	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Entity Typing#Ontonotes v5 (English)#Precision	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Lipreading#GRID corpus (mixed-speech)#Word Error Rate (WER)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#F-measure (Decay)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Relation Extraction#CoNLL04#Relation F1 	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Clustering#MNIST-test#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Generation#Stanford Dogs#Inception score	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Dense Pixel Correspondence Estimation#HPatches#Viewpoint IV AEPE	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Subjectivity Analysis#SUBJ#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Classification#ImageNet#Number of params	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#bAbi#Accuracy (trained on 10k)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	3D Human Pose Estimation#MPI-INF-3DHP#MJPE	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Speech Recognition#TIMIT#Percentage error	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Part-Of-Speech Tagging#UD#Avg accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Classification#ImageNet#Top 5 Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Clustering#CIFAR-10#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Object Detection#COCO test-dev#APM	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Object Detection#COCO test-dev#APL	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#MS MARCO#Rouge-L	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Graph Classification#NEURON-BINARY#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Language Modelling#Penn Treebank (Word Level)#Test perplexity	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Pose Tracking#PoseTrack2017#MOTA	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image-to-Image Translation#SYNTHIA-to-Cityscapes#mIoU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Visual Question Answering#MSVD-QA#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Relation Extraction#CoNLL04#Entity F1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Motion Segmentation#Hopkins155#Classification Error	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Person Re-Identification#DukeMTMC-reID#MAP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Pose Estimation#MPII Human Pose#PCKh-0.5	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Graph Classification#NCI1#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Named Entity Recognition#SciERC#F1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Scene Text Detection#MSRA-TD500#Precision	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	3D Human Pose Estimation#3DPW#PA-MPJPE	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Classification#Kuzushiji-MNIST#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Language Modelling#Text8#Bit per Character (BPC)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image-to-Image Translation#COCO-Stuff Labels-to-Photos#FID	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Super-Resolution#Urban100 - 8x upscaling#PSNR	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semantic Segmentation#LIP val#mIoU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Data-to-Text Generation#E2E NLG Challenge#NIST	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Sentiment Analysis#Multi-Domain Sentiment Dataset#DVD	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Domain Adaptation#Office-Caltech#Average Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Relation Extraction#DocRED#F1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#MS MARCO#BLEU-1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Music Source Separation#MUSDB18#SDR (drums)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Machine Translation#WMT2016 English-German#BLEU score	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Face Verification#CK+#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Language Modelling#Penn Treebank (Word Level)#Validation perplexity	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#J&F	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Relation Extraction#NYT#F1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Generation#ImageNet 64x64#Bits per dim	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semantic Segmentation#ScanNet#3DIoU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Dense Pixel Correspondence Estimation#HPatches#Viewpoint I AEPE	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Few-Shot Image Classification#Mini-ImageNet-CUB 5-way (5-shot)#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Language Modelling#WikiText-103#Validation perplexity	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Neural Architecture Search#NAS-Bench-201, ImageNet-16-120#Accuracy (Test)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Object Detection#COCO minival#AP50	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Few-Shot Image Classification#CIFAR-FS 5-way (5-shot)#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Object Counting#Pascal VOC 2007 count-test#mRMSE-nz	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Action Classification#Kinetics-400#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	3D Object Detection#ScanNetV2#mAP@0.5	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Instance Segmentation#COCO test-dev#mask AP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Named Entity Recognition#ACE 2005#F1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Clustering#MNIST-full#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Object Detection#COCO test-dev#box AP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image-to-Image Translation#RaFD#Classification Error	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Sentiment Analysis#IMDb#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#WikiQA#MAP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image-to-Image Translation#COCO-Stuff Labels-to-Photos#mIoU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Node Classification#Cora with Public Split: fixed 20 nodes per class#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Instance Segmentation#COCO test-dev#APS	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Generation#Binarized MNIST#nats	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Instance Segmentation#COCO test-dev#APM	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Instance Segmentation#COCO test-dev#APL	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Passage Re-Ranking#MS MARCO#MRR	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Entity Alignment#DBP15k zh-en#Hits@1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Object Detection#COCO minival#AP75	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	3D Object Detection#ScanNetV2#mAP@0.25	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Causal Inference#IDHP#Average Treatment Effect Error	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Text Summarization#DUC 2004 Task 1#ROUGE-1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Text Summarization#DUC 2004 Task 1#ROUGE-2	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Link Prediction#YAGO3-10#Hits@10	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Pose Estimation#COCO test-dev#AR	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Pose Estimation#COCO test-dev#AP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#Children's Book Test#Accuracy-NE	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Dense Pixel Correspondence Estimation#HPatches#Viewpoint III AEPE	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Action Recognition#Sports-1M#Video hit@5	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Natural Language Inference#MultiNLI#Mismatched	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Dependency Parsing#Penn Treebank#UAS	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Text Summarization#DUC 2004 Task 1#ROUGE-L	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Human Part Segmentation#PASCAL-Person-Part#mIoU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Generation#LSUN Churches 256 x 256#FID	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Action Recognition#HMDB-51#Average accuracy of 3 splits	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Relation Extraction#ACE 2004#Entity+Relation F1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Language Modelling#WikiText-2#Validation perplexity	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image-to-Image Translation#Cityscapes Labels-to-Photo#Per-class Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Text Generation#Yahoo Questions#Perplexity	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Machine Translation#IWSLT2014 German-English#BLEU score	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Dehazing#SOTS Indoor#PSNR	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Recommendation Systems#Netflix#nDCG@100	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Classification#ImageNet#Top 1 Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Data-to-Text Generation#E2E NLG Challenge#BLEU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Ad-Hoc Information Retrieval#TREC Robust04#P@20	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Graph Classification#NCI109#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Natural Language Inference#SciTail#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Drug Discovery#Tox21#AUC	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Fake News Detection#FNC-1#Weighted Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#Jaccard (Mean)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Face Verification#CFP-FP#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Scene Text Detection#SCUT-CTW1500#F-Measure	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Named Entity Recognition#CoNLL 2003 (English)#F1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Action Recognition#Something-Something V1#Top 1 Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Optical Character Recognition#FSNS - Test#Sequence error	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Link Prediction#FB15k-237#Hits@3	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Link Prediction#FB15k-237#Hits@1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image-to-Image Translation#Cityscapes-to-Foggy Cityscapes#mAP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#TrecQA#MRR	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Generation#CIFAR-10#bits/dimension	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Horizon Line Estimation#York Urban Dataset#AUC (horizon error)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	3D Object Detection#KITTI Pedestrians Moderate#AP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Speech Recognition#swb_hub_500 WER fullSWBCH#Percentage error	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Neural Architecture Search#NAS-Bench-201, ImageNet-16-120#Accuracy (val)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Depth Completion#KITTI Depth Completion#MAE	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Machine Translation#IWSLT2015 German-English#BLEU score	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Person Re-Identification#Market-1501#MAP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Classification#EMNIST-Letters#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Face Detection#WIDER Face (Hard)#AP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Text Style Transfer#Yelp Review Dataset (Small)#G-Score (BLEU, Accuracy)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Extractive Document Summarization#CNN / Daily Mail#ROUGE-2	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Super-Resolution#PIRM-test#NIQE	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Extractive Document Summarization#CNN / Daily Mail#ROUGE-1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Link Prediction#Cora#AP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image-to-Image Translation#GTAV-to-Cityscapes Labels#mIoU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#QASent#MRR	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Aspect-Based Sentiment Analysis#SemEval 2014 Task 4 Sub Task 2#Laptop (Acc)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#WebQuestions#F1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Few-Shot Image Classification#OMNIGLOT - 1-Shot, 5-way#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Few-Shot Semantic Segmentation#FSS-1000#Mean IoU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Scene Text Detection#ICDAR 2013#Recall	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Lipreading#LRW-1000#Top-1 Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Language Modelling#WikiText-2#Test perplexity	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Scene Text Detection#SCUT-CTW1500#Recall	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Graph Classification#IMDb-B#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	3D Human Pose Estimation#Total Capture#Average MPJPE (mm)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Salient Object Detection#DUTS-TE#F-measure	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.6	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.5	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Multi-tissue Nucleus Segmentation#Kumar#Hausdorff Distance (mm)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.7	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.2	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.4	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.3	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Super-Resolution#FFHQ 1024 x 1024 - 4x upscaling#SSIM	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Face Verification#YouTube Faces DB#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Link Prediction#Cora#AUC	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Document Image Classification#RVL-CDIP#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Semantic Segmentation#PASCAL VOC 2012 val#mIoU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Atari Games#Atari 2600 Breakout#Score	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Sentiment Analysis#Multi-Domain Sentiment Dataset#Kitchen	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Super-Resolution#VggFace2 - 8x upscaling#PSNR	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Click-Through Rate Prediction#Dianping#AUC	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#bAbi#Accuracy (trained on 1k)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Object Counting#CARPK#RMSE	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Retrieval#Oxf5k#MAP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#Children's Book Test#Accuracy-CN	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Super-Resolution#Set5 - 4x upscaling#SSIM	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Recommendation Systems#MovieLens 1M#RMSE	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Sentiment Analysis#MR#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Video Generation#UCF-101 16 frames, Unconditional, Single GPU#Inception Score	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Atari Games#Atari 2600 Fishing Derby#Score	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Dehazing#O-Haze#PSNR	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Language Modelling#enwik8#Number of params	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Dense Pixel Correspondence Estimation#HPatches#Viewpoint II AEPE	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Machine Translation#WMT2014 English-German#SacreBLEU	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Human action generation#Human3.6M#MMD	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Generation#Stanford Dogs#FID	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Domain Adaptation#Office-31#Average Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Clustering#MNIST-full#NMI	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Constituency Parsing#Penn Treebank#F1 score	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Image Reconstruction#Edge-to-Handbags#LPIPS	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Language Modelling#enwik8#Bit per Character (BPC)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Reading Comprehension#RACE#Accuracy	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Text Generation#Yahoo Questions#NLL	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Object Detection#COCO minival#box AP	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Anomaly Detection#CIFAR-10 model detecting CIFAR-10#AUPR	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Music Source Separation#MUSDB18#SDR (other)	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Scene Text Detection#SCUT-CTW1500#Precision	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#NewsQA#EM	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Speech Separation#wsj0-2mix#SI-SDRi	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Language Modelling#One Billion Word#PPL	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Coreference Resolution#OntoNotes#F1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	Question Answering#NewsQA#F1	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	1910.10750v1.pdf	unknown	6-PACK: Category-level 6D Pose Tracker with Anchor-Based Keypoints Change of 6D Pose between frames We present 6-PACK, a deep learning approach to category-level 6D object pose tracking on RGB-D data. Our method tracks in real time novel object instances of known object categories such as bowls, laptops, and mugs. 6-PACK learns to compactly represent an object by a handful of 3D keypoints, based on which the interframe motion of an object instance can be estimated through keypoint matching. These keypoints are learned end-to-end without manual supervision in order to be most effective for tracking. Our experiments show that our method substantially outperforms existing methods on the NOCS category-level 6D pose estimation benchmark and supports a physical robot to perform simple vision-based closed-loop manipulation tasks. Our code and video are available at https://sites.google.com/view/6packtracking.
true	2002.03651v4.pdf	Question Answering#SQuAD1.1#F1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Human-Object Interaction Detection#HICO#mAP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Node Classification#Wikipedia#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#SQuAD1.1#EM	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Visual Question Answering#VQA v2 test-std#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semantic Segmentation#PASCAL VOC 2012 test#Mean IoU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#YahooCQA#P@1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semantic Segmentation#ShapeNet#Mean IoU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Pose Estimation#UPenn Action#Mean PCK@0.2	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Person Re-Identification#PRID2011#Rank-5	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Neural Architecture Search#ImageNet#Top-1 Error Rate	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Person Re-Identification#PRID2011#Rank-1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Classification#MNIST#Percentage error	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#WikiQA#MRR	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Code Generation#WikiSQL#Execution Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#Jaccard (Decay)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#CoQA#In-domain	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Temporal Action Localization#ActivityNet-1.3#mAP IOU@0.5	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Text Classification#IMDb#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Super-Resolution#Urban100 - 8x upscaling#SSIM	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Action Detection#UCF101-24#Video-mAP 0.2	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Language Modelling#One Billion Word#Number of params	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Neural Architecture Search#ImageNet#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Classification#Clothing1M#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Classification#CIFAR-100#Percentage correct	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Machine Translation#WMT2014 German-English#BLEU score	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Object Counting#Pascal VOC 2007 count-test#m-reIRMSE-nz	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Instance Segmentation#Cityscapes test#Average Precision	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Action Classification#Charades#MAP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Scene Text Detection#Total-Text#Precision	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Link Prediction#YAGO3-10#Hits@3	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Link Prediction#YAGO3-10#Hits@1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Incremental Learning#CIFAR-100 - 50 classes + 10 steps of 5 classes#Average Incremental Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Trajectory Prediction#ETH/UCY#ADE-8/12	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Language Modelling#WikiText-2#Number of params	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Scene Text Detection#MSRA-TD500#F-Measure	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Neural Architecture Search#CIFAR-10#Search Time (GPU days)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Action Recognition#UCF101#3-fold Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Clustering#CIFAR-10#NMI	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Scene Text Detection#ICDAR 2013#Precision	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Node Classification#Pubmed#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Breast Tumour Classification#PCam#AUC	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Recommendation Systems#MovieLens 1M#nDCG@10	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Line Segment Detection#York Urban Dataset#sAP10	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Visual Question Answering#VQA-CP#Score	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Machine Translation#IWSLT2015 English-Vietnamese#BLEU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	3D Reconstruction#Data3DâR2N2#3DIoU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#WikiHop#Test	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Data-to-Text Generation#Rotowire (Content Selection)#Recall	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Atari Games#Atari 2600 Freeway#Score	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semantic Segmentation#NYU Depth v2#Mean IoU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Person Re-Identification#MARS#mAP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Link Prediction#WN18#MRR	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	3D Object Detection#nuScenes#NDS	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Drug Discovery#ToxCast#AUC	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Pose Estimation# ITOP front-view#Mean mAP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Classification#Tiered ImageNet 5-way (5-shot)#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Neural Architecture Search#NAS-Bench-201, ImageNet-16-120#Search time (s)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#YahooCQA#MRR	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Sentiment Analysis#Multi-Domain Sentiment Dataset#Books	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Relation Extraction#TACRED#F1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Instance Segmentation#COCO test-dev#AP75	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Aspect-Based Sentiment Analysis#SemEval 2014 Task 4 Sub Task 2#Restaurant (Acc)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Recommendation Systems#Netflix#Recall@50	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semantic Segmentation#Semantic3D#mIoU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	3D Human Pose Estimation#Human3.6M#Average MPJPE (mm)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Denoising#Darmstadt Noise Dataset#PSNR	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Speech Enhancement#DEMAND#PESQ	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Object Counting#Pascal VOC 2007 count-test#mRMSE	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Recommendation Systems#Netflix#Recall@20	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Pose Estimation#COCO test-dev#APL	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Multi-Label Classification#MS-COCO#mAP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Pose Estimation#COCO test-dev#APM	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Generation#CIFAR-10#Model Entropy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Classification#SVHN#Percentage error	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Machine Translation#WMT2014 English-French#BLEU score	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Temporal Action Localization#ActivityNet-1.3#mAP IOU@0.95	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Human Interaction Recognition#UT#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Node Classification#Reddit#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Face Verification#IJB-A#TAR @ FAR=0.01	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	3D Human Pose Estimation#MPI-INF-3DHP#AUC	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Natural Language Inference#SNLI#Parameters	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Depth Completion#KITTI Depth Completion#RMSE	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Lipreading#LRS2#Word Error Rate (WER)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Multi-tissue Nucleus Segmentation#Kumar#Dice	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Link Prediction#WN18#Hits@10	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Person Re-Identification#Market-1501->DukeMTMC-reID#mAP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Visual Question Answering#GQA test-std#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Entity Typing#Ontonotes v5 (English)#Recall	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Action Recognition#THUMOSâ14#mAP@0.5	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Action Recognition#THUMOSâ14#mAP@0.4	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Matting#Composition-1K#MSE	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Action Recognition#THUMOSâ14#mAP@0.3	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Temporal Action Localization#ActivityNet-1.3#mAP IOU@0.75	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Data-to-Text Generation#Rotowire (Content Selection)#Precision	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Atari Games#Atari 2600 Montezuma's Revenge#Score	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Action Recognition#Jester#Val	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Few-Shot Image Classification#Mini-ImageNet - 1-Shot Learning#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Classification#CIFAR-100#Percentage error	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Action Recognition#Something-Something V2#Top-1 Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#QASent#MAP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semantic Segmentation#CamVid#Mean IoU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Neural Architecture Search#ImageNet#Params	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Instance Segmentation#COCO test-dev#AP50	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Data-to-Text Generation#E2E NLG Challenge#CIDEr	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Speech Recognition#LibriSpeech test-other#Word Error Rate (WER)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semantic Segmentation#PASCAL Context#mIoU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Graph Classification#PTC#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Classification#MNIST#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Visual Question Answering#COCO Visual Question Answering (VQA) real images 1.0 open ended#Percentage correct	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image-to-Image Translation#Cityscapes Labels-to-Photo#Per-pixel Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Fine-Grained Image Classification# CUB-200-2011#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Visual Question Answering#VQA v2 test-dev#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semantic Segmentation#Cityscapes val#mIoU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Generation#CIFAR-10#Inception score	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Click-Through Rate Prediction#MovieLens 20M#AUC	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Super-Resolution#Set5 - 4x upscaling#PSNR	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Automated Theorem Proving#HOList benchmark#Percentage correct	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Text Summarization#arXiv#ROUGE-1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	3D Semantic Segmentation#SemanticKITTI#mIoU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Action Recognition#NTU RGB+D#Accuracy (CV)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Domain Adaptation#UCF-to-HMDBfull#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Lipreading#Lip Reading in the Wild#Top-1 Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Super-Resolution#FFHQ 1024 x 1024 - 4x upscaling#FID	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semantic Segmentation#S3DIS#Mean IoU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semantic Segmentation#ADE20K val#mIoU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Machine Translation#WMT2014 English-German#BLEU score	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Relation Extraction#ACE 2004#Entity F1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Face Detection#Annotated Faces in the Wild#AP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Ad-Hoc Information Retrieval#TREC Robust04#nDCG@20	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Scene Text Detection#Total-Text#Recall	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Language Modelling#WikiText-103#Test perplexity	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Continual Learning#CUBS (Fine-grained 6 Tasks)#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	3D Human Pose Estimation#Human3.6M#Multi-View or Monocular	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Visual Question Answering#VQA v1 test-dev#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Link Prediction#WN18#Hits@3	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Word Sense Disambiguation#SemEval 2007 Task 7#F1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Link Prediction#WN18#Hits@1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Retrieval#Flickr30K 1K test#R@5	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Retrieval#Flickr30K 1K test#R@1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#F-measure (Mean)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Fine-Grained Image Classification#FGVC Aircraft#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Music Source Separation#MUSDB18#SDR (vocals)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Super-Resolution#FFHQ 1024 x 1024 - 4x upscaling#MS-SSIM	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	3D Human Pose Estimation#Human3.6M#Using 2D ground-truth joints	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Data-to-Text Generation#E2E NLG Challenge#METEOR	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Link Prediction#FB15k-237#MRR	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Sentiment Analysis#Multi-Domain Sentiment Dataset#Electronics	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Recommendation Systems#MovieLens 1M#HR@10	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Time Series Classification#PhysioNet Challenge 2012#AUC Stdev	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Drug Discovery#QM9#Error ratio	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Face Verification#CASIA NIR-VIS 2.0#TAR @ FAR=0.001	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Action Recognition#NTU RGB+D#Accuracy (CS)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Graph Regression#Lipophilicity #RMSE	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Scene Text Detection#ICDAR 2017 MLT#Precision	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Face Verification#Labeled Faces in the Wild#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Action Recognition#NTU RGB+D 120#Accuracy (Cross-Subject)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	3D Human Pose Estimation#MPI-INF-3DHP#3DPCK	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#Jaccard (Mean)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Meta-Learning#MT50#Average Success Rate	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image-to-Image Translation#COCO-Stuff Labels-to-Photos#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Scene Text Detection#MSRA-TD500#Recall	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Scene Text Detection#ICDAR 2017 MLT#Recall	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Robotic Grasping#Cornell Grasp Dataset#5 fold cross validation	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Temporal Action Localization#ActivityNet-1.3#mAP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Link Prediction#FB15k-237#Hits@10	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	6D Pose Estimation#YCB-Video#ADDS AUC	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	3D Human Pose Estimation#HumanEva-I#Mean Reconstruction Error (mm)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Visual Question Answering#VCR (QA-R) test#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Pose Estimation#COCO test-dev#AP50	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Shadow Detection#SBU#BER	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#F-measure (Mean)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Dehazing#SOTS Indoor#SSIM	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Clustering#Extended Yale-B#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#F-measure (Recall)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Link Prediction#WN18RR#MRR	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Action Recognition#Something-Something V1#Top 5 Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Object Detection#COCO test-dev#AP75	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Generation#LSUN Bedroom 256 x 256#FID	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#SQuAD2.0 dev#F1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Text Generation#EMNLP2017 WMT#BLEU-5	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Text Generation#EMNLP2017 WMT#BLEU-4	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#SQuAD2.0 dev#EM	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Sentiment Analysis#Multi-Domain Sentiment Dataset#Average	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Natural Language Inference#MultiNLI#Matched	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Dense Pixel Correspondence Estimation#HPatches#Viewpoint V AEPE	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Traffic Prediction#METR-LA#MAE @ 12 step	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Click-Through Rate Prediction#Dianping#Log Loss	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Person Re-Identification#PRID2011#Rank-20	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Speech Recognition#LibriSpeech test-clean#Word Error Rate (WER)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Text Generation#EMNLP2017 WMT#BLEU-3	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Text Generation#EMNLP2017 WMT#BLEU-2	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Neural Architecture Search#ImageNet#FLOPS	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Cross-Lingual Bitext Mining#BUCC German-to-English#F1 score	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Scene Text Detection#ICDAR 2017 MLT#F-Measure	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Metric Learning#CARS196#R@1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Code Generation#WikiSQL#Exact Match Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Scene Text Detection#Total-Text#F-Measure	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Classification#STL-10#Percentage correct	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Facial Expression Recognition#FER2013#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Graph Classification#PROTEINS#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Neural Architecture Search#CIFAR-10 Image Classification#Percentage error	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#SQuAD1.1 dev#F1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Scene Text Detection#ICDAR 2015#F-Measure	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Speech Recognition#Switchboard + Hub500#Percentage error	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#SQuAD1.1 dev#EM	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Few-Shot Image Classification#Mini-Imagenet 5-way (5-shot)#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Classification#CIFAR-10#Percentage correct	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Neural Architecture Search#CIFAR-10 Image Classification#Params	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	3D Human Pose Estimation#Surreal#MPJPE	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Facial Landmark Detection#300W#NME	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Multi-Object Tracking#MOT16#MOTA	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Language Modelling#Text8#Number of params	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#Jaccard (Decay)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Action Classification#HMDB51#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Reading Comprehension#RACE#Accuracy (High)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Person Re-Identification#Market-1501#Rank-1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#Jaccard (Recall)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Generation#Fashion-MNIST#FID	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Data-to-Text Generation#E2E NLG Challenge#ROUGE-L	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#F-measure (Recall)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Object Detection#COCO minival#APL	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Object Detection#COCO minival#APM	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Generation#CIFAR-10#FID	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Temporal Action Localization#CrossTask#Recall	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Generation#MNIST#bits/dimension	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Pose Estimation#COCO test-dev#AP75	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Temporal Action Localization#J-HMDB-21#Frame-mAP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Object Detection#COCO minival#APS	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Named Entity Recognition#GENIA#F1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Super-Resolution#FFHQ 1024 x 1024 - 4x upscaling#PSNR	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Reading Comprehension#RACE#Accuracy (Middle)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Object Detection#COCO test-dev#AP50	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#Jaccard (Recall)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Conversational Response Selection#Ubuntu Dialogue (v1, Ranking)#R2@1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Person Re-Identification#DukeMTMC-reID#Rank-1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Ad-Hoc Information Retrieval#TREC Robust04#MAP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semantic Role Labeling#CoNLL 2005#F1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Person Re-Identification#DukeMTMC-reID#Rank-5	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Music Source Separation#MUSDB18#SDR (bass)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Language Modelling#Penn Treebank (Word Level)#Params	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Object Detection#PASCAL VOC 2007#MAP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#CNN / Daily Mail#CNN	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#TrecQA#MAP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Common Sense Reasoning#CommonsenseQA#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Retrieval#CARS196#R@1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Sentiment Analysis#Yelp Fine-grained classification#Error	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Action Recognition#NTU RGB+D 120#Accuracy (Cross-Setup)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Link Prediction#WN18#MR	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Salient Object Detection#DUTS-TE#MAE	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Scene Text Detection#ICDAR 2013#F-Measure	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Generation#SQuAD1.1#BLEU-4	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Neural Architecture Search#CIFAR-10#Top-1 Error Rate	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Action Recognition#Sports-1M#Video hit@1 	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Scene Text Detection#ICDAR 2015#Precision	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Object Counting#CARPK#MAE	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	3D Object Reconstruction#Data3DâR2N2#3DIoU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Dependency Parsing#Penn Treebank#LAS	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	3D Object Reconstruction#Data3DâR2N2#Avg F1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Hand Pose Estimation#NYU Hands#Average 3D Error	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Conversational Response Selection#Ubuntu Dialogue (v1, Ranking)#R10@5	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#CoQA#Out-of-domain	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Conversational Response Selection#Ubuntu Dialogue (v1, Ranking)#R10@1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Conversational Response Selection#Ubuntu Dialogue (v1, Ranking)#R10@2	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#F-measure (Decay)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Optical Flow Estimation#KITTI 2012#Average End-Point Error	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Domain Adaptation#ImageCLEF-DA#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Aspect-Based Sentiment Analysis#SemEval 2014 Task 4 Sub Task 2#Mean Acc (Restaurant + Laptop)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#CoQA#Overall	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Text Generation#Yahoo Questions#KL	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Line Segment Detection#York Urban Dataset#sAP5	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image-to-Image Translation#Cityscapes Labels-to-Photo#Class IOU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Part-Of-Speech Tagging#Penn Treebank#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#J&F	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Object Counting#Pascal VOC 2007 count-test#m-relRMSE	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semantic Segmentation#Cityscapes test#Mean IoU (class)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Natural Language Inference#SNLI#% Train Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Person Re-Identification#MARS#Rank-5	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Clustering#MNIST-test#NMI	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Sentiment Analysis#SST-2 Binary classification#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Link Prediction#FB15k-237#MR	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Person Re-Identification#MARS#Rank-1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Human-Object Interaction Detection#HICO-DET#MAP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Text Classification#R8#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Person Re-Identification#Market-1501->DukeMTMC-reID#Rank-1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Time Series Classification#PhysioNet Challenge 2012#AUC	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Node Classification#Wikipedia#Macro-F1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Facial Expression Recognition#AffectNet#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#Quora Question Pairs#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Atari Games#Atari 2600 Q*Bert#Score	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Data-to-Text Generation#MULTIWOZ 2.1#BLEU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Speech Synthesis#North American English#Mean Opinion Score	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Text Summarization#GigaWord#ROUGE-L	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Retrieval#Flickr30K 1K test#R@10	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Reconstruction#Edge-to-Handbags#FID	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Natural Language Inference#SNLI#% Test Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Text Summarization#GigaWord#ROUGE-1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Text Summarization#GigaWord#ROUGE-2	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Entity Typing#Ontonotes v5 (English)#F1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Click-Through Rate Prediction#Criteo#Log Loss	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Named Entity Recognition#Ontonotes v5 (English)#F1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Dependency Parsing#Penn Treebank#POS	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Domain Adaptation#USPS-to-MNIST#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Object Detection#COCO test-dev#APS	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Optical Flow Estimation#Sintel-clean#Average End-Point Error	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Link Prediction#YAGO3-10#MRR	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Object Detection#KITTI Cars Moderate#AP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#bAbi#Mean Error Rate	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Click-Through Rate Prediction#Criteo#AUC	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Scene Text Detection#ICDAR 2015#Recall	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Entity Typing#Ontonotes v5 (English)#Precision	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Lipreading#GRID corpus (mixed-speech)#Word Error Rate (WER)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#F-measure (Decay)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Relation Extraction#CoNLL04#Relation F1 	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Clustering#MNIST-test#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Generation#Stanford Dogs#Inception score	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Dense Pixel Correspondence Estimation#HPatches#Viewpoint IV AEPE	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Subjectivity Analysis#SUBJ#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Classification#ImageNet#Number of params	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#bAbi#Accuracy (trained on 10k)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	3D Human Pose Estimation#MPI-INF-3DHP#MJPE	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Speech Recognition#TIMIT#Percentage error	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Part-Of-Speech Tagging#UD#Avg accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Classification#ImageNet#Top 5 Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Clustering#CIFAR-10#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Object Detection#COCO test-dev#APM	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Object Detection#COCO test-dev#APL	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#MS MARCO#Rouge-L	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Graph Classification#NEURON-BINARY#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Language Modelling#Penn Treebank (Word Level)#Test perplexity	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Pose Tracking#PoseTrack2017#MOTA	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image-to-Image Translation#SYNTHIA-to-Cityscapes#mIoU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Visual Question Answering#MSVD-QA#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Relation Extraction#CoNLL04#Entity F1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Motion Segmentation#Hopkins155#Classification Error	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Person Re-Identification#DukeMTMC-reID#MAP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Pose Estimation#MPII Human Pose#PCKh-0.5	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Graph Classification#NCI1#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Named Entity Recognition#SciERC#F1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Scene Text Detection#MSRA-TD500#Precision	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	3D Human Pose Estimation#3DPW#PA-MPJPE	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Classification#Kuzushiji-MNIST#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Language Modelling#Text8#Bit per Character (BPC)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image-to-Image Translation#COCO-Stuff Labels-to-Photos#FID	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Super-Resolution#Urban100 - 8x upscaling#PSNR	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semantic Segmentation#LIP val#mIoU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Data-to-Text Generation#E2E NLG Challenge#NIST	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Sentiment Analysis#Multi-Domain Sentiment Dataset#DVD	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Domain Adaptation#Office-Caltech#Average Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Relation Extraction#DocRED#F1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#MS MARCO#BLEU-1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Music Source Separation#MUSDB18#SDR (drums)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Machine Translation#WMT2016 English-German#BLEU score	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Face Verification#CK+#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Language Modelling#Penn Treebank (Word Level)#Validation perplexity	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#J&F	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Relation Extraction#NYT#F1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Generation#ImageNet 64x64#Bits per dim	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semantic Segmentation#ScanNet#3DIoU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Dense Pixel Correspondence Estimation#HPatches#Viewpoint I AEPE	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Few-Shot Image Classification#Mini-ImageNet-CUB 5-way (5-shot)#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Language Modelling#WikiText-103#Validation perplexity	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Neural Architecture Search#NAS-Bench-201, ImageNet-16-120#Accuracy (Test)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Object Detection#COCO minival#AP50	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Few-Shot Image Classification#CIFAR-FS 5-way (5-shot)#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Object Counting#Pascal VOC 2007 count-test#mRMSE-nz	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Action Classification#Kinetics-400#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	3D Object Detection#ScanNetV2#mAP@0.5	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Instance Segmentation#COCO test-dev#mask AP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Named Entity Recognition#ACE 2005#F1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Clustering#MNIST-full#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Object Detection#COCO test-dev#box AP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image-to-Image Translation#RaFD#Classification Error	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Sentiment Analysis#IMDb#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#WikiQA#MAP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image-to-Image Translation#COCO-Stuff Labels-to-Photos#mIoU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Node Classification#Cora with Public Split: fixed 20 nodes per class#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Instance Segmentation#COCO test-dev#APS	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Generation#Binarized MNIST#nats	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Instance Segmentation#COCO test-dev#APM	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Instance Segmentation#COCO test-dev#APL	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Passage Re-Ranking#MS MARCO#MRR	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Entity Alignment#DBP15k zh-en#Hits@1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Object Detection#COCO minival#AP75	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	3D Object Detection#ScanNetV2#mAP@0.25	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Causal Inference#IDHP#Average Treatment Effect Error	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Text Summarization#DUC 2004 Task 1#ROUGE-1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Text Summarization#DUC 2004 Task 1#ROUGE-2	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Link Prediction#YAGO3-10#Hits@10	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Pose Estimation#COCO test-dev#AR	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Pose Estimation#COCO test-dev#AP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#Children's Book Test#Accuracy-NE	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Dense Pixel Correspondence Estimation#HPatches#Viewpoint III AEPE	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Action Recognition#Sports-1M#Video hit@5	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Natural Language Inference#MultiNLI#Mismatched	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Dependency Parsing#Penn Treebank#UAS	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Text Summarization#DUC 2004 Task 1#ROUGE-L	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Human Part Segmentation#PASCAL-Person-Part#mIoU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Generation#LSUN Churches 256 x 256#FID	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Action Recognition#HMDB-51#Average accuracy of 3 splits	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Relation Extraction#ACE 2004#Entity+Relation F1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Language Modelling#WikiText-2#Validation perplexity	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image-to-Image Translation#Cityscapes Labels-to-Photo#Per-class Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Text Generation#Yahoo Questions#Perplexity	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Machine Translation#IWSLT2014 German-English#BLEU score	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Dehazing#SOTS Indoor#PSNR	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Recommendation Systems#Netflix#nDCG@100	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Classification#ImageNet#Top 1 Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Data-to-Text Generation#E2E NLG Challenge#BLEU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Ad-Hoc Information Retrieval#TREC Robust04#P@20	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Graph Classification#NCI109#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Natural Language Inference#SciTail#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Drug Discovery#Tox21#AUC	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Fake News Detection#FNC-1#Weighted Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#Jaccard (Mean)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Face Verification#CFP-FP#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Scene Text Detection#SCUT-CTW1500#F-Measure	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Named Entity Recognition#CoNLL 2003 (English)#F1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Action Recognition#Something-Something V1#Top 1 Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Optical Character Recognition#FSNS - Test#Sequence error	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Link Prediction#FB15k-237#Hits@3	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Link Prediction#FB15k-237#Hits@1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image-to-Image Translation#Cityscapes-to-Foggy Cityscapes#mAP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#TrecQA#MRR	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Generation#CIFAR-10#bits/dimension	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Horizon Line Estimation#York Urban Dataset#AUC (horizon error)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	3D Object Detection#KITTI Pedestrians Moderate#AP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Speech Recognition#swb_hub_500 WER fullSWBCH#Percentage error	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Neural Architecture Search#NAS-Bench-201, ImageNet-16-120#Accuracy (val)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Depth Completion#KITTI Depth Completion#MAE	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Machine Translation#IWSLT2015 German-English#BLEU score	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Person Re-Identification#Market-1501#MAP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Classification#EMNIST-Letters#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Face Detection#WIDER Face (Hard)#AP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Text Style Transfer#Yelp Review Dataset (Small)#G-Score (BLEU, Accuracy)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Extractive Document Summarization#CNN / Daily Mail#ROUGE-2	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Super-Resolution#PIRM-test#NIQE	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Extractive Document Summarization#CNN / Daily Mail#ROUGE-1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Link Prediction#Cora#AP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image-to-Image Translation#GTAV-to-Cityscapes Labels#mIoU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#QASent#MRR	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Aspect-Based Sentiment Analysis#SemEval 2014 Task 4 Sub Task 2#Laptop (Acc)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#WebQuestions#F1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Few-Shot Image Classification#OMNIGLOT - 1-Shot, 5-way#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Few-Shot Semantic Segmentation#FSS-1000#Mean IoU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Scene Text Detection#ICDAR 2013#Recall	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Lipreading#LRW-1000#Top-1 Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Language Modelling#WikiText-2#Test perplexity	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Scene Text Detection#SCUT-CTW1500#Recall	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Graph Classification#IMDb-B#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	3D Human Pose Estimation#Total Capture#Average MPJPE (mm)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Salient Object Detection#DUTS-TE#F-measure	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.6	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.5	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Multi-tissue Nucleus Segmentation#Kumar#Hausdorff Distance (mm)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.7	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.2	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.4	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.3	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Super-Resolution#FFHQ 1024 x 1024 - 4x upscaling#SSIM	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Face Verification#YouTube Faces DB#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Link Prediction#Cora#AUC	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Document Image Classification#RVL-CDIP#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Semantic Segmentation#PASCAL VOC 2012 val#mIoU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Atari Games#Atari 2600 Breakout#Score	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Sentiment Analysis#Multi-Domain Sentiment Dataset#Kitchen	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Super-Resolution#VggFace2 - 8x upscaling#PSNR	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Click-Through Rate Prediction#Dianping#AUC	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#bAbi#Accuracy (trained on 1k)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Object Counting#CARPK#RMSE	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Retrieval#Oxf5k#MAP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#Children's Book Test#Accuracy-CN	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Super-Resolution#Set5 - 4x upscaling#SSIM	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Recommendation Systems#MovieLens 1M#RMSE	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Sentiment Analysis#MR#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Video Generation#UCF-101 16 frames, Unconditional, Single GPU#Inception Score	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Atari Games#Atari 2600 Fishing Derby#Score	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Dehazing#O-Haze#PSNR	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Language Modelling#enwik8#Number of params	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Dense Pixel Correspondence Estimation#HPatches#Viewpoint II AEPE	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Machine Translation#WMT2014 English-German#SacreBLEU	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Human action generation#Human3.6M#MMD	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Generation#Stanford Dogs#FID	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Domain Adaptation#Office-31#Average Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Clustering#MNIST-full#NMI	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Constituency Parsing#Penn Treebank#F1 score	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Image Reconstruction#Edge-to-Handbags#LPIPS	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Language Modelling#enwik8#Bit per Character (BPC)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Reading Comprehension#RACE#Accuracy	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Text Generation#Yahoo Questions#NLL	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Object Detection#COCO minival#box AP	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Anomaly Detection#CIFAR-10 model detecting CIFAR-10#AUPR	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Music Source Separation#MUSDB18#SDR (other)	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Scene Text Detection#SCUT-CTW1500#Precision	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#NewsQA#EM	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Speech Separation#wsj0-2mix#SI-SDRi	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Language Modelling#One Billion Word#PPL	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Coreference Resolution#OntoNotes#F1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	Question Answering#NewsQA#F1	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2002.03651v4.pdf	unknown	CRVOS: CLUE REFINING NETWORK FOR VIDEO OBJECT SEGMENTATION The encoder-decoder based methods for semi-supervised video object segmentation (Semi-VOS) have received extensive attention due to their superior performances. However, most of them have complex intermediate networks which generate strong specifiers to be robust against challenging scenarios , and this is quite inefficient when dealing with relatively simple scenarios. To solve this problem, we propose a real-time network, Clue Refining Network for Video Object Segmentation (CRVOS), that does not have any intermediate network to efficiently deal with these scenarios. In this work, we propose a simple specifier, referred to as the Clue, which consists of the previous frames coarse mask and coordinates information. We also propose a novel refine module which shows the better performance compared with the general ones by using a deconvolution layer instead of a bilinear upsam-pling layer. Our proposed method shows the fastest speed among the existing methods with a competitive accuracy. On DAVIS 2016 validation set, our method achieves 63.5 fps and J &F score of 81.6%.  Table 3: Ablation studies on DAVIS 2016 validation set. RM indicates using our novel refine modules instead of general ones. PM indicates using previous frame's coarse mask as a specifier, and Clue indicates using the Clue as a specifier. Evaluation of the entire targets are generated , and then overlapped . 3 . ( 0 . 75?1 . 25 ) for data augmentation in fine - tuning stage . F ? fps J ?
true	2005.12661v1.pdf	Question Answering#SQuAD1.1#F1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Human-Object Interaction Detection#HICO#mAP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Node Classification#Wikipedia#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#SQuAD1.1#EM	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Visual Question Answering#VQA v2 test-std#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semantic Segmentation#PASCAL VOC 2012 test#Mean IoU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#YahooCQA#P@1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semantic Segmentation#ShapeNet#Mean IoU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Pose Estimation#UPenn Action#Mean PCK@0.2	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Person Re-Identification#PRID2011#Rank-5	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Neural Architecture Search#ImageNet#Top-1 Error Rate	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Person Re-Identification#PRID2011#Rank-1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Classification#MNIST#Percentage error	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#WikiQA#MRR	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Code Generation#WikiSQL#Execution Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#Jaccard (Decay)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#CoQA#In-domain	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Temporal Action Localization#ActivityNet-1.3#mAP IOU@0.5	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Text Classification#IMDb#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Super-Resolution#Urban100 - 8x upscaling#SSIM	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Action Detection#UCF101-24#Video-mAP 0.2	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Language Modelling#One Billion Word#Number of params	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Neural Architecture Search#ImageNet#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Classification#Clothing1M#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Classification#CIFAR-100#Percentage correct	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Machine Translation#WMT2014 German-English#BLEU score	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Object Counting#Pascal VOC 2007 count-test#m-reIRMSE-nz	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Instance Segmentation#Cityscapes test#Average Precision	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Action Classification#Charades#MAP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Scene Text Detection#Total-Text#Precision	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Link Prediction#YAGO3-10#Hits@3	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Link Prediction#YAGO3-10#Hits@1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Incremental Learning#CIFAR-100 - 50 classes + 10 steps of 5 classes#Average Incremental Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Trajectory Prediction#ETH/UCY#ADE-8/12	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Language Modelling#WikiText-2#Number of params	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Scene Text Detection#MSRA-TD500#F-Measure	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Neural Architecture Search#CIFAR-10#Search Time (GPU days)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Action Recognition#UCF101#3-fold Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Clustering#CIFAR-10#NMI	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Scene Text Detection#ICDAR 2013#Precision	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Node Classification#Pubmed#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Breast Tumour Classification#PCam#AUC	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Recommendation Systems#MovieLens 1M#nDCG@10	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Line Segment Detection#York Urban Dataset#sAP10	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Visual Question Answering#VQA-CP#Score	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Machine Translation#IWSLT2015 English-Vietnamese#BLEU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	3D Reconstruction#Data3DâR2N2#3DIoU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#WikiHop#Test	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Data-to-Text Generation#Rotowire (Content Selection)#Recall	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Atari Games#Atari 2600 Freeway#Score	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semantic Segmentation#NYU Depth v2#Mean IoU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Person Re-Identification#MARS#mAP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Link Prediction#WN18#MRR	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	3D Object Detection#nuScenes#NDS	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Drug Discovery#ToxCast#AUC	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Pose Estimation# ITOP front-view#Mean mAP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Classification#Tiered ImageNet 5-way (5-shot)#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Neural Architecture Search#NAS-Bench-201, ImageNet-16-120#Search time (s)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#YahooCQA#MRR	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Sentiment Analysis#Multi-Domain Sentiment Dataset#Books	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Relation Extraction#TACRED#F1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Instance Segmentation#COCO test-dev#AP75	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Aspect-Based Sentiment Analysis#SemEval 2014 Task 4 Sub Task 2#Restaurant (Acc)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Recommendation Systems#Netflix#Recall@50	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semantic Segmentation#Semantic3D#mIoU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	3D Human Pose Estimation#Human3.6M#Average MPJPE (mm)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Denoising#Darmstadt Noise Dataset#PSNR	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Speech Enhancement#DEMAND#PESQ	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Object Counting#Pascal VOC 2007 count-test#mRMSE	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Recommendation Systems#Netflix#Recall@20	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Pose Estimation#COCO test-dev#APL	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Multi-Label Classification#MS-COCO#mAP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Pose Estimation#COCO test-dev#APM	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Generation#CIFAR-10#Model Entropy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Classification#SVHN#Percentage error	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Machine Translation#WMT2014 English-French#BLEU score	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Temporal Action Localization#ActivityNet-1.3#mAP IOU@0.95	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Human Interaction Recognition#UT#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Node Classification#Reddit#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Face Verification#IJB-A#TAR @ FAR=0.01	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	3D Human Pose Estimation#MPI-INF-3DHP#AUC	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Natural Language Inference#SNLI#Parameters	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Depth Completion#KITTI Depth Completion#RMSE	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Lipreading#LRS2#Word Error Rate (WER)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Multi-tissue Nucleus Segmentation#Kumar#Dice	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Link Prediction#WN18#Hits@10	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Person Re-Identification#Market-1501->DukeMTMC-reID#mAP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Visual Question Answering#GQA test-std#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Entity Typing#Ontonotes v5 (English)#Recall	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Action Recognition#THUMOSâ14#mAP@0.5	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Action Recognition#THUMOSâ14#mAP@0.4	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Matting#Composition-1K#MSE	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Action Recognition#THUMOSâ14#mAP@0.3	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Temporal Action Localization#ActivityNet-1.3#mAP IOU@0.75	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Data-to-Text Generation#Rotowire (Content Selection)#Precision	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Atari Games#Atari 2600 Montezuma's Revenge#Score	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Action Recognition#Jester#Val	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Few-Shot Image Classification#Mini-ImageNet - 1-Shot Learning#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Classification#CIFAR-100#Percentage error	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Action Recognition#Something-Something V2#Top-1 Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#QASent#MAP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semantic Segmentation#CamVid#Mean IoU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Neural Architecture Search#ImageNet#Params	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Instance Segmentation#COCO test-dev#AP50	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Data-to-Text Generation#E2E NLG Challenge#CIDEr	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Speech Recognition#LibriSpeech test-other#Word Error Rate (WER)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semantic Segmentation#PASCAL Context#mIoU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Graph Classification#PTC#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Classification#MNIST#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Visual Question Answering#COCO Visual Question Answering (VQA) real images 1.0 open ended#Percentage correct	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image-to-Image Translation#Cityscapes Labels-to-Photo#Per-pixel Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Fine-Grained Image Classification# CUB-200-2011#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Visual Question Answering#VQA v2 test-dev#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semantic Segmentation#Cityscapes val#mIoU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Generation#CIFAR-10#Inception score	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Click-Through Rate Prediction#MovieLens 20M#AUC	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Super-Resolution#Set5 - 4x upscaling#PSNR	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Automated Theorem Proving#HOList benchmark#Percentage correct	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Text Summarization#arXiv#ROUGE-1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	3D Semantic Segmentation#SemanticKITTI#mIoU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Action Recognition#NTU RGB+D#Accuracy (CV)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Domain Adaptation#UCF-to-HMDBfull#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Lipreading#Lip Reading in the Wild#Top-1 Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Super-Resolution#FFHQ 1024 x 1024 - 4x upscaling#FID	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semantic Segmentation#S3DIS#Mean IoU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semantic Segmentation#ADE20K val#mIoU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Machine Translation#WMT2014 English-German#BLEU score	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Relation Extraction#ACE 2004#Entity F1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Face Detection#Annotated Faces in the Wild#AP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Ad-Hoc Information Retrieval#TREC Robust04#nDCG@20	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Scene Text Detection#Total-Text#Recall	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Language Modelling#WikiText-103#Test perplexity	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Continual Learning#CUBS (Fine-grained 6 Tasks)#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	3D Human Pose Estimation#Human3.6M#Multi-View or Monocular	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Visual Question Answering#VQA v1 test-dev#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Link Prediction#WN18#Hits@3	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Word Sense Disambiguation#SemEval 2007 Task 7#F1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Link Prediction#WN18#Hits@1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Retrieval#Flickr30K 1K test#R@5	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Retrieval#Flickr30K 1K test#R@1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#F-measure (Mean)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Fine-Grained Image Classification#FGVC Aircraft#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Music Source Separation#MUSDB18#SDR (vocals)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Super-Resolution#FFHQ 1024 x 1024 - 4x upscaling#MS-SSIM	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	3D Human Pose Estimation#Human3.6M#Using 2D ground-truth joints	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Data-to-Text Generation#E2E NLG Challenge#METEOR	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Link Prediction#FB15k-237#MRR	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Sentiment Analysis#Multi-Domain Sentiment Dataset#Electronics	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Recommendation Systems#MovieLens 1M#HR@10	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Time Series Classification#PhysioNet Challenge 2012#AUC Stdev	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Drug Discovery#QM9#Error ratio	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Face Verification#CASIA NIR-VIS 2.0#TAR @ FAR=0.001	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Action Recognition#NTU RGB+D#Accuracy (CS)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Graph Regression#Lipophilicity #RMSE	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Scene Text Detection#ICDAR 2017 MLT#Precision	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Face Verification#Labeled Faces in the Wild#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Action Recognition#NTU RGB+D 120#Accuracy (Cross-Subject)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	3D Human Pose Estimation#MPI-INF-3DHP#3DPCK	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#Jaccard (Mean)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Meta-Learning#MT50#Average Success Rate	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image-to-Image Translation#COCO-Stuff Labels-to-Photos#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Scene Text Detection#MSRA-TD500#Recall	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Scene Text Detection#ICDAR 2017 MLT#Recall	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Robotic Grasping#Cornell Grasp Dataset#5 fold cross validation	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Temporal Action Localization#ActivityNet-1.3#mAP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Link Prediction#FB15k-237#Hits@10	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	6D Pose Estimation#YCB-Video#ADDS AUC	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	3D Human Pose Estimation#HumanEva-I#Mean Reconstruction Error (mm)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Visual Question Answering#VCR (QA-R) test#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Pose Estimation#COCO test-dev#AP50	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Shadow Detection#SBU#BER	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#F-measure (Mean)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Dehazing#SOTS Indoor#SSIM	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Clustering#Extended Yale-B#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#F-measure (Recall)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Link Prediction#WN18RR#MRR	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Action Recognition#Something-Something V1#Top 5 Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Object Detection#COCO test-dev#AP75	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Generation#LSUN Bedroom 256 x 256#FID	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#SQuAD2.0 dev#F1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Text Generation#EMNLP2017 WMT#BLEU-5	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Text Generation#EMNLP2017 WMT#BLEU-4	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#SQuAD2.0 dev#EM	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Sentiment Analysis#Multi-Domain Sentiment Dataset#Average	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Natural Language Inference#MultiNLI#Matched	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Dense Pixel Correspondence Estimation#HPatches#Viewpoint V AEPE	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Traffic Prediction#METR-LA#MAE @ 12 step	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Click-Through Rate Prediction#Dianping#Log Loss	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Person Re-Identification#PRID2011#Rank-20	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Speech Recognition#LibriSpeech test-clean#Word Error Rate (WER)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Text Generation#EMNLP2017 WMT#BLEU-3	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Text Generation#EMNLP2017 WMT#BLEU-2	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Neural Architecture Search#ImageNet#FLOPS	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Cross-Lingual Bitext Mining#BUCC German-to-English#F1 score	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Scene Text Detection#ICDAR 2017 MLT#F-Measure	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Metric Learning#CARS196#R@1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Code Generation#WikiSQL#Exact Match Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Scene Text Detection#Total-Text#F-Measure	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Classification#STL-10#Percentage correct	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Facial Expression Recognition#FER2013#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Graph Classification#PROTEINS#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Neural Architecture Search#CIFAR-10 Image Classification#Percentage error	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#SQuAD1.1 dev#F1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Scene Text Detection#ICDAR 2015#F-Measure	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Speech Recognition#Switchboard + Hub500#Percentage error	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#SQuAD1.1 dev#EM	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Few-Shot Image Classification#Mini-Imagenet 5-way (5-shot)#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Classification#CIFAR-10#Percentage correct	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Neural Architecture Search#CIFAR-10 Image Classification#Params	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	3D Human Pose Estimation#Surreal#MPJPE	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Facial Landmark Detection#300W#NME	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Multi-Object Tracking#MOT16#MOTA	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Language Modelling#Text8#Number of params	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#Jaccard (Decay)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Action Classification#HMDB51#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Reading Comprehension#RACE#Accuracy (High)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Person Re-Identification#Market-1501#Rank-1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#Jaccard (Recall)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Generation#Fashion-MNIST#FID	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Data-to-Text Generation#E2E NLG Challenge#ROUGE-L	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#F-measure (Recall)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Object Detection#COCO minival#APL	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Object Detection#COCO minival#APM	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Generation#CIFAR-10#FID	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Temporal Action Localization#CrossTask#Recall	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Generation#MNIST#bits/dimension	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Pose Estimation#COCO test-dev#AP75	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Temporal Action Localization#J-HMDB-21#Frame-mAP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Object Detection#COCO minival#APS	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Named Entity Recognition#GENIA#F1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Super-Resolution#FFHQ 1024 x 1024 - 4x upscaling#PSNR	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Reading Comprehension#RACE#Accuracy (Middle)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Object Detection#COCO test-dev#AP50	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#Jaccard (Recall)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Conversational Response Selection#Ubuntu Dialogue (v1, Ranking)#R2@1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Person Re-Identification#DukeMTMC-reID#Rank-1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Ad-Hoc Information Retrieval#TREC Robust04#MAP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semantic Role Labeling#CoNLL 2005#F1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Person Re-Identification#DukeMTMC-reID#Rank-5	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Music Source Separation#MUSDB18#SDR (bass)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Language Modelling#Penn Treebank (Word Level)#Params	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Object Detection#PASCAL VOC 2007#MAP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#CNN / Daily Mail#CNN	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#TrecQA#MAP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Common Sense Reasoning#CommonsenseQA#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Retrieval#CARS196#R@1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Sentiment Analysis#Yelp Fine-grained classification#Error	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Action Recognition#NTU RGB+D 120#Accuracy (Cross-Setup)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Link Prediction#WN18#MR	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Salient Object Detection#DUTS-TE#MAE	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Scene Text Detection#ICDAR 2013#F-Measure	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Generation#SQuAD1.1#BLEU-4	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Neural Architecture Search#CIFAR-10#Top-1 Error Rate	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Action Recognition#Sports-1M#Video hit@1 	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Scene Text Detection#ICDAR 2015#Precision	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Object Counting#CARPK#MAE	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	3D Object Reconstruction#Data3DâR2N2#3DIoU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Dependency Parsing#Penn Treebank#LAS	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	3D Object Reconstruction#Data3DâR2N2#Avg F1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Hand Pose Estimation#NYU Hands#Average 3D Error	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Conversational Response Selection#Ubuntu Dialogue (v1, Ranking)#R10@5	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#CoQA#Out-of-domain	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Conversational Response Selection#Ubuntu Dialogue (v1, Ranking)#R10@1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Conversational Response Selection#Ubuntu Dialogue (v1, Ranking)#R10@2	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#F-measure (Decay)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Optical Flow Estimation#KITTI 2012#Average End-Point Error	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Domain Adaptation#ImageCLEF-DA#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Aspect-Based Sentiment Analysis#SemEval 2014 Task 4 Sub Task 2#Mean Acc (Restaurant + Laptop)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#CoQA#Overall	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Text Generation#Yahoo Questions#KL	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Line Segment Detection#York Urban Dataset#sAP5	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image-to-Image Translation#Cityscapes Labels-to-Photo#Class IOU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Part-Of-Speech Tagging#Penn Treebank#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#J&F	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Object Counting#Pascal VOC 2007 count-test#m-relRMSE	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semantic Segmentation#Cityscapes test#Mean IoU (class)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Natural Language Inference#SNLI#% Train Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Person Re-Identification#MARS#Rank-5	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Clustering#MNIST-test#NMI	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Sentiment Analysis#SST-2 Binary classification#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Link Prediction#FB15k-237#MR	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Person Re-Identification#MARS#Rank-1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Human-Object Interaction Detection#HICO-DET#MAP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Text Classification#R8#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Person Re-Identification#Market-1501->DukeMTMC-reID#Rank-1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Time Series Classification#PhysioNet Challenge 2012#AUC	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Node Classification#Wikipedia#Macro-F1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Facial Expression Recognition#AffectNet#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#Quora Question Pairs#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Atari Games#Atari 2600 Q*Bert#Score	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Data-to-Text Generation#MULTIWOZ 2.1#BLEU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Speech Synthesis#North American English#Mean Opinion Score	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Text Summarization#GigaWord#ROUGE-L	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Retrieval#Flickr30K 1K test#R@10	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Reconstruction#Edge-to-Handbags#FID	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Natural Language Inference#SNLI#% Test Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Text Summarization#GigaWord#ROUGE-1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Text Summarization#GigaWord#ROUGE-2	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Entity Typing#Ontonotes v5 (English)#F1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Click-Through Rate Prediction#Criteo#Log Loss	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Named Entity Recognition#Ontonotes v5 (English)#F1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Dependency Parsing#Penn Treebank#POS	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Domain Adaptation#USPS-to-MNIST#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Object Detection#COCO test-dev#APS	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Optical Flow Estimation#Sintel-clean#Average End-Point Error	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Link Prediction#YAGO3-10#MRR	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Object Detection#KITTI Cars Moderate#AP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#bAbi#Mean Error Rate	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Click-Through Rate Prediction#Criteo#AUC	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Scene Text Detection#ICDAR 2015#Recall	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Entity Typing#Ontonotes v5 (English)#Precision	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Lipreading#GRID corpus (mixed-speech)#Word Error Rate (WER)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#F-measure (Decay)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Relation Extraction#CoNLL04#Relation F1 	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Clustering#MNIST-test#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Generation#Stanford Dogs#Inception score	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Dense Pixel Correspondence Estimation#HPatches#Viewpoint IV AEPE	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Subjectivity Analysis#SUBJ#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Classification#ImageNet#Number of params	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#bAbi#Accuracy (trained on 10k)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	3D Human Pose Estimation#MPI-INF-3DHP#MJPE	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Speech Recognition#TIMIT#Percentage error	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Part-Of-Speech Tagging#UD#Avg accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Classification#ImageNet#Top 5 Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Clustering#CIFAR-10#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Object Detection#COCO test-dev#APM	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Object Detection#COCO test-dev#APL	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#MS MARCO#Rouge-L	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Graph Classification#NEURON-BINARY#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Language Modelling#Penn Treebank (Word Level)#Test perplexity	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Pose Tracking#PoseTrack2017#MOTA	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image-to-Image Translation#SYNTHIA-to-Cityscapes#mIoU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Visual Question Answering#MSVD-QA#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Relation Extraction#CoNLL04#Entity F1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Motion Segmentation#Hopkins155#Classification Error	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Person Re-Identification#DukeMTMC-reID#MAP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Pose Estimation#MPII Human Pose#PCKh-0.5	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Graph Classification#NCI1#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Named Entity Recognition#SciERC#F1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Scene Text Detection#MSRA-TD500#Precision	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	3D Human Pose Estimation#3DPW#PA-MPJPE	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Classification#Kuzushiji-MNIST#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Language Modelling#Text8#Bit per Character (BPC)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image-to-Image Translation#COCO-Stuff Labels-to-Photos#FID	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Super-Resolution#Urban100 - 8x upscaling#PSNR	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semantic Segmentation#LIP val#mIoU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Data-to-Text Generation#E2E NLG Challenge#NIST	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Sentiment Analysis#Multi-Domain Sentiment Dataset#DVD	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Domain Adaptation#Office-Caltech#Average Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Relation Extraction#DocRED#F1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#MS MARCO#BLEU-1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Music Source Separation#MUSDB18#SDR (drums)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Machine Translation#WMT2016 English-German#BLEU score	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Face Verification#CK+#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Language Modelling#Penn Treebank (Word Level)#Validation perplexity	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2016#J&F	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Relation Extraction#NYT#F1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Generation#ImageNet 64x64#Bits per dim	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semantic Segmentation#ScanNet#3DIoU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Dense Pixel Correspondence Estimation#HPatches#Viewpoint I AEPE	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Few-Shot Image Classification#Mini-ImageNet-CUB 5-way (5-shot)#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Language Modelling#WikiText-103#Validation perplexity	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Neural Architecture Search#NAS-Bench-201, ImageNet-16-120#Accuracy (Test)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Object Detection#COCO minival#AP50	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Few-Shot Image Classification#CIFAR-FS 5-way (5-shot)#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Object Counting#Pascal VOC 2007 count-test#mRMSE-nz	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Action Classification#Kinetics-400#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	3D Object Detection#ScanNetV2#mAP@0.5	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Instance Segmentation#COCO test-dev#mask AP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Named Entity Recognition#ACE 2005#F1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Clustering#MNIST-full#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Object Detection#COCO test-dev#box AP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image-to-Image Translation#RaFD#Classification Error	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Sentiment Analysis#IMDb#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#WikiQA#MAP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image-to-Image Translation#COCO-Stuff Labels-to-Photos#mIoU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Node Classification#Cora with Public Split: fixed 20 nodes per class#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Instance Segmentation#COCO test-dev#APS	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Generation#Binarized MNIST#nats	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Instance Segmentation#COCO test-dev#APM	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Instance Segmentation#COCO test-dev#APL	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Passage Re-Ranking#MS MARCO#MRR	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Entity Alignment#DBP15k zh-en#Hits@1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Object Detection#COCO minival#AP75	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	3D Object Detection#ScanNetV2#mAP@0.25	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Causal Inference#IDHP#Average Treatment Effect Error	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Text Summarization#DUC 2004 Task 1#ROUGE-1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Text Summarization#DUC 2004 Task 1#ROUGE-2	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Link Prediction#YAGO3-10#Hits@10	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Pose Estimation#COCO test-dev#AR	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Pose Estimation#COCO test-dev#AP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#Children's Book Test#Accuracy-NE	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Dense Pixel Correspondence Estimation#HPatches#Viewpoint III AEPE	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Action Recognition#Sports-1M#Video hit@5	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Natural Language Inference#MultiNLI#Mismatched	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Dependency Parsing#Penn Treebank#UAS	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Text Summarization#DUC 2004 Task 1#ROUGE-L	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Human Part Segmentation#PASCAL-Person-Part#mIoU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Generation#LSUN Churches 256 x 256#FID	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Action Recognition#HMDB-51#Average accuracy of 3 splits	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Relation Extraction#ACE 2004#Entity+Relation F1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Language Modelling#WikiText-2#Validation perplexity	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image-to-Image Translation#Cityscapes Labels-to-Photo#Per-class Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Text Generation#Yahoo Questions#Perplexity	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Machine Translation#IWSLT2014 German-English#BLEU score	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Dehazing#SOTS Indoor#PSNR	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Recommendation Systems#Netflix#nDCG@100	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Classification#ImageNet#Top 1 Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Data-to-Text Generation#E2E NLG Challenge#BLEU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Ad-Hoc Information Retrieval#TREC Robust04#P@20	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Graph Classification#NCI109#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Natural Language Inference#SciTail#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Drug Discovery#Tox21#AUC	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Fake News Detection#FNC-1#Weighted Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semi-Supervised Video Object Segmentation#DAVIS 2017 (test-dev)#Jaccard (Mean)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Face Verification#CFP-FP#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Scene Text Detection#SCUT-CTW1500#F-Measure	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Named Entity Recognition#CoNLL 2003 (English)#F1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Action Recognition#Something-Something V1#Top 1 Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Optical Character Recognition#FSNS - Test#Sequence error	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Link Prediction#FB15k-237#Hits@3	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Link Prediction#FB15k-237#Hits@1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image-to-Image Translation#Cityscapes-to-Foggy Cityscapes#mAP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#TrecQA#MRR	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Generation#CIFAR-10#bits/dimension	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Horizon Line Estimation#York Urban Dataset#AUC (horizon error)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	3D Object Detection#KITTI Pedestrians Moderate#AP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Speech Recognition#swb_hub_500 WER fullSWBCH#Percentage error	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Neural Architecture Search#NAS-Bench-201, ImageNet-16-120#Accuracy (val)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Depth Completion#KITTI Depth Completion#MAE	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Machine Translation#IWSLT2015 German-English#BLEU score	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Person Re-Identification#Market-1501#MAP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Classification#EMNIST-Letters#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Face Detection#WIDER Face (Hard)#AP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Text Style Transfer#Yelp Review Dataset (Small)#G-Score (BLEU, Accuracy)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Extractive Document Summarization#CNN / Daily Mail#ROUGE-2	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Super-Resolution#PIRM-test#NIQE	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Extractive Document Summarization#CNN / Daily Mail#ROUGE-1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Link Prediction#Cora#AP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image-to-Image Translation#GTAV-to-Cityscapes Labels#mIoU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#QASent#MRR	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Aspect-Based Sentiment Analysis#SemEval 2014 Task 4 Sub Task 2#Laptop (Acc)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#WebQuestions#F1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Few-Shot Image Classification#OMNIGLOT - 1-Shot, 5-way#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Few-Shot Semantic Segmentation#FSS-1000#Mean IoU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Scene Text Detection#ICDAR 2013#Recall	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Lipreading#LRW-1000#Top-1 Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Language Modelling#WikiText-2#Test perplexity	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Scene Text Detection#SCUT-CTW1500#Recall	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Graph Classification#IMDb-B#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	3D Human Pose Estimation#Total Capture#Average MPJPE (mm)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Salient Object Detection#DUTS-TE#F-measure	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.6	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.5	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Multi-tissue Nucleus Segmentation#Kumar#Hausdorff Distance (mm)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.7	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.2	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.4	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Temporal Action Localization#THUMOSâ14#mAP IOU@0.3	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Super-Resolution#FFHQ 1024 x 1024 - 4x upscaling#SSIM	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Face Verification#YouTube Faces DB#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Link Prediction#Cora#AUC	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Document Image Classification#RVL-CDIP#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Semantic Segmentation#PASCAL VOC 2012 val#mIoU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Atari Games#Atari 2600 Breakout#Score	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Sentiment Analysis#Multi-Domain Sentiment Dataset#Kitchen	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Super-Resolution#VggFace2 - 8x upscaling#PSNR	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Click-Through Rate Prediction#Dianping#AUC	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#bAbi#Accuracy (trained on 1k)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Object Counting#CARPK#RMSE	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Retrieval#Oxf5k#MAP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#Children's Book Test#Accuracy-CN	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Super-Resolution#Set5 - 4x upscaling#SSIM	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Recommendation Systems#MovieLens 1M#RMSE	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Sentiment Analysis#MR#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Video Generation#UCF-101 16 frames, Unconditional, Single GPU#Inception Score	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Atari Games#Atari 2600 Fishing Derby#Score	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Dehazing#O-Haze#PSNR	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Language Modelling#enwik8#Number of params	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Dense Pixel Correspondence Estimation#HPatches#Viewpoint II AEPE	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Machine Translation#WMT2014 English-German#SacreBLEU	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Human action generation#Human3.6M#MMD	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Generation#Stanford Dogs#FID	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Domain Adaptation#Office-31#Average Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Clustering#MNIST-full#NMI	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Constituency Parsing#Penn Treebank#F1 score	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Image Reconstruction#Edge-to-Handbags#LPIPS	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Language Modelling#enwik8#Bit per Character (BPC)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Reading Comprehension#RACE#Accuracy	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Text Generation#Yahoo Questions#NLL	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Object Detection#COCO minival#box AP	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Anomaly Detection#CIFAR-10 model detecting CIFAR-10#AUPR	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Music Source Separation#MUSDB18#SDR (other)	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Scene Text Detection#SCUT-CTW1500#Precision	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#NewsQA#EM	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Speech Separation#wsj0-2mix#SI-SDRi	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Language Modelling#One Billion Word#PPL	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Coreference Resolution#OntoNotes#F1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	Question Answering#NewsQA#F1	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
true	2005.12661v1.pdf	unknown	DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting Understanding human motion behaviour is a critical task for several possible applications like self-driving cars or social robots, and in general for all those settings where an autonomous agent has to navigate inside a human-centric environment. This is non-trivial because human motion is inherently multi-modal: given a history of human motion paths, there are many plausible ways by which people could move in the future. Additionally, people activities are often driven by goals, e.g. reaching particular locations or interacting with the environment. We address both the aforementioned aspects by proposing anew recurrent generative model that considers both single agents' future goals and interactions between different agents. The model exploits a double attention-based graph neural network to collect information about the mutual influences among different agents and integrates it with data about agents' possible future objectives. Our proposal is general enough to be applied in different scenarios: the model achieves state-of-the-art results in both urban environments and also in sports applications.
