30 Dec 2020 00:02.11 [ERROR] FullTextParser            - DocumentPointer for block 449 points to 801 token, but block token size is 787
30 Dec 2020 00:02.17 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:02.17 [INFO ] ReferenceMarkerMatcher    -   Borgwardt, K. M., & Kriegel, H.-P. (2005). Shortest-path kernels on graphs. In Proceedings 
of the 5th IEEE International Conference on Data Mining, pp. 74-81.
30 Dec 2020 00:02.17 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:02.17 [INFO ] ReferenceMarkerMatcher    -   Borgwardt, K. M., Ong, C. S., Schönauer, S., Vishwanathan, S., Smola, A. J., & Kriegel, 
H.-P. (2005). Protein function prediction via graph kernels. Bioinformatics, 21 (suppl 
1), i47-i56.
30 Dec 2020 00:02.17 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:02.17 [INFO ] ReferenceMarkerMatcher    -   Borgwardt, K. M., & Kriegel, H.-P. (2005). Shortest-path kernels on graphs. In Proceedings 
of the 5th IEEE International Conference on Data Mining, pp. 74-81.
30 Dec 2020 00:02.17 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:02.17 [INFO ] ReferenceMarkerMatcher    -   Borgwardt, K. M., Ong, C. S., Schönauer, S., Vishwanathan, S., Smola, A. J., & Kriegel, 
H.-P. (2005). Protein function prediction via graph kernels. Bioinformatics, 21 (suppl 
1), i47-i56.
30 Dec 2020 00:02.17 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:02.17 [INFO ] ReferenceMarkerMatcher    -   Borgwardt, K. M., & Kriegel, H.-P. (2005). Shortest-path kernels on graphs. In Proceedings 
of the 5th IEEE International Conference on Data Mining, pp. 74-81.
30 Dec 2020 00:02.17 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:02.17 [INFO ] ReferenceMarkerMatcher    -   Borgwardt, K. M., Ong, C. S., Schönauer, S., Vishwanathan, S., Smola, A. J., & Kriegel, 
H.-P. (2005). Protein function prediction via graph kernels. Bioinformatics, 21 (suppl 
1), i47-i56.
30 Dec 2020 00:02.17 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:02.17 [INFO ] ReferenceMarkerMatcher    -   Borgwardt, K. M., & Kriegel, H.-P. (2005). Shortest-path kernels on graphs. In Proceedings 
of the 5th IEEE International Conference on Data Mining, pp. 74-81.
30 Dec 2020 00:02.17 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:02.17 [INFO ] ReferenceMarkerMatcher    -   Borgwardt, K. M., Ong, C. S., Schönauer, S., Vishwanathan, S., Smola, A. J., & Kriegel, 
H.-P. (2005). Protein function prediction via graph kernels. Bioinformatics, 21 (suppl 
1), i47-i56.
30 Dec 2020 00:02.22 [ERROR] FullTextParser            - DocumentPointer for block 449 points to 801 token, but block token size is 787
30 Dec 2020 00:02.28 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:02.28 [INFO ] ReferenceMarkerMatcher    -   Borgwardt, K. M., & Kriegel, H.-P. (2005). Shortest-path kernels on graphs. In Proceedings 
of the 5th IEEE International Conference on Data Mining, pp. 74-81.
30 Dec 2020 00:02.28 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:02.28 [INFO ] ReferenceMarkerMatcher    -   Borgwardt, K. M., Ong, C. S., Schönauer, S., Vishwanathan, S., Smola, A. J., & Kriegel, 
H.-P. (2005). Protein function prediction via graph kernels. Bioinformatics, 21 (suppl 
1), i47-i56.
30 Dec 2020 00:02.28 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:02.28 [INFO ] ReferenceMarkerMatcher    -   Borgwardt, K. M., & Kriegel, H.-P. (2005). Shortest-path kernels on graphs. In Proceedings 
of the 5th IEEE International Conference on Data Mining, pp. 74-81.
30 Dec 2020 00:02.28 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:02.28 [INFO ] ReferenceMarkerMatcher    -   Borgwardt, K. M., Ong, C. S., Schönauer, S., Vishwanathan, S., Smola, A. J., & Kriegel, 
H.-P. (2005). Protein function prediction via graph kernels. Bioinformatics, 21 (suppl 
1), i47-i56.
30 Dec 2020 00:02.28 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:02.28 [INFO ] ReferenceMarkerMatcher    -   Borgwardt, K. M., & Kriegel, H.-P. (2005). Shortest-path kernels on graphs. In Proceedings 
of the 5th IEEE International Conference on Data Mining, pp. 74-81.
30 Dec 2020 00:02.28 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:02.28 [INFO ] ReferenceMarkerMatcher    -   Borgwardt, K. M., Ong, C. S., Schönauer, S., Vishwanathan, S., Smola, A. J., & Kriegel, 
H.-P. (2005). Protein function prediction via graph kernels. Bioinformatics, 21 (suppl 
1), i47-i56.
30 Dec 2020 00:02.28 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:02.28 [INFO ] ReferenceMarkerMatcher    -   Borgwardt, K. M., & Kriegel, H.-P. (2005). Shortest-path kernels on graphs. In Proceedings 
of the 5th IEEE International Conference on Data Mining, pp. 74-81.
30 Dec 2020 00:02.28 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:02.28 [INFO ] ReferenceMarkerMatcher    -   Borgwardt, K. M., Ong, C. S., Schönauer, S., Vishwanathan, S., Smola, A. J., & Kriegel, 
H.-P. (2005). Protein function prediction via graph kernels. Bioinformatics, 21 (suppl 
1), i47-i56.
30 Dec 2020 00:02.28 [ERROR] FullTextParser            - DocumentPointer for block 449 points to 801 token, but block token size is 787
30 Dec 2020 00:06.29 [ERROR] FullTextParser            - DocumentPointer for block 83 points to 137 token, but block token size is 125
30 Dec 2020 00:06.31 [ERROR] FullTextParser            - DocumentPointer for block 83 points to 137 token, but block token size is 125
30 Dec 2020 00:06.32 [ERROR] FullTextParser            - DocumentPointer for block 83 points to 137 token, but block token size is 125
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   MAML (Finn et al., 2017) 
48.70 ± 1.84% 
LLAMA (Finn & Levine, 2017) 
49.40 ± 1.83%
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   Finn, C. and Levine, S. Meta-learning and universal-
ity: Deep representations and gradient descent can 
approximate any learning algorithm. arXiv preprint 
arXiv:1710.11622, 2017.
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta-
learning for fast adaptation of deep networks. In ICML, 
pp. 1126-1135, 2017.
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   MAML (Finn et al., 2017) 
48.70 ± 1.84% 
LLAMA (Finn & Levine, 2017) 
49.40 ± 1.83%
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   Finn, C. and Levine, S. Meta-learning and universal-
ity: Deep representations and gradient descent can 
approximate any learning algorithm. arXiv preprint 
arXiv:1710.11622, 2017.
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta-
learning for fast adaptation of deep networks. In ICML, 
pp. 1126-1135, 2017.
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   MUMOMAML (Vuorio et al., 2018) 
49.86 ± 1.85% 
Reptile (Nichol & Schulman, 2018) 
49.97 ± 0.32%
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   MT-Net (Lee & Choi, 2018) 
49.75 ± 1.83%
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   PLATIPUS (Finn et al., 2018) 
classification challenge, 2018. 
URL 
https://www.kaggle.com/c/ 
fungi-challenge-fgvc-2018.
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   Lee, Y. and Choi, S. Gradient-based meta-learning with 
learned layerwise metric and subspace. In ICML, pp. 
2933-2942, 2018.
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   Nguyen, Q. and Hein, M. Optimization landscape and 
expressivity of deep cnns. In ICML, 2018.
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   Nichol, A. and Schulman, J. Reptile: a scalable metalearn-
ing algorithm. arXiv preprint arXiv:1803.02999, 2018.
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   SNAIL (Mishra et al., 2018) 
45.10 ± 0.00% 
mAP-DLM (Triantafillou et al., 2017) 
49.82 ± 0.78% 
Relation Net (Yang et al., 2018) 
50.44 ± 0.82%
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   Finn, C., Xu, K., and Levine, S. Probabilistic model-
agnostic meta-learning. arXiv preprint arXiv:1806.02817, 
2018.
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   Munkhdalai, T., Yuan, X., Mehri, S., and Trischler, A. Rapid 
adaptation with conditionally shifted neurons. In ICML, 
pp. 3661-3670, 2018.
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   Grant, E., Finn, C., Levine, S., Darrell, T., and Griffiths, T. 
Recasting gradient-based meta-learning as hierarchical 
bayes. arXiv preprint arXiv:1801.08930, 2018.
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   MAML (Finn et al., 2017) 
48.70 ± 1.84% 
LLAMA (Finn & Levine, 2017) 
49.40 ± 1.83%
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   Finn, C. and Levine, S. Meta-learning and universal-
ity: Deep representations and gradient descent can 
approximate any learning algorithm. arXiv preprint 
arXiv:1710.11622, 2017.
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta-
learning for fast adaptation of deep networks. In ICML, 
pp. 1126-1135, 2017.
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   MAML (Finn et al., 2017) 
48.70 ± 1.84% 
LLAMA (Finn & Levine, 2017) 
49.40 ± 1.83%
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   Finn, C. and Levine, S. Meta-learning and universal-
ity: Deep representations and gradient descent can 
approximate any learning algorithm. arXiv preprint 
arXiv:1710.11622, 2017.
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta-
learning for fast adaptation of deep networks. In ICML, 
pp. 1126-1135, 2017.
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   MAML (Finn et al., 2017) 
48.70 ± 1.84% 
LLAMA (Finn & Levine, 2017) 
49.40 ± 1.83%
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   Finn, C. and Levine, S. Meta-learning and universal-
ity: Deep representations and gradient descent can 
approximate any learning algorithm. arXiv preprint 
arXiv:1710.11622, 2017.
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta-
learning for fast adaptation of deep networks. In ICML, 
pp. 1126-1135, 2017.
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   MAML (Finn et al., 2017) 
48.70 ± 1.84% 
LLAMA (Finn & Levine, 2017) 
49.40 ± 1.83%
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   Finn, C. and Levine, S. Meta-learning and universal-
ity: Deep representations and gradient descent can 
approximate any learning algorithm. arXiv preprint 
arXiv:1710.11622, 2017.
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.54 [INFO ] ReferenceMarkerMatcher    -   Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta-
learning for fast adaptation of deep networks. In ICML, 
pp. 1126-1135, 2017.
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   MAML (Finn et al., 2017) 
48.70 ± 1.84% 
LLAMA (Finn & Levine, 2017) 
49.40 ± 1.83%
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   Finn, C. and Levine, S. Meta-learning and universal-
ity: Deep representations and gradient descent can 
approximate any learning algorithm. arXiv preprint 
arXiv:1710.11622, 2017.
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta-
learning for fast adaptation of deep networks. In ICML, 
pp. 1126-1135, 2017.
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   MAML (Finn et al., 2017) 
48.70 ± 1.84% 
LLAMA (Finn & Levine, 2017) 
49.40 ± 1.83%
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   Finn, C. and Levine, S. Meta-learning and universal-
ity: Deep representations and gradient descent can 
approximate any learning algorithm. arXiv preprint 
arXiv:1710.11622, 2017.
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta-
learning for fast adaptation of deep networks. In ICML, 
pp. 1126-1135, 2017.
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   MUMOMAML (Vuorio et al., 2018) 
49.86 ± 1.85% 
Reptile (Nichol & Schulman, 2018) 
49.97 ± 0.32%
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   MT-Net (Lee & Choi, 2018) 
49.75 ± 1.83%
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   PLATIPUS (Finn et al., 2018) 
classification challenge, 2018. 
URL 
https://www.kaggle.com/c/ 
fungi-challenge-fgvc-2018.
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   Lee, Y. and Choi, S. Gradient-based meta-learning with 
learned layerwise metric and subspace. In ICML, pp. 
2933-2942, 2018.
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   Nguyen, Q. and Hein, M. Optimization landscape and 
expressivity of deep cnns. In ICML, 2018.
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   Nichol, A. and Schulman, J. Reptile: a scalable metalearn-
ing algorithm. arXiv preprint arXiv:1803.02999, 2018.
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   SNAIL (Mishra et al., 2018) 
45.10 ± 0.00% 
mAP-DLM (Triantafillou et al., 2017) 
49.82 ± 0.78% 
Relation Net (Yang et al., 2018) 
50.44 ± 0.82%
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   Finn, C., Xu, K., and Levine, S. Probabilistic model-
agnostic meta-learning. arXiv preprint arXiv:1806.02817, 
2018.
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   Munkhdalai, T., Yuan, X., Mehri, S., and Trischler, A. Rapid 
adaptation with conditionally shifted neurons. In ICML, 
pp. 3661-3670, 2018.
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   Grant, E., Finn, C., Levine, S., Darrell, T., and Griffiths, T. 
Recasting gradient-based meta-learning as hierarchical 
bayes. arXiv preprint arXiv:1801.08930, 2018.
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   MAML (Finn et al., 2017) 
48.70 ± 1.84% 
LLAMA (Finn & Levine, 2017) 
49.40 ± 1.83%
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   Finn, C. and Levine, S. Meta-learning and universal-
ity: Deep representations and gradient descent can 
approximate any learning algorithm. arXiv preprint 
arXiv:1710.11622, 2017.
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta-
learning for fast adaptation of deep networks. In ICML, 
pp. 1126-1135, 2017.
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   MAML (Finn et al., 2017) 
48.70 ± 1.84% 
LLAMA (Finn & Levine, 2017) 
49.40 ± 1.83%
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   Finn, C. and Levine, S. Meta-learning and universal-
ity: Deep representations and gradient descent can 
approximate any learning algorithm. arXiv preprint 
arXiv:1710.11622, 2017.
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta-
learning for fast adaptation of deep networks. In ICML, 
pp. 1126-1135, 2017.
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   MAML (Finn et al., 2017) 
48.70 ± 1.84% 
LLAMA (Finn & Levine, 2017) 
49.40 ± 1.83%
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   Finn, C. and Levine, S. Meta-learning and universal-
ity: Deep representations and gradient descent can 
approximate any learning algorithm. arXiv preprint 
arXiv:1710.11622, 2017.
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta-
learning for fast adaptation of deep networks. In ICML, 
pp. 1126-1135, 2017.
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   MAML (Finn et al., 2017) 
48.70 ± 1.84% 
LLAMA (Finn & Levine, 2017) 
49.40 ± 1.83%
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   Finn, C. and Levine, S. Meta-learning and universal-
ity: Deep representations and gradient descent can 
approximate any learning algorithm. arXiv preprint 
arXiv:1710.11622, 2017.
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:08.59 [INFO ] ReferenceMarkerMatcher    -   Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta-
learning for fast adaptation of deep networks. In ICML, 
pp. 1126-1135, 2017.
30 Dec 2020 00:17.55 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:17.55 [INFO ] ReferenceMarkerMatcher    -   TransE (Bordes et al., 2013) 
2300 0.243 4.27 44.1 53.2 
323 0.279 19.8 37.6 44.1
30 Dec 2020 00:17.55 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:17.55 [INFO ] ReferenceMarkerMatcher    -   TransE (Bordes et al., 2013) 
2100 0.401 34.4 47.2 50.1 
6.8 0.309 0.9 64.3 84.1
30 Dec 2020 00:17.57 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:17.57 [INFO ] ReferenceMarkerMatcher    -   TransE (Bordes et al., 2013) 
2300 0.243 4.27 44.1 53.2 
323 0.279 19.8 37.6 44.1
30 Dec 2020 00:17.57 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:17.57 [INFO ] ReferenceMarkerMatcher    -   TransE (Bordes et al., 2013) 
2100 0.401 34.4 47.2 50.1 
6.8 0.309 0.9 64.3 84.1
30 Dec 2020 00:18.00 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:18.00 [INFO ] ReferenceMarkerMatcher    -   TransE (Bordes et al., 2013) 
2300 0.243 4.27 44.1 53.2 
323 0.279 19.8 37.6 44.1
30 Dec 2020 00:18.00 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:18.00 [INFO ] ReferenceMarkerMatcher    -   TransE (Bordes et al., 2013) 
2100 0.401 34.4 47.2 50.1 
6.8 0.309 0.9 64.3 84.1
30 Dec 2020 00:18.02 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:18.02 [INFO ] ReferenceMarkerMatcher    -   TransE (Bordes et al., 2013) 
2300 0.243 4.27 44.1 53.2 
323 0.279 19.8 37.6 44.1
30 Dec 2020 00:18.02 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:18.02 [INFO ] ReferenceMarkerMatcher    -   TransE (Bordes et al., 2013) 
2100 0.401 34.4 47.2 50.1 
6.8 0.309 0.9 64.3 84.1
30 Dec 2020 00:30.04 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:30.04 [INFO ] ReferenceMarkerMatcher    -   Frankle, J. and Carbin, M. (2019). The lottery ticket hypothesis: Finding sparse, trainable neural 
networks. In ICLR 2019.
30 Dec 2020 00:30.04 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:30.04 [INFO ] ReferenceMarkerMatcher    -   Frankle, J., Dziugaite, G. K., Roy, D. M., and Carbin, M. (2019). The lottery ticket hypothesis at 
scale. CoRR, abs/1903.01611.
30 Dec 2020 00:30.06 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:30.06 [INFO ] ReferenceMarkerMatcher    -   Frankle, J. and Carbin, M. (2019). The lottery ticket hypothesis: Finding sparse, trainable neural 
networks. In ICLR 2019.
30 Dec 2020 00:30.06 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:30.06 [INFO ] ReferenceMarkerMatcher    -   Frankle, J., Dziugaite, G. K., Roy, D. M., and Carbin, M. (2019). The lottery ticket hypothesis at 
scale. CoRR, abs/1903.01611.
30 Dec 2020 00:40.07 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:40.07 [INFO ] ReferenceMarkerMatcher    -   , Zhang, and Zhao 2019] Zhou, J.; Zhang, Z.; and 
Zhao, H. 2019. LIMIT-BERT: Linguistic informed multi-
task bert. arXiv preprint arXiv:1910.14296.
30 Dec 2020 00:40.07 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:40.07 [INFO ] ReferenceMarkerMatcher    -   [Zhang et al. 2019] Zhang, Z.; Wang, R.; Chen, K.; Utiyama, 
M.; Sumita, E.; and Zhao, H. 2019. Probing contextualized 
sentence representations with visual awareness. arXiv 
preprint arXiv:1911.02971.
30 Dec 2020 00:40.07 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:40.07 [INFO ] ReferenceMarkerMatcher    -   [Mudrakarta et al. 2018] Mudrakarta, P. K.; Taly, A.; Sun-
dararajan, M.; and Dhamdhere, K. 2018. Did the model 
understand the question? ACL. 
[Nguyen et al. 2016] Nguyen, T.; Rosenberg, M.; Song, X.; 
Gao, J.; Tiwary, S.; Majumder, R.; and Deng, L. 2016. Ms 
marco: A human generated machine reading comprehension 
dataset. ArXiv:1611.09268v2. 
[Radford et al. 2018] Radford, A.; Narasimhan, K.; Sali-
mans, T.; and Sutskever, I. 2018. Improving language 
understanding by generative pre-training. Technical report. 
[Rajpurkar et al. 2016] Rajpurkar, P.; Zhang, J.; Lopyrev, K.; 
and Liang, P. 2016. SQuAD: 100,000+ questions for 
machine comprehension of text. EMNLP. 
[Rajpurkar, Jia, and Liang 2018] Rajpurkar, P.; Jia, R.; and 
Liang, P. 2018. Know what you don't know: Unanswerable 
questions for SQuAD. ACL. 
[Ran et al. 2019] Ran, Q.; Li, P.; Hu, W.; and Zhou, J. 2019. 
Option comparison network for multiple-choice reading 
comprehension. arXiv preprint arXiv:1903.03033. 
[Seo et al. 2016] Seo, M.; Kembhavi, A.; Farhadi, A.; and 
Hajishirzi, H. 2016. Bidirectional attention flow for machine 
comprehension. arXiv preprint arXiv:1611.01603. 
[Strubell et al. 2018] Strubell, E.; Verga, P.; Andor, D.; 
Weiss, D.; and McCallum, A. 2018. Linguistically-informed 
self-attention for semantic role labeling. In EMNLP. 
[Sun et al. 2018] Sun, K.; Yu, D.; Yu, D.; and Cardie, C. 
2018. Improving machine reading comprehension with gen-
eral reading strategies. arXiv preprint arXiv:1810.13441. 
[Vaswani et al. 2017] Vaswani, A.; Shazeer, N.; Parmar, N.; 
Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, ?.; and 
Polosukhin, I. 2017. Attention is all you need. In NIPS. 
[Wang et al. 2017] Wang, W.; Yang, N.; Wei, F.; Chang, B.; 
and Zhou, M. 2017. Gated self-matching networks for 
reading comprehension and question answering. ACL. 
[Wang, Zhang, and Zong 2017] Wang, S.; Zhang, J.; and 
Zong, C. 2017. Learning sentence representation with 
guidance of human attention. In IJCAI.
30 Dec 2020 00:40.08 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:40.08 [INFO ] ReferenceMarkerMatcher    -   , Zhang, and Zhao 2019] Zhou, J.; Zhang, Z.; and 
Zhao, H. 2019. LIMIT-BERT: Linguistic informed multi-
task bert. arXiv preprint arXiv:1910.14296.
30 Dec 2020 00:40.08 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:40.08 [INFO ] ReferenceMarkerMatcher    -   [Zhang et al. 2019] Zhang, Z.; Wang, R.; Chen, K.; Utiyama, 
M.; Sumita, E.; and Zhao, H. 2019. Probing contextualized 
sentence representations with visual awareness. arXiv 
preprint arXiv:1911.02971.
30 Dec 2020 00:40.08 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:40.08 [INFO ] ReferenceMarkerMatcher    -   [Mudrakarta et al. 2018] Mudrakarta, P. K.; Taly, A.; Sun-
dararajan, M.; and Dhamdhere, K. 2018. Did the model 
understand the question? ACL. 
[Nguyen et al. 2016] Nguyen, T.; Rosenberg, M.; Song, X.; 
Gao, J.; Tiwary, S.; Majumder, R.; and Deng, L. 2016. Ms 
marco: A human generated machine reading comprehension 
dataset. ArXiv:1611.09268v2. 
[Radford et al. 2018] Radford, A.; Narasimhan, K.; Sali-
mans, T.; and Sutskever, I. 2018. Improving language 
understanding by generative pre-training. Technical report. 
[Rajpurkar et al. 2016] Rajpurkar, P.; Zhang, J.; Lopyrev, K.; 
and Liang, P. 2016. SQuAD: 100,000+ questions for 
machine comprehension of text. EMNLP. 
[Rajpurkar, Jia, and Liang 2018] Rajpurkar, P.; Jia, R.; and 
Liang, P. 2018. Know what you don't know: Unanswerable 
questions for SQuAD. ACL. 
[Ran et al. 2019] Ran, Q.; Li, P.; Hu, W.; and Zhou, J. 2019. 
Option comparison network for multiple-choice reading 
comprehension. arXiv preprint arXiv:1903.03033. 
[Seo et al. 2016] Seo, M.; Kembhavi, A.; Farhadi, A.; and 
Hajishirzi, H. 2016. Bidirectional attention flow for machine 
comprehension. arXiv preprint arXiv:1611.01603. 
[Strubell et al. 2018] Strubell, E.; Verga, P.; Andor, D.; 
Weiss, D.; and McCallum, A. 2018. Linguistically-informed 
self-attention for semantic role labeling. In EMNLP. 
[Sun et al. 2018] Sun, K.; Yu, D.; Yu, D.; and Cardie, C. 
2018. Improving machine reading comprehension with gen-
eral reading strategies. arXiv preprint arXiv:1810.13441. 
[Vaswani et al. 2017] Vaswani, A.; Shazeer, N.; Parmar, N.; 
Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, ?.; and 
Polosukhin, I. 2017. Attention is all you need. In NIPS. 
[Wang et al. 2017] Wang, W.; Yang, N.; Wei, F.; Chang, B.; 
and Zhou, M. 2017. Gated self-matching networks for 
reading comprehension and question answering. ACL. 
[Wang, Zhang, and Zong 2017] Wang, S.; Zhang, J.; and 
Zong, C. 2017. Learning sentence representation with 
guidance of human attention. In IJCAI.
30 Dec 2020 00:41.19 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:41.19 [INFO ] ReferenceMarkerMatcher    -   [Ding et al., 2018] B. Ding, Q. Wang, B. Wang, and L. Guo. 
Improving knowledge graph embedding using simple con-
straints. In ACL, 2018.
30 Dec 2020 00:41.19 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:41.19 [INFO ] ReferenceMarkerMatcher    -   References 
[Akrami et al., 2018] F. Akrami, L. Guo, W. Hu, and C. Li. 
Re-evaluating embedding-based knowledge graph com-
pletion methods. In ACM-CIKM, 2018. 
[Bordes et al., 2013] A. Bordes, N. Usunier, A. Garcia-
Duran, J. Weston, and O. Yakhnenko. Translating embed-
dings for modeling multi-relational data. In NIPS, 2013. 
[Demeester et al., 2016] T. Demeester, T. Rocktäschel, and 
S. Riedel. Lifted rule injection for relation embeddings. 
arXiv:1606.08359, 2016. 
[Dettmers et al., 2018] T. Dettmers, P. Minervini, P. Stene-
torp, and S. Riedel. Convolutional 2d knowledge graph 
embeddings. In AAAI, 2018. 
[Ding et al., 2018] B. Ding, Q. Wang, B. Wang, and L. Guo. 
Improving knowledge graph embedding using simple con-
straints. In ACL, 2018. 
[Dong et al., 2014] X. Dong, E. Gabrilovich, G. Heitz, 
W. Horn, Ni Lao, K. Murphy, T. Strohmann, S. Sun, and 
W. Zhang. Knowledge vault: A web-scale approach to 
probabilistic knowledge fusion. In ACM-SIGKDD, 2014. 
[Guan et al., 2018] S. Guan, X. Jin, Y. Wang, and X. Cheng. 
Shared embedding based neural networks for knowledge 
graph completion. In 27th ACM-CIKM, 2018. 
[Guo et al., 2016] S. Guo, Q. Wang, L. Wang, B. Wang, and 
Li Guo. Jointly embedding knowledge graphs and logical 
rules. In EMNLP, 2016. 
[Guo et al., 2018] S. Guo, Q. Wang, L. Wang, B. Wang, and 
Li Guo. Knowledge graph embedding with iterative guid-
ance from soft rules. In AAAI, 2018. 
[Guu et al., 2015] K. Guu, J. Miller, and P. Liang. Traversing 
knowledge graphs in vector space. In EMNLP, 2015. 
[Han et al., 2018] X. Han, C. Zhang, T. Sun, Y. Ji, and Z. Hu. 
A triple-branch neural network for knowledge graph em-
bedding. IEEE Access, 6, 2018. 
[Huang et al., 2000] G.B Huang, Y.Q Chen, and H.A Babri. 
Classification ability of single hidden layer feedforward 
neural networks. IEEE TNN, 11(3), 2000. 
[Kazemi and Poole, 2018] S.M Kazemi and D. Poole. Sim-
ple embedding for link prediction in knowledge graphs. 
arXiv:1802.04868, 2018.
30 Dec 2020 00:41.22 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:41.22 [INFO ] ReferenceMarkerMatcher    -   [Ding et al., 2018] B. Ding, Q. Wang, B. Wang, and L. Guo. 
Improving knowledge graph embedding using simple con-
straints. In ACL, 2018.
30 Dec 2020 00:41.22 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:41.22 [INFO ] ReferenceMarkerMatcher    -   References 
[Akrami et al., 2018] F. Akrami, L. Guo, W. Hu, and C. Li. 
Re-evaluating embedding-based knowledge graph com-
pletion methods. In ACM-CIKM, 2018. 
[Bordes et al., 2013] A. Bordes, N. Usunier, A. Garcia-
Duran, J. Weston, and O. Yakhnenko. Translating embed-
dings for modeling multi-relational data. In NIPS, 2013. 
[Demeester et al., 2016] T. Demeester, T. Rocktäschel, and 
S. Riedel. Lifted rule injection for relation embeddings. 
arXiv:1606.08359, 2016. 
[Dettmers et al., 2018] T. Dettmers, P. Minervini, P. Stene-
torp, and S. Riedel. Convolutional 2d knowledge graph 
embeddings. In AAAI, 2018. 
[Ding et al., 2018] B. Ding, Q. Wang, B. Wang, and L. Guo. 
Improving knowledge graph embedding using simple con-
straints. In ACL, 2018. 
[Dong et al., 2014] X. Dong, E. Gabrilovich, G. Heitz, 
W. Horn, Ni Lao, K. Murphy, T. Strohmann, S. Sun, and 
W. Zhang. Knowledge vault: A web-scale approach to 
probabilistic knowledge fusion. In ACM-SIGKDD, 2014. 
[Guan et al., 2018] S. Guan, X. Jin, Y. Wang, and X. Cheng. 
Shared embedding based neural networks for knowledge 
graph completion. In 27th ACM-CIKM, 2018. 
[Guo et al., 2016] S. Guo, Q. Wang, L. Wang, B. Wang, and 
Li Guo. Jointly embedding knowledge graphs and logical 
rules. In EMNLP, 2016. 
[Guo et al., 2018] S. Guo, Q. Wang, L. Wang, B. Wang, and 
Li Guo. Knowledge graph embedding with iterative guid-
ance from soft rules. In AAAI, 2018. 
[Guu et al., 2015] K. Guu, J. Miller, and P. Liang. Traversing 
knowledge graphs in vector space. In EMNLP, 2015. 
[Han et al., 2018] X. Han, C. Zhang, T. Sun, Y. Ji, and Z. Hu. 
A triple-branch neural network for knowledge graph em-
bedding. IEEE Access, 6, 2018. 
[Huang et al., 2000] G.B Huang, Y.Q Chen, and H.A Babri. 
Classification ability of single hidden layer feedforward 
neural networks. IEEE TNN, 11(3), 2000. 
[Kazemi and Poole, 2018] S.M Kazemi and D. Poole. Sim-
ple embedding for link prediction in knowledge graphs. 
arXiv:1802.04868, 2018.
30 Dec 2020 00:49.52 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:49.52 [INFO ] ReferenceMarkerMatcher    -   Xie, R.; Liu, Z.; and Sun, M. 2016. Representation learn-
ing of knowledge graphs with hierarchical types. In IJCAI, 
2965-2971.
30 Dec 2020 00:49.52 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:49.52 [INFO ] ReferenceMarkerMatcher    -   Xie, R.; Liu, Z.; Jia, J.; Luan, H.; and Sun, M. 2016. Repre-
sentation learning of knowledge graphs with entity descrip-
tions. In AAAI.
30 Dec 2020 00:49.52 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:49.52 [INFO ] ReferenceMarkerMatcher    -   Xie, R.; Liu, Z.; and Sun, M. 2016. Representation learn-
ing of knowledge graphs with hierarchical types. In IJCAI, 
2965-2971.
30 Dec 2020 00:49.52 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:49.52 [INFO ] ReferenceMarkerMatcher    -   Xie, R.; Liu, Z.; Jia, J.; Luan, H.; and Sun, M. 2016. Repre-
sentation learning of knowledge graphs with entity descrip-
tions. In AAAI.
30 Dec 2020 00:49.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:49.54 [INFO ] ReferenceMarkerMatcher    -   Xie, R.; Liu, Z.; and Sun, M. 2016. Representation learn-
ing of knowledge graphs with hierarchical types. In IJCAI, 
2965-2971.
30 Dec 2020 00:49.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:49.54 [INFO ] ReferenceMarkerMatcher    -   Xie, R.; Liu, Z.; Jia, J.; Luan, H.; and Sun, M. 2016. Repre-
sentation learning of knowledge graphs with entity descrip-
tions. In AAAI.
30 Dec 2020 00:49.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:49.54 [INFO ] ReferenceMarkerMatcher    -   Xie, R.; Liu, Z.; and Sun, M. 2016. Representation learn-
ing of knowledge graphs with hierarchical types. In IJCAI, 
2965-2971.
30 Dec 2020 00:49.54 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:49.54 [INFO ] ReferenceMarkerMatcher    -   Xie, R.; Liu, Z.; Jia, J.; Luan, H.; and Sun, M. 2016. Repre-
sentation learning of knowledge graphs with entity descrip-
tions. In AAAI.
30 Dec 2020 00:53.52 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:53.52 [INFO ] ReferenceMarkerMatcher    -   CoRR, abs/1708.00107, 2017.
30 Dec 2020 00:53.52 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:53.52 [INFO ] ReferenceMarkerMatcher    -   CoRR, abs/1710.03740, 2017.
30 Dec 2020 00:53.52 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:53.52 [INFO ] ReferenceMarkerMatcher    -   Radford, A., Józefowicz, R., and Sutskever, I. Learning 
to generate reviews and discovering sentiment. CoRR, 
abs/1704.01444, 2017.
30 Dec 2020 00:53.52 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:53.52 [INFO ] ReferenceMarkerMatcher    -   Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, 
M., and Tang, P. T. P. On large-batch training for deep 
learning: Generalization gap and sharp minima. ICLR, 
2017.
30 Dec 2020 00:53.52 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:53.52 [INFO ] ReferenceMarkerMatcher    -   Lai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. Race: 
Large-scale reading comprehension dataset from exami-
nations. arXiv:1704.04683, 2017.
30 Dec 2020 00:53.55 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:53.55 [INFO ] ReferenceMarkerMatcher    -   CoRR, abs/1708.00107, 2017.
30 Dec 2020 00:53.55 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:53.55 [INFO ] ReferenceMarkerMatcher    -   CoRR, abs/1710.03740, 2017.
30 Dec 2020 00:53.55 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:53.55 [INFO ] ReferenceMarkerMatcher    -   Radford, A., Józefowicz, R., and Sutskever, I. Learning 
to generate reviews and discovering sentiment. CoRR, 
abs/1704.01444, 2017.
30 Dec 2020 00:53.55 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:53.55 [INFO ] ReferenceMarkerMatcher    -   Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, 
M., and Tang, P. T. P. On large-batch training for deep 
learning: Generalization gap and sharp minima. ICLR, 
2017.
30 Dec 2020 00:53.55 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:53.55 [INFO ] ReferenceMarkerMatcher    -   Lai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. Race: 
Large-scale reading comprehension dataset from exami-
nations. arXiv:1704.04683, 2017.
30 Dec 2020 00:55.21 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:55.21 [INFO ] ReferenceMarkerMatcher    -   Adam Kilgarriff. 2001. English lex-
ical sample task description. In Proceedings of 
SENSEVAL-2 Second International Workshop on 
Evaluating Word Sense Disambiguation Systems, 
pages 17-20, Toulouse, France.
30 Dec 2020 00:55.21 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:55.21 [INFO ] ReferenceMarkerMatcher    -   [Devlin et al.2019] Jacob Devlin, Ming-Wei Chang, 
Kenton Lee, and Kristina Toutanova. 2019. BERT: 
Pre-training of deep bidirectional transformers for 
language understanding. In Proceedings of the 
2019 Conference of the North American Chapter 
of the Association for Computational Linguistics: 
Human Language Technologies, pages 4171-4186, 
Minneapolis, MN, USA. 
[Mihalcea et al.2004] Rada 
Mihalcea, 
Timothy 
Chklovski, and Adam Kilgarriff. 2004. The 
senseval-3 English lexical sample task. In Pro-
ceedings of SENSEVAL-3, the Third International 
Workshop on the Evaluation of Systems for the 
Semantic Analysis of Text, pages 25-28, Barcelona, 
Spain. 
[Edmonds and Cotton2001] Philip Edmonds and Scott 
Cotton. 2001. SENSEVAL-2: Overview. In 
Proceedings of SENSEVAL-2: Second International 
Workshop on Evaluating Word Sense Disambigua-
tion Systems, pages 1-5, Toulouse, France. 
[Mikolov et al.2013] Tomas Mikolov, Kai Chen, Greg 
Corrado, and Jeffrey Dean. 2013. Efficient esti-
mation of word representations in vector space. In 
Yoshua Bengio and Yann LeCun, editors, 1st Inter-
national Conference on Learning Representations, 
ICLR 2013, Scottsdale, AZ, USA.
30 Dec 2020 00:55.23 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:55.23 [INFO ] ReferenceMarkerMatcher    -   Adam Kilgarriff. 2001. English lex-
ical sample task description. In Proceedings of 
SENSEVAL-2 Second International Workshop on 
Evaluating Word Sense Disambiguation Systems, 
pages 17-20, Toulouse, France.
30 Dec 2020 00:55.23 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:55.23 [INFO ] ReferenceMarkerMatcher    -   [Devlin et al.2019] Jacob Devlin, Ming-Wei Chang, 
Kenton Lee, and Kristina Toutanova. 2019. BERT: 
Pre-training of deep bidirectional transformers for 
language understanding. In Proceedings of the 
2019 Conference of the North American Chapter 
of the Association for Computational Linguistics: 
Human Language Technologies, pages 4171-4186, 
Minneapolis, MN, USA. 
[Mihalcea et al.2004] Rada 
Mihalcea, 
Timothy 
Chklovski, and Adam Kilgarriff. 2004. The 
senseval-3 English lexical sample task. In Pro-
ceedings of SENSEVAL-3, the Third International 
Workshop on the Evaluation of Systems for the 
Semantic Analysis of Text, pages 25-28, Barcelona, 
Spain. 
[Edmonds and Cotton2001] Philip Edmonds and Scott 
Cotton. 2001. SENSEVAL-2: Overview. In 
Proceedings of SENSEVAL-2: Second International 
Workshop on Evaluating Word Sense Disambigua-
tion Systems, pages 1-5, Toulouse, France. 
[Mikolov et al.2013] Tomas Mikolov, Kai Chen, Greg 
Corrado, and Jeffrey Dean. 2013. Efficient esti-
mation of word representations in vector space. In 
Yoshua Bengio and Yann LeCun, editors, 1st Inter-
national Conference on Learning Representations, 
ICLR 2013, Scottsdale, AZ, USA.
30 Dec 2020 00:56.07 [ERROR] FullTextParser            - DocumentPointer for block 68 points to 194 token, but block token size is 178
30 Dec 2020 00:56.10 [ERROR] FullTextParser            - DocumentPointer for block 68 points to 194 token, but block token size is 178
30 Dec 2020 00:56.12 [ERROR] FullTextParser            - DocumentPointer for block 68 points to 194 token, but block token size is 178
30 Dec 2020 00:56.17 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:56.17 [INFO ] ReferenceMarkerMatcher    -   Berthelot*, D., Raffel*, C., Roy, A., and Goodfellow, I. Un-
derstanding and improving interpolation in autoencoders 
via an adversarial regularizer. In International Confer-
ence on Learning Representations, 2019. URL https: 
//openreview.net/forum?id=S1fQSiCcYm.
30 Dec 2020 00:56.17 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:56.17 [INFO ] ReferenceMarkerMatcher    -   Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., 
Oliver, A., and Raffel, C. MixMatch: A Holistic Ap-
proach to Semi-Supervised Learning. arXiv e-prints, art. 
arXiv:1905.02249, May 2019.
30 Dec 2020 00:56.17 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:56.17 [INFO ] ReferenceMarkerMatcher    -   Berthelot*, D., Raffel*, C., Roy, A., and Goodfellow, I. Un-
derstanding and improving interpolation in autoencoders 
via an adversarial regularizer. In International Confer-
ence on Learning Representations, 2019. URL https: 
//openreview.net/forum?id=S1fQSiCcYm.
30 Dec 2020 00:56.17 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:56.17 [INFO ] ReferenceMarkerMatcher    -   Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., 
Oliver, A., and Raffel, C. MixMatch: A Holistic Ap-
proach to Semi-Supervised Learning. arXiv e-prints, art. 
arXiv:1905.02249, May 2019.
30 Dec 2020 00:56.20 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:56.20 [INFO ] ReferenceMarkerMatcher    -   Berthelot*, D., Raffel*, C., Roy, A., and Goodfellow, I. Un-
derstanding and improving interpolation in autoencoders 
via an adversarial regularizer. In International Confer-
ence on Learning Representations, 2019. URL https: 
//openreview.net/forum?id=S1fQSiCcYm.
30 Dec 2020 00:56.20 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:56.20 [INFO ] ReferenceMarkerMatcher    -   Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., 
Oliver, A., and Raffel, C. MixMatch: A Holistic Ap-
proach to Semi-Supervised Learning. arXiv e-prints, art. 
arXiv:1905.02249, May 2019.
30 Dec 2020 00:56.20 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:56.20 [INFO ] ReferenceMarkerMatcher    -   Berthelot*, D., Raffel*, C., Roy, A., and Goodfellow, I. Un-
derstanding and improving interpolation in autoencoders 
via an adversarial regularizer. In International Confer-
ence on Learning Representations, 2019. URL https: 
//openreview.net/forum?id=S1fQSiCcYm.
30 Dec 2020 00:56.20 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:56.20 [INFO ] ReferenceMarkerMatcher    -   Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., 
Oliver, A., and Raffel, C. MixMatch: A Holistic Ap-
proach to Semi-Supervised Learning. arXiv e-prints, art. 
arXiv:1905.02249, May 2019.
30 Dec 2020 00:57.15 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:57.15 [INFO ] ReferenceMarkerMatcher    -   SQuAD SQuAD is an extractive question answering dataset built from Wikipedia. The answers 
are segments from the context paragraphs and the task is to predict answer spans. We evaluate our 
models on two versions of SQuAD: v1.1 and v2.0. SQuAD v1.1 has 100,000 human-annotated 
question/answer pairs. SQuAD v2.0 additionally introduced 50,000 unanswerable questions. For 
SQuAD v1.1, we use the same training procedure as BERT, whereas for SQuAD v2.0, models are 
jointly trained with a span extraction loss and an additional classifier for predicting answerabil-
ity (Yang et al., 2019; Liu et al., 2019). We report both development set and test set performance.
30 Dec 2020 00:57.15 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:57.15 [INFO ] ReferenceMarkerMatcher    -   RACE RACE is a large-scale dataset for multi-choice reading comprehension, collected from En-
glish examinations in China with nearly 100,000 questions. Each instance in RACE has 4 candidate 
answers. Following prior work (Yang et al., 2019; Liu et al., 2019), we use the concatenation of the 
passage, question, and each candidate answer as the input to models. Then, we use the represen-
tations from the "[CLS]" token for predicting the probability of each answer. The dataset consists 
of two domains: middle school and high school. We train our models on both domains and report 
accuracies on both the development set and test set.
30 Dec 2020 00:57.15 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:57.15 [INFO ] ReferenceMarkerMatcher    -   Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Neural Information 
Processing Systems (NeurIPS), 2019.
30 Dec 2020 00:57.15 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:57.15 [INFO ] ReferenceMarkerMatcher    -   Allen Nie, Erin Bennett, and Noah Goodman. DisSent: Learning sentence representations from ex-
plicit discourse relations. In Proceedings of the 57th Annual Meeting of the Association for Com-
putational Linguistics, pp. 4497-4510, Florence, Italy, July 2019. Association for Computational 
Linguistics. doi: 10.18653/v1/P19-1442. URL https://www.aclweb.org/anthology/ 
P19-1442.
30 Dec 2020 00:57.15 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:57.15 [INFO ] ReferenceMarkerMatcher    -   Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse 
transformers. arXiv preprint arXiv:1904.10509, 2019.
30 Dec 2020 00:57.15 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:57.15 [INFO ] ReferenceMarkerMatcher    -   Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D Manning, and Quoc V 
Le. Bam! born-again multi-task networks for natural language understanding. arXiv preprint 
arXiv:1907.04829, 2019.
30 Dec 2020 00:57.15 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:57.15 [INFO ] ReferenceMarkerMatcher    -   Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep 
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of 
the North American Chapter of the Association for Computational Linguistics: Human Language 
Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, June 
2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: 
//www.aclweb.org/anthology/N19-1423.
30 Dec 2020 00:57.15 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:57.15 [INFO ] ReferenceMarkerMatcher    -   Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. Efficient training of bert 
by progressively stacking. In International Conference on Machine Learning, pp. 2337-2346, 
2019.
30 Dec 2020 00:57.15 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:57.15 [INFO ] ReferenceMarkerMatcher    -   Jie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, and Zhaopeng Tu. Modeling 
recurrence for transformer. Proceedings of the 2019 Conference of the North, 2019. doi: 10. 
18653/v1/n19-1122. URL http://dx.doi.org/10.18653/v1/n19-1122.
30 Dec 2020 00:57.15 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:57.15 [INFO ] ReferenceMarkerMatcher    -   Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. 
SpanBERT: Improving pre-training by representing and predicting spans. arXiv preprint 
arXiv:1907.10529, 2019.
30 Dec 2020 00:57.19 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:57.19 [INFO ] ReferenceMarkerMatcher    -   SQuAD SQuAD is an extractive question answering dataset built from Wikipedia. The answers 
are segments from the context paragraphs and the task is to predict answer spans. We evaluate our 
models on two versions of SQuAD: v1.1 and v2.0. SQuAD v1.1 has 100,000 human-annotated 
question/answer pairs. SQuAD v2.0 additionally introduced 50,000 unanswerable questions. For 
SQuAD v1.1, we use the same training procedure as BERT, whereas for SQuAD v2.0, models are 
jointly trained with a span extraction loss and an additional classifier for predicting answerabil-
ity (Yang et al., 2019; Liu et al., 2019). We report both development set and test set performance.
30 Dec 2020 00:57.19 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:57.19 [INFO ] ReferenceMarkerMatcher    -   RACE RACE is a large-scale dataset for multi-choice reading comprehension, collected from En-
glish examinations in China with nearly 100,000 questions. Each instance in RACE has 4 candidate 
answers. Following prior work (Yang et al., 2019; Liu et al., 2019), we use the concatenation of the 
passage, question, and each candidate answer as the input to models. Then, we use the represen-
tations from the "[CLS]" token for predicting the probability of each answer. The dataset consists 
of two domains: middle school and high school. We train our models on both domains and report 
accuracies on both the development set and test set.
30 Dec 2020 00:57.19 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:57.19 [INFO ] ReferenceMarkerMatcher    -   Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Neural Information 
Processing Systems (NeurIPS), 2019.
30 Dec 2020 00:57.19 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:57.19 [INFO ] ReferenceMarkerMatcher    -   Allen Nie, Erin Bennett, and Noah Goodman. DisSent: Learning sentence representations from ex-
plicit discourse relations. In Proceedings of the 57th Annual Meeting of the Association for Com-
putational Linguistics, pp. 4497-4510, Florence, Italy, July 2019. Association for Computational 
Linguistics. doi: 10.18653/v1/P19-1442. URL https://www.aclweb.org/anthology/ 
P19-1442.
30 Dec 2020 00:57.19 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:57.19 [INFO ] ReferenceMarkerMatcher    -   Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse 
transformers. arXiv preprint arXiv:1904.10509, 2019.
30 Dec 2020 00:57.19 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:57.19 [INFO ] ReferenceMarkerMatcher    -   Kevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D Manning, and Quoc V 
Le. Bam! born-again multi-task networks for natural language understanding. arXiv preprint 
arXiv:1907.04829, 2019.
30 Dec 2020 00:57.19 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:57.19 [INFO ] ReferenceMarkerMatcher    -   Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep 
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of 
the North American Chapter of the Association for Computational Linguistics: Human Language 
Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, June 
2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: 
//www.aclweb.org/anthology/N19-1423.
30 Dec 2020 00:57.19 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:57.19 [INFO ] ReferenceMarkerMatcher    -   Linyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu. Efficient training of bert 
by progressively stacking. In International Conference on Machine Learning, pp. 2337-2346, 
2019.
30 Dec 2020 00:57.19 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:57.19 [INFO ] ReferenceMarkerMatcher    -   Jie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, and Zhaopeng Tu. Modeling 
recurrence for transformer. Proceedings of the 2019 Conference of the North, 2019. doi: 10. 
18653/v1/n19-1122. URL http://dx.doi.org/10.18653/v1/n19-1122.
30 Dec 2020 00:57.19 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 00:57.19 [INFO ] ReferenceMarkerMatcher    -   Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. 
SpanBERT: Improving pre-training by representing and predicting spans. arXiv preprint 
arXiv:1907.10529, 2019.
30 Dec 2020 01:12.27 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:12.27 [INFO ] ReferenceMarkerMatcher    -   Liao, M.; Shi, B.; and Bai, X. 2018. Textboxes++: A single-
shot oriented scene text detector. IEEE Trans. Image Processing 
27(8):3676-3690.
30 Dec 2020 01:12.27 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:12.27 [INFO ] ReferenceMarkerMatcher    -   Liao, M.; Zhu, Z.; Shi, B.; Xia, G.; and Bai, X. 2018. Rotation-
sensitive regression for oriented scene text detection. In Proc. 
CVPR, 5909-5918.
30 Dec 2020 01:12.27 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:12.27 [INFO ] ReferenceMarkerMatcher    -   Liao, M.; Shi, B.; and Bai, X. 2018. Textboxes++: A single-
shot oriented scene text detector. IEEE Trans. Image Processing 
27(8):3676-3690.
30 Dec 2020 01:12.27 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:12.27 [INFO ] ReferenceMarkerMatcher    -   Liao, M.; Zhu, Z.; Shi, B.; Xia, G.; and Bai, X. 2018. Rotation-
sensitive regression for oriented scene text detection. In Proc. 
CVPR, 5909-5918.
30 Dec 2020 01:12.30 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:12.30 [INFO ] ReferenceMarkerMatcher    -   Liao, M.; Shi, B.; and Bai, X. 2018. Textboxes++: A single-
shot oriented scene text detector. IEEE Trans. Image Processing 
27(8):3676-3690.
30 Dec 2020 01:12.30 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:12.30 [INFO ] ReferenceMarkerMatcher    -   Liao, M.; Zhu, Z.; Shi, B.; Xia, G.; and Bai, X. 2018. Rotation-
sensitive regression for oriented scene text detection. In Proc. 
CVPR, 5909-5918.
30 Dec 2020 01:12.30 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:12.30 [INFO ] ReferenceMarkerMatcher    -   Liao, M.; Shi, B.; and Bai, X. 2018. Textboxes++: A single-
shot oriented scene text detector. IEEE Trans. Image Processing 
27(8):3676-3690.
30 Dec 2020 01:12.30 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:12.30 [INFO ] ReferenceMarkerMatcher    -   Liao, M.; Zhu, Z.; Shi, B.; Xia, G.; and Bai, X. 2018. Rotation-
sensitive regression for oriented scene text detection. In Proc. 
CVPR, 5909-5918.
30 Dec 2020 01:14.17 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:14.17 [INFO ] ReferenceMarkerMatcher    -   Frankle, J. and Carbin, M. The lottery ticket hypothe-
sis: Finding sparse, trainable neural networks. In In-
ternational Conference on Learning Representations, 
2019. URL https://openreview.net/forum? 
id=rJl-b3RcF7.
30 Dec 2020 01:14.17 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:14.17 [INFO ] ReferenceMarkerMatcher    -   Frankle, J., Dziugaite, G. K., Roy, D. M., and Carbin, M. 
The lottery ticket hypothesis at scale. ArXiv, 2019. URL 
http://arxiv.org/abs/1903.01611.
30 Dec 2020 01:14.17 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:14.17 [INFO ] ReferenceMarkerMatcher    -   Frankle, J. and Carbin, M. The lottery ticket hypothe-
sis: Finding sparse, trainable neural networks. In In-
ternational Conference on Learning Representations, 
2019. URL https://openreview.net/forum? 
id=rJl-b3RcF7.
30 Dec 2020 01:14.17 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:14.17 [INFO ] ReferenceMarkerMatcher    -   Frankle, J., Dziugaite, G. K., Roy, D. M., and Carbin, M. 
The lottery ticket hypothesis at scale. ArXiv, 2019. URL 
http://arxiv.org/abs/1903.01611.
30 Dec 2020 01:14.21 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:14.21 [INFO ] ReferenceMarkerMatcher    -   Frankle, J. and Carbin, M. The lottery ticket hypothe-
sis: Finding sparse, trainable neural networks. In In-
ternational Conference on Learning Representations, 
2019. URL https://openreview.net/forum? 
id=rJl-b3RcF7.
30 Dec 2020 01:14.21 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:14.21 [INFO ] ReferenceMarkerMatcher    -   Frankle, J., Dziugaite, G. K., Roy, D. M., and Carbin, M. 
The lottery ticket hypothesis at scale. ArXiv, 2019. URL 
http://arxiv.org/abs/1903.01611.
30 Dec 2020 01:14.21 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:14.21 [INFO ] ReferenceMarkerMatcher    -   Frankle, J. and Carbin, M. The lottery ticket hypothe-
sis: Finding sparse, trainable neural networks. In In-
ternational Conference on Learning Representations, 
2019. URL https://openreview.net/forum? 
id=rJl-b3RcF7.
30 Dec 2020 01:14.21 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:14.21 [INFO ] ReferenceMarkerMatcher    -   Frankle, J., Dziugaite, G. K., Roy, D. M., and Carbin, M. 
The lottery ticket hypothesis at scale. ArXiv, 2019. URL 
http://arxiv.org/abs/1903.01611.
30 Dec 2020 01:17.34 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:17.34 [INFO ] ReferenceMarkerMatcher    -   [Deng et al. 2019] Deng, J.; Guo, J.; Xue, N.; and Zafeiriou, 
S. 2019. Arcface: Additive angular margin loss for deep 
face recognition. In CVPR. 
[Fu et al. 2019] Fu, Y.; Wei, Y.; Zhou, Y.; Shi, H.; Huang, G.; 
Wang, X.; Yao, Z.; and Huang, T. 2019. Horizontal pyramid 
matching for person re-identification. In AAAI, volume 33, 
8295-8302. 
[Kalayeh et al. 2018] Kalayeh, M. M.; Basaran, E.; Gkmen, 
M.; Kamasak, M. E.; and Shah, M. 2018. Human semantic 
parsing for person re-identification. In CVPR.
30 Dec 2020 01:17.34 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:17.34 [INFO ] ReferenceMarkerMatcher    -   [Zhang et al. 2019] Zhang, Z.; Lan, C.; Zeng, W.; and Chen, 
Z. 
2019. 
Densely semantically aligned person re-
identification. In CVPR. 
[Zhao et al. 2017] Zhao, L.; Li, X.; Zhuang, Y.; and Wang, J. 
2017. Deeply-learned part-aligned representations for per-
son re-identification. In The IEEE International Conference 
on Computer Vision. 
[Zheng et al. 2015] Zheng, L.; Shen, L.; Tian, L.; Wang, 
S.; Wang, J.; and Tian, Q. 2015. Scalable person re-
identification: A benchmark. In The IEEE Conference on 
Computer Vision and Pattern Recognition. 
[Zheng et al. 2019a] Zheng, F.; Deng, C.; Sun, X.; Jiang, X.; 
Guo, X.; Yu, Z.; Huang, F.; and Ji, R. 2019a. Pyramidal 
person re-identification via multi-loss dynamic training. In 
CVPR.
30 Dec 2020 01:17.36 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:17.36 [INFO ] ReferenceMarkerMatcher    -   [Deng et al. 2019] Deng, J.; Guo, J.; Xue, N.; and Zafeiriou, 
S. 2019. Arcface: Additive angular margin loss for deep 
face recognition. In CVPR. 
[Fu et al. 2019] Fu, Y.; Wei, Y.; Zhou, Y.; Shi, H.; Huang, G.; 
Wang, X.; Yao, Z.; and Huang, T. 2019. Horizontal pyramid 
matching for person re-identification. In AAAI, volume 33, 
8295-8302. 
[Kalayeh et al. 2018] Kalayeh, M. M.; Basaran, E.; Gkmen, 
M.; Kamasak, M. E.; and Shah, M. 2018. Human semantic 
parsing for person re-identification. In CVPR.
30 Dec 2020 01:17.36 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:17.36 [INFO ] ReferenceMarkerMatcher    -   [Zhang et al. 2019] Zhang, Z.; Lan, C.; Zeng, W.; and Chen, 
Z. 
2019. 
Densely semantically aligned person re-
identification. In CVPR. 
[Zhao et al. 2017] Zhao, L.; Li, X.; Zhuang, Y.; and Wang, J. 
2017. Deeply-learned part-aligned representations for per-
son re-identification. In The IEEE International Conference 
on Computer Vision. 
[Zheng et al. 2015] Zheng, L.; Shen, L.; Tian, L.; Wang, 
S.; Wang, J.; and Tian, Q. 2015. Scalable person re-
identification: A benchmark. In The IEEE Conference on 
Computer Vision and Pattern Recognition. 
[Zheng et al. 2019a] Zheng, F.; Deng, C.; Sun, X.; Jiang, X.; 
Guo, X.; Yu, Z.; Huang, F.; and Ji, R. 2019a. Pyramidal 
person re-identification via multi-loss dynamic training. In 
CVPR.
30 Dec 2020 01:24.35 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:24.35 [INFO ] ReferenceMarkerMatcher    -   Zhang, S. and Bansal, M. Addressing semantic drift in ques-
tion generation for semi-supervised question answering. 
arXiv preprint arXiv:1909.06356, 2019.
30 Dec 2020 01:24.35 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:24.35 [INFO ] ReferenceMarkerMatcher    -   Zhang, J., Zhao, Y., Saleh, M., and Liu, P. J. Pegasus: 
Pre-training with extracted gap-sentences for abstractive 
summarization. arXiv preprint arXiv:1912.08777, 2019.
30 Dec 2020 01:24.35 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:24.35 [INFO ] ReferenceMarkerMatcher    -   Zhang, S. and Bansal, M. Addressing semantic drift in ques-
tion generation for semi-supervised question answering. 
arXiv preprint arXiv:1909.06356, 2019.
30 Dec 2020 01:24.35 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:24.35 [INFO ] ReferenceMarkerMatcher    -   Zhang, J., Zhao, Y., Saleh, M., and Liu, P. J. Pegasus: 
Pre-training with extracted gap-sentences for abstractive 
summarization. arXiv preprint arXiv:1912.08777, 2019.
30 Dec 2020 01:24.38 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:24.38 [INFO ] ReferenceMarkerMatcher    -   Zhang, S. and Bansal, M. Addressing semantic drift in ques-
tion generation for semi-supervised question answering. 
arXiv preprint arXiv:1909.06356, 2019.
30 Dec 2020 01:24.38 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:24.38 [INFO ] ReferenceMarkerMatcher    -   Zhang, J., Zhao, Y., Saleh, M., and Liu, P. J. Pegasus: 
Pre-training with extracted gap-sentences for abstractive 
summarization. arXiv preprint arXiv:1912.08777, 2019.
30 Dec 2020 01:24.38 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:24.38 [INFO ] ReferenceMarkerMatcher    -   Zhang, S. and Bansal, M. Addressing semantic drift in ques-
tion generation for semi-supervised question answering. 
arXiv preprint arXiv:1909.06356, 2019.
30 Dec 2020 01:24.38 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 01:24.38 [INFO ] ReferenceMarkerMatcher    -   Zhang, J., Zhao, Y., Saleh, M., and Liu, P. J. Pegasus: 
Pre-training with extracted gap-sentences for abstractive 
summarization. arXiv preprint arXiv:1912.08777, 2019.
30 Dec 2020 01:26.00 [INFO ] PDF2XMLAnnotationSaxHandler - the link annotation type is not recognized: GoToR
30 Dec 2020 01:26.00 [INFO ] PDF2XMLAnnotationSaxHandler - the link annotation type is not recognized: GoToR
30 Dec 2020 01:26.02 [INFO ] PDF2XMLAnnotationSaxHandler - the link annotation type is not recognized: GoToR
30 Dec 2020 01:26.02 [INFO ] PDF2XMLAnnotationSaxHandler - the link annotation type is not recognized: GoToR
30 Dec 2020 09:09.15 [INFO ] PDF2XMLAnnotationSaxHandler - the link annotation type is not recognized: GoToR
30 Dec 2020 09:09.18 [INFO ] PDF2XMLAnnotationSaxHandler - the link annotation type is not recognized: GoToR
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    -   Behrmann, J., Grathwohl, W., Chen, R. T., Duvenaud, D., 
and Jacobsen, J.-H. Invertible residual networks. In 
International Conference on Machine Learning, pp. 573-
582, 2019.
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    -   Chen, T. Q., Behrmann, J., Duvenaud, D. K., and Jacobsen, 
J.-H. Residual flows for invertible generative modeling. 
In Advances in Neural Information Processing Systems, 
pp. 9913-9923, 2019.
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    -   Ho, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel, P. 
Flow++: Improving flow-based generative models with 
variational dequantization and architecture design. In 
International Conference on Machine Learning, pp. 2722-
2730, 2019.
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    -   Chen, X., Kingma, D. P., Salimans, T., Duan, Y., Dhariwal, 
P., Schulman, J., Sutskever, I., and Abbeel, P. Varia-
Hoogeboom, E., Van Den Berg, R., and Welling, M. Emerg-
ing convolutions for generative normalizing flows. In 
International Conference on Machine Learning, pp. 2771-
2780, 2019.
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    -   Behrmann, J., Grathwohl, W., Chen, R. T., Duvenaud, D., 
and Jacobsen, J.-H. Invertible residual networks. In 
International Conference on Machine Learning, pp. 573-
582, 2019.
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    -   Chen, T. Q., Behrmann, J., Duvenaud, D. K., and Jacobsen, 
J.-H. Residual flows for invertible generative modeling. 
In Advances in Neural Information Processing Systems, 
pp. 9913-9923, 2019.
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    -   Ho, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel, P. 
Flow++: Improving flow-based generative models with 
variational dequantization and architecture design. In 
International Conference on Machine Learning, pp. 2722-
2730, 2019.
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    -   Chen, X., Kingma, D. P., Salimans, T., Duan, Y., Dhariwal, 
P., Schulman, J., Sutskever, I., and Abbeel, P. Varia-
Hoogeboom, E., Van Den Berg, R., and Welling, M. Emerg-
ing convolutions for generative normalizing flows. In 
International Conference on Machine Learning, pp. 2771-
2780, 2019.
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    -   Behrmann, J., Grathwohl, W., Chen, R. T., Duvenaud, D., 
and Jacobsen, J.-H. Invertible residual networks. In 
International Conference on Machine Learning, pp. 573-
582, 2019.
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    -   Chen, T. Q., Behrmann, J., Duvenaud, D. K., and Jacobsen, 
J.-H. Residual flows for invertible generative modeling. 
In Advances in Neural Information Processing Systems, 
pp. 9913-9923, 2019.
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    -   Ho, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel, P. 
Flow++: Improving flow-based generative models with 
variational dequantization and architecture design. In 
International Conference on Machine Learning, pp. 2722-
2730, 2019.
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    -   Chen, X., Kingma, D. P., Salimans, T., Duan, Y., Dhariwal, 
P., Schulman, J., Sutskever, I., and Abbeel, P. Varia-
Hoogeboom, E., Van Den Berg, R., and Welling, M. Emerg-
ing convolutions for generative normalizing flows. In 
International Conference on Machine Learning, pp. 2771-
2780, 2019.
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    -   Behrmann, J., Grathwohl, W., Chen, R. T., Duvenaud, D., 
and Jacobsen, J.-H. Invertible residual networks. In 
International Conference on Machine Learning, pp. 573-
582, 2019.
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    -   Chen, T. Q., Behrmann, J., Duvenaud, D. K., and Jacobsen, 
J.-H. Residual flows for invertible generative modeling. 
In Advances in Neural Information Processing Systems, 
pp. 9913-9923, 2019.
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    -   Ho, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel, P. 
Flow++: Improving flow-based generative models with 
variational dequantization and architecture design. In 
International Conference on Machine Learning, pp. 2722-
2730, 2019.
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    -   Chen, X., Kingma, D. P., Salimans, T., Duan, Y., Dhariwal, 
P., Schulman, J., Sutskever, I., and Abbeel, P. Varia-
Hoogeboom, E., Van Den Berg, R., and Welling, M. Emerg-
ing convolutions for generative normalizing flows. In 
International Conference on Machine Learning, pp. 2771-
2780, 2019.
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    -   Behrmann, J., Grathwohl, W., Chen, R. T., Duvenaud, D., 
and Jacobsen, J.-H. Invertible residual networks. In 
International Conference on Machine Learning, pp. 573-
582, 2019.
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    -   Chen, T. Q., Behrmann, J., Duvenaud, D. K., and Jacobsen, 
J.-H. Residual flows for invertible generative modeling. 
In Advances in Neural Information Processing Systems, 
pp. 9913-9923, 2019.
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    -   Ho, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel, P. 
Flow++: Improving flow-based generative models with 
variational dequantization and architecture design. In 
International Conference on Machine Learning, pp. 2722-
2730, 2019.
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    -   Chen, X., Kingma, D. P., Salimans, T., Duan, Y., Dhariwal, 
P., Schulman, J., Sutskever, I., and Abbeel, P. Varia-
Hoogeboom, E., Van Den Berg, R., and Welling, M. Emerg-
ing convolutions for generative normalizing flows. In 
International Conference on Machine Learning, pp. 2771-
2780, 2019.
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    -   Behrmann, J., Grathwohl, W., Chen, R. T., Duvenaud, D., 
and Jacobsen, J.-H. Invertible residual networks. In 
International Conference on Machine Learning, pp. 573-
582, 2019.
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    -   Chen, T. Q., Behrmann, J., Duvenaud, D. K., and Jacobsen, 
J.-H. Residual flows for invertible generative modeling. 
In Advances in Neural Information Processing Systems, 
pp. 9913-9923, 2019.
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    -   Ho, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel, P. 
Flow++: Improving flow-based generative models with 
variational dequantization and architecture design. In 
International Conference on Machine Learning, pp. 2722-
2730, 2019.
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.56 [INFO ] ReferenceMarkerMatcher    -   Chen, X., Kingma, D. P., Salimans, T., Duan, Y., Dhariwal, 
P., Schulman, J., Sutskever, I., and Abbeel, P. Varia-
Hoogeboom, E., Van Den Berg, R., and Welling, M. Emerg-
ing convolutions for generative normalizing flows. In 
International Conference on Machine Learning, pp. 2771-
2780, 2019.
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    -   Behrmann, J., Grathwohl, W., Chen, R. T., Duvenaud, D., 
and Jacobsen, J.-H. Invertible residual networks. In 
International Conference on Machine Learning, pp. 573-
582, 2019.
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    -   Chen, T. Q., Behrmann, J., Duvenaud, D. K., and Jacobsen, 
J.-H. Residual flows for invertible generative modeling. 
In Advances in Neural Information Processing Systems, 
pp. 9913-9923, 2019.
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    -   Ho, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel, P. 
Flow++: Improving flow-based generative models with 
variational dequantization and architecture design. In 
International Conference on Machine Learning, pp. 2722-
2730, 2019.
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    -   Chen, X., Kingma, D. P., Salimans, T., Duan, Y., Dhariwal, 
P., Schulman, J., Sutskever, I., and Abbeel, P. Varia-
Hoogeboom, E., Van Den Berg, R., and Welling, M. Emerg-
ing convolutions for generative normalizing flows. In 
International Conference on Machine Learning, pp. 2771-
2780, 2019.
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    -   Behrmann, J., Grathwohl, W., Chen, R. T., Duvenaud, D., 
and Jacobsen, J.-H. Invertible residual networks. In 
International Conference on Machine Learning, pp. 573-
582, 2019.
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    -   Chen, T. Q., Behrmann, J., Duvenaud, D. K., and Jacobsen, 
J.-H. Residual flows for invertible generative modeling. 
In Advances in Neural Information Processing Systems, 
pp. 9913-9923, 2019.
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    -   Ho, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel, P. 
Flow++: Improving flow-based generative models with 
variational dequantization and architecture design. In 
International Conference on Machine Learning, pp. 2722-
2730, 2019.
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    -   Chen, X., Kingma, D. P., Salimans, T., Duan, Y., Dhariwal, 
P., Schulman, J., Sutskever, I., and Abbeel, P. Varia-
Hoogeboom, E., Van Den Berg, R., and Welling, M. Emerg-
ing convolutions for generative normalizing flows. In 
International Conference on Machine Learning, pp. 2771-
2780, 2019.
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    -   Behrmann, J., Grathwohl, W., Chen, R. T., Duvenaud, D., 
and Jacobsen, J.-H. Invertible residual networks. In 
International Conference on Machine Learning, pp. 573-
582, 2019.
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    -   Chen, T. Q., Behrmann, J., Duvenaud, D. K., and Jacobsen, 
J.-H. Residual flows for invertible generative modeling. 
In Advances in Neural Information Processing Systems, 
pp. 9913-9923, 2019.
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    -   Ho, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel, P. 
Flow++: Improving flow-based generative models with 
variational dequantization and architecture design. In 
International Conference on Machine Learning, pp. 2722-
2730, 2019.
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    -   Chen, X., Kingma, D. P., Salimans, T., Duan, Y., Dhariwal, 
P., Schulman, J., Sutskever, I., and Abbeel, P. Varia-
Hoogeboom, E., Van Den Berg, R., and Welling, M. Emerg-
ing convolutions for generative normalizing flows. In 
International Conference on Machine Learning, pp. 2771-
2780, 2019.
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    -   Behrmann, J., Grathwohl, W., Chen, R. T., Duvenaud, D., 
and Jacobsen, J.-H. Invertible residual networks. In 
International Conference on Machine Learning, pp. 573-
582, 2019.
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    -   Chen, T. Q., Behrmann, J., Duvenaud, D. K., and Jacobsen, 
J.-H. Residual flows for invertible generative modeling. 
In Advances in Neural Information Processing Systems, 
pp. 9913-9923, 2019.
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    -   Ho, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel, P. 
Flow++: Improving flow-based generative models with 
variational dequantization and architecture design. In 
International Conference on Machine Learning, pp. 2722-
2730, 2019.
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    -   Chen, X., Kingma, D. P., Salimans, T., Duan, Y., Dhariwal, 
P., Schulman, J., Sutskever, I., and Abbeel, P. Varia-
Hoogeboom, E., Van Den Berg, R., and Welling, M. Emerg-
ing convolutions for generative normalizing flows. In 
International Conference on Machine Learning, pp. 2771-
2780, 2019.
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    -   Behrmann, J., Grathwohl, W., Chen, R. T., Duvenaud, D., 
and Jacobsen, J.-H. Invertible residual networks. In 
International Conference on Machine Learning, pp. 573-
582, 2019.
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    -   Chen, T. Q., Behrmann, J., Duvenaud, D. K., and Jacobsen, 
J.-H. Residual flows for invertible generative modeling. 
In Advances in Neural Information Processing Systems, 
pp. 9913-9923, 2019.
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    -   Ho, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel, P. 
Flow++: Improving flow-based generative models with 
variational dequantization and architecture design. In 
International Conference on Machine Learning, pp. 2722-
2730, 2019.
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    -   Chen, X., Kingma, D. P., Salimans, T., Duan, Y., Dhariwal, 
P., Schulman, J., Sutskever, I., and Abbeel, P. Varia-
Hoogeboom, E., Van Den Berg, R., and Welling, M. Emerg-
ing convolutions for generative normalizing flows. In 
International Conference on Machine Learning, pp. 2771-
2780, 2019.
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    -   Behrmann, J., Grathwohl, W., Chen, R. T., Duvenaud, D., 
and Jacobsen, J.-H. Invertible residual networks. In 
International Conference on Machine Learning, pp. 573-
582, 2019.
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    -   Chen, T. Q., Behrmann, J., Duvenaud, D. K., and Jacobsen, 
J.-H. Residual flows for invertible generative modeling. 
In Advances in Neural Information Processing Systems, 
pp. 9913-9923, 2019.
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    -   Ho, J., Chen, X., Srinivas, A., Duan, Y., and Abbeel, P. 
Flow++: Improving flow-based generative models with 
variational dequantization and architecture design. In 
International Conference on Machine Learning, pp. 2722-
2730, 2019.
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:11.59 [INFO ] ReferenceMarkerMatcher    -   Chen, X., Kingma, D. P., Salimans, T., Duan, Y., Dhariwal, 
P., Schulman, J., Sutskever, I., and Abbeel, P. Varia-
Hoogeboom, E., Van Den Berg, R., and Welling, M. Emerg-
ing convolutions for generative normalizing flows. In 
International Conference on Machine Learning, pp. 2771-
2780, 2019.
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    -   Dupoux, E. Cognitive science in the era of artificial intel-
ligence: A roadmap for reverse-engineering the infant 
language-learner. Cognition, 173:43-59, 2018.
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    -   Kudo, T. Subword regularization: Improving neural network 
translation models with multiple subword candidates. In 
Proc. Annual Meeting of the Association for Computa-
tional Linguistics (ACL), 2018.
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    -   Drexler, J. and Glass, J. Combining end-to-end and ad-
versarial training for low-resource speech recognition. 
In Proc. IEEE Spoken Language Technology Workshop 
(SLT), 2018.
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    -   Kudo, T. and Richardson, J. Sentencepiece: A simple and 
language independent subword tokenizer and detokenizer 
for neural text processing. In Proc. Empirical Methods in 
Natural Language Processing (EMNLP), 2018.
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    -   Hsu, W.-N. and Glass, J. Extracting domain invariant 
features by unsupervised learning for robust automatic 
speech recognition. In Proc. International Conference 
on Acoustics, Speech and Signal Processing (ICASSP), 
2018.
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    -   Semi-Supervised Speech Recognition via Local Prior Matching 
Hsu, W.-N., Tang, H., and Glass, J. Unsupervised adapta-
tion with interpretable disentangled representations for 
distant conversational speech recognition. In Proc. An-
nual Conference of International Speech Communication 
Association (INTERSPEECH), 2018.
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    -   Meng, Z., Li, J., Gong, Y., and Juang, B.-H. Adversarial 
teacher-student learning for unsupervised domain adap-
tation. In Proc. International Conference on Acoustics, 
Speech and Signal Processing (ICASSP), 2018.
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    -   Karita, S., Watanabe, S., Iwata, T., Ogawa, A., and Delcroix, 
M. Semi-supervised end-to-end speech recognition. In 
Proc. Annual Conference of International Speech Com-
munication Association (INTERSPEECH), 2018.
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    -   Zeghidour, N., Xu, Q., Liptchinsky, V., Usunier, N., Syn-
naeve, G., and Collobert, R. Fully convolutional speech 
recognition. arXiv preprint arXiv:1812.06864, 2018.
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    -   Hayashi, T., Watanabe, S., Zhang, Y., Toda, T., Hori, T., 
Astudillo, R., and Takeda, K. Back-translation-style data 
augmentation for end-to-end ASR. In Proc. IEEE Spoken 
Language Technology Workshop (SLT), 2018.
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    -   Dupoux, E. Cognitive science in the era of artificial intel-
ligence: A roadmap for reverse-engineering the infant 
language-learner. Cognition, 173:43-59, 2018.
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    -   Kudo, T. Subword regularization: Improving neural network 
translation models with multiple subword candidates. In 
Proc. Annual Meeting of the Association for Computa-
tional Linguistics (ACL), 2018.
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    -   Drexler, J. and Glass, J. Combining end-to-end and ad-
versarial training for low-resource speech recognition. 
In Proc. IEEE Spoken Language Technology Workshop 
(SLT), 2018.
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    -   Kudo, T. and Richardson, J. Sentencepiece: A simple and 
language independent subword tokenizer and detokenizer 
for neural text processing. In Proc. Empirical Methods in 
Natural Language Processing (EMNLP), 2018.
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    -   Hsu, W.-N. and Glass, J. Extracting domain invariant 
features by unsupervised learning for robust automatic 
speech recognition. In Proc. International Conference 
on Acoustics, Speech and Signal Processing (ICASSP), 
2018.
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    -   Semi-Supervised Speech Recognition via Local Prior Matching 
Hsu, W.-N., Tang, H., and Glass, J. Unsupervised adapta-
tion with interpretable disentangled representations for 
distant conversational speech recognition. In Proc. An-
nual Conference of International Speech Communication 
Association (INTERSPEECH), 2018.
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    -   Meng, Z., Li, J., Gong, Y., and Juang, B.-H. Adversarial 
teacher-student learning for unsupervised domain adap-
tation. In Proc. International Conference on Acoustics, 
Speech and Signal Processing (ICASSP), 2018.
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    -   Karita, S., Watanabe, S., Iwata, T., Ogawa, A., and Delcroix, 
M. Semi-supervised end-to-end speech recognition. In 
Proc. Annual Conference of International Speech Com-
munication Association (INTERSPEECH), 2018.
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    -   Zeghidour, N., Xu, Q., Liptchinsky, V., Usunier, N., Syn-
naeve, G., and Collobert, R. Fully convolutional speech 
recognition. arXiv preprint arXiv:1812.06864, 2018.
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.22 [INFO ] ReferenceMarkerMatcher    -   Hayashi, T., Watanabe, S., Zhang, Y., Toda, T., Hori, T., 
Astudillo, R., and Takeda, K. Back-translation-style data 
augmentation for end-to-end ASR. In Proc. IEEE Spoken 
Language Technology Workshop (SLT), 2018.
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    -   Dupoux, E. Cognitive science in the era of artificial intel-
ligence: A roadmap for reverse-engineering the infant 
language-learner. Cognition, 173:43-59, 2018.
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    -   Kudo, T. Subword regularization: Improving neural network 
translation models with multiple subword candidates. In 
Proc. Annual Meeting of the Association for Computa-
tional Linguistics (ACL), 2018.
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    -   Drexler, J. and Glass, J. Combining end-to-end and ad-
versarial training for low-resource speech recognition. 
In Proc. IEEE Spoken Language Technology Workshop 
(SLT), 2018.
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    -   Kudo, T. and Richardson, J. Sentencepiece: A simple and 
language independent subword tokenizer and detokenizer 
for neural text processing. In Proc. Empirical Methods in 
Natural Language Processing (EMNLP), 2018.
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    -   Hsu, W.-N. and Glass, J. Extracting domain invariant 
features by unsupervised learning for robust automatic 
speech recognition. In Proc. International Conference 
on Acoustics, Speech and Signal Processing (ICASSP), 
2018.
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    -   Semi-Supervised Speech Recognition via Local Prior Matching 
Hsu, W.-N., Tang, H., and Glass, J. Unsupervised adapta-
tion with interpretable disentangled representations for 
distant conversational speech recognition. In Proc. An-
nual Conference of International Speech Communication 
Association (INTERSPEECH), 2018.
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    -   Meng, Z., Li, J., Gong, Y., and Juang, B.-H. Adversarial 
teacher-student learning for unsupervised domain adap-
tation. In Proc. International Conference on Acoustics, 
Speech and Signal Processing (ICASSP), 2018.
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    -   Karita, S., Watanabe, S., Iwata, T., Ogawa, A., and Delcroix, 
M. Semi-supervised end-to-end speech recognition. In 
Proc. Annual Conference of International Speech Com-
munication Association (INTERSPEECH), 2018.
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    -   Zeghidour, N., Xu, Q., Liptchinsky, V., Usunier, N., Syn-
naeve, G., and Collobert, R. Fully convolutional speech 
recognition. arXiv preprint arXiv:1812.06864, 2018.
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    -   Hayashi, T., Watanabe, S., Zhang, Y., Toda, T., Hori, T., 
Astudillo, R., and Takeda, K. Back-translation-style data 
augmentation for end-to-end ASR. In Proc. IEEE Spoken 
Language Technology Workshop (SLT), 2018.
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    -   Dupoux, E. Cognitive science in the era of artificial intel-
ligence: A roadmap for reverse-engineering the infant 
language-learner. Cognition, 173:43-59, 2018.
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    -   Kudo, T. Subword regularization: Improving neural network 
translation models with multiple subword candidates. In 
Proc. Annual Meeting of the Association for Computa-
tional Linguistics (ACL), 2018.
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    -   Drexler, J. and Glass, J. Combining end-to-end and ad-
versarial training for low-resource speech recognition. 
In Proc. IEEE Spoken Language Technology Workshop 
(SLT), 2018.
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    -   Kudo, T. and Richardson, J. Sentencepiece: A simple and 
language independent subword tokenizer and detokenizer 
for neural text processing. In Proc. Empirical Methods in 
Natural Language Processing (EMNLP), 2018.
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    -   Hsu, W.-N. and Glass, J. Extracting domain invariant 
features by unsupervised learning for robust automatic 
speech recognition. In Proc. International Conference 
on Acoustics, Speech and Signal Processing (ICASSP), 
2018.
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    -   Semi-Supervised Speech Recognition via Local Prior Matching 
Hsu, W.-N., Tang, H., and Glass, J. Unsupervised adapta-
tion with interpretable disentangled representations for 
distant conversational speech recognition. In Proc. An-
nual Conference of International Speech Communication 
Association (INTERSPEECH), 2018.
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    -   Meng, Z., Li, J., Gong, Y., and Juang, B.-H. Adversarial 
teacher-student learning for unsupervised domain adap-
tation. In Proc. International Conference on Acoustics, 
Speech and Signal Processing (ICASSP), 2018.
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    -   Karita, S., Watanabe, S., Iwata, T., Ogawa, A., and Delcroix, 
M. Semi-supervised end-to-end speech recognition. In 
Proc. Annual Conference of International Speech Com-
munication Association (INTERSPEECH), 2018.
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    -   Zeghidour, N., Xu, Q., Liptchinsky, V., Usunier, N., Syn-
naeve, G., and Collobert, R. Fully convolutional speech 
recognition. arXiv preprint arXiv:1812.06864, 2018.
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:12.26 [INFO ] ReferenceMarkerMatcher    -   Hayashi, T., Watanabe, S., Zhang, Y., Toda, T., Hori, T., 
Astudillo, R., and Takeda, K. Back-translation-style data 
augmentation for end-to-end ASR. In Proc. IEEE Spoken 
Language Technology Workshop (SLT), 2018.
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    -   Liu, Y. and Lapata, M. Text summarization with pretrained 
encoders. In Proceedings of the 2019 Conference on Em-
pirical Methods in Natural Language Processing and the 
9th International Joint Conference on Natural Language 
Processing, pp. 3730-3740, Hong Kong, China, 2019.
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    -   Taylor, W. L. Cloze procedure: A new tool for measuring 
readability. Journalism Bulletin, 30(4):415-433, 1953. 
UNILMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training 
A. Hyperparameters for Pre-Training 
C. Hyperparameters for NLU Fine-Tuning 
As shown in Table 7, we present the hyperparameters used 
for pre-training UNILMv2 BASE . We use the same Word-
Piece (Wu et al., 2016) vocabulary and model size as 
BERT BASE (Devlin et al., 2018). We follow the optimiza-
tion hyperparameters of RoBERTa BASE (Liu et al., 2019) for 
comparisons.
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    -   Song, K., Tan, X., Qin, T., Lu, J., and Liu, T.-Y. Mass: 
Masked sequence to sequence pre-training for language 
generation. arXiv preprint arXiv:1905.02450, 2019.
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    -   Joshi, M., Chen, D., Liu, Y., Weld, D. S., Zettlemoyer, 
L., and Levy, O. SpanBERT: Improving pre-training 
by representing and predicting spans. arXiv preprint 
arXiv:1907.10529, 2019.
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    -   Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mo-
hamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 
BART: Denoising sequence-to-sequence pre-training for 
natural language generation, translation, and comprehen-
sion. arXiv preprint arXiv:1910.13461, 2019.
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    -   Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., 
Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 
RoBERTa: A robustly optimized BERT pretraining ap-
proach. arXiv preprint arXiv:1907.11692, 2019.
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    -   Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., 
Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring 
the limits of transfer learning with a unified text-to-text 
transformer. arXiv preprint arXiv:1910.10683, 2019.
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    -   Liu, Y. and Lapata, M. Text summarization with pretrained 
encoders. In Proceedings of the 2019 Conference on Em-
pirical Methods in Natural Language Processing and the 
9th International Joint Conference on Natural Language 
Processing, pp. 3730-3740, Hong Kong, China, 2019.
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    -   Taylor, W. L. Cloze procedure: A new tool for measuring 
readability. Journalism Bulletin, 30(4):415-433, 1953. 
UNILMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training 
A. Hyperparameters for Pre-Training 
C. Hyperparameters for NLU Fine-Tuning 
As shown in Table 7, we present the hyperparameters used 
for pre-training UNILMv2 BASE . We use the same Word-
Piece (Wu et al., 2016) vocabulary and model size as 
BERT BASE (Devlin et al., 2018). We follow the optimiza-
tion hyperparameters of RoBERTa BASE (Liu et al., 2019) for 
comparisons.
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    -   Song, K., Tan, X., Qin, T., Lu, J., and Liu, T.-Y. Mass: 
Masked sequence to sequence pre-training for language 
generation. arXiv preprint arXiv:1905.02450, 2019.
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    -   Joshi, M., Chen, D., Liu, Y., Weld, D. S., Zettlemoyer, 
L., and Levy, O. SpanBERT: Improving pre-training 
by representing and predicting spans. arXiv preprint 
arXiv:1907.10529, 2019.
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    -   Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mo-
hamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 
BART: Denoising sequence-to-sequence pre-training for 
natural language generation, translation, and comprehen-
sion. arXiv preprint arXiv:1910.13461, 2019.
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    -   Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., 
Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 
RoBERTa: A robustly optimized BERT pretraining ap-
proach. arXiv preprint arXiv:1907.11692, 2019.
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    -   Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., 
Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring 
the limits of transfer learning with a unified text-to-text 
transformer. arXiv preprint arXiv:1910.10683, 2019.
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    -   Liu, Y. and Lapata, M. Text summarization with pretrained 
encoders. In Proceedings of the 2019 Conference on Em-
pirical Methods in Natural Language Processing and the 
9th International Joint Conference on Natural Language 
Processing, pp. 3730-3740, Hong Kong, China, 2019.
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    -   Taylor, W. L. Cloze procedure: A new tool for measuring 
readability. Journalism Bulletin, 30(4):415-433, 1953. 
UNILMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training 
A. Hyperparameters for Pre-Training 
C. Hyperparameters for NLU Fine-Tuning 
As shown in Table 7, we present the hyperparameters used 
for pre-training UNILMv2 BASE . We use the same Word-
Piece (Wu et al., 2016) vocabulary and model size as 
BERT BASE (Devlin et al., 2018). We follow the optimiza-
tion hyperparameters of RoBERTa BASE (Liu et al., 2019) for 
comparisons.
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    -   Song, K., Tan, X., Qin, T., Lu, J., and Liu, T.-Y. Mass: 
Masked sequence to sequence pre-training for language 
generation. arXiv preprint arXiv:1905.02450, 2019.
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    -   Joshi, M., Chen, D., Liu, Y., Weld, D. S., Zettlemoyer, 
L., and Levy, O. SpanBERT: Improving pre-training 
by representing and predicting spans. arXiv preprint 
arXiv:1907.10529, 2019.
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    -   Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mo-
hamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 
BART: Denoising sequence-to-sequence pre-training for 
natural language generation, translation, and comprehen-
sion. arXiv preprint arXiv:1910.13461, 2019.
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    -   Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., 
Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 
RoBERTa: A robustly optimized BERT pretraining ap-
proach. arXiv preprint arXiv:1907.11692, 2019.
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.30 [INFO ] ReferenceMarkerMatcher    -   Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., 
Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring 
the limits of transfer learning with a unified text-to-text 
transformer. arXiv preprint arXiv:1910.10683, 2019.
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    -   Liu, Y. and Lapata, M. Text summarization with pretrained 
encoders. In Proceedings of the 2019 Conference on Em-
pirical Methods in Natural Language Processing and the 
9th International Joint Conference on Natural Language 
Processing, pp. 3730-3740, Hong Kong, China, 2019.
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    -   Taylor, W. L. Cloze procedure: A new tool for measuring 
readability. Journalism Bulletin, 30(4):415-433, 1953. 
UNILMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training 
A. Hyperparameters for Pre-Training 
C. Hyperparameters for NLU Fine-Tuning 
As shown in Table 7, we present the hyperparameters used 
for pre-training UNILMv2 BASE . We use the same Word-
Piece (Wu et al., 2016) vocabulary and model size as 
BERT BASE (Devlin et al., 2018). We follow the optimiza-
tion hyperparameters of RoBERTa BASE (Liu et al., 2019) for 
comparisons.
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    -   Song, K., Tan, X., Qin, T., Lu, J., and Liu, T.-Y. Mass: 
Masked sequence to sequence pre-training for language 
generation. arXiv preprint arXiv:1905.02450, 2019.
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    -   Joshi, M., Chen, D., Liu, Y., Weld, D. S., Zettlemoyer, 
L., and Levy, O. SpanBERT: Improving pre-training 
by representing and predicting spans. arXiv preprint 
arXiv:1907.10529, 2019.
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    -   Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mo-
hamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 
BART: Denoising sequence-to-sequence pre-training for 
natural language generation, translation, and comprehen-
sion. arXiv preprint arXiv:1910.13461, 2019.
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    -   Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., 
Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 
RoBERTa: A robustly optimized BERT pretraining ap-
proach. arXiv preprint arXiv:1907.11692, 2019.
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    -   Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., 
Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring 
the limits of transfer learning with a unified text-to-text 
transformer. arXiv preprint arXiv:1910.10683, 2019.
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    -   Liu, Y. and Lapata, M. Text summarization with pretrained 
encoders. In Proceedings of the 2019 Conference on Em-
pirical Methods in Natural Language Processing and the 
9th International Joint Conference on Natural Language 
Processing, pp. 3730-3740, Hong Kong, China, 2019.
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    -   Taylor, W. L. Cloze procedure: A new tool for measuring 
readability. Journalism Bulletin, 30(4):415-433, 1953. 
UNILMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training 
A. Hyperparameters for Pre-Training 
C. Hyperparameters for NLU Fine-Tuning 
As shown in Table 7, we present the hyperparameters used 
for pre-training UNILMv2 BASE . We use the same Word-
Piece (Wu et al., 2016) vocabulary and model size as 
BERT BASE (Devlin et al., 2018). We follow the optimiza-
tion hyperparameters of RoBERTa BASE (Liu et al., 2019) for 
comparisons.
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    -   Song, K., Tan, X., Qin, T., Lu, J., and Liu, T.-Y. Mass: 
Masked sequence to sequence pre-training for language 
generation. arXiv preprint arXiv:1905.02450, 2019.
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    -   Joshi, M., Chen, D., Liu, Y., Weld, D. S., Zettlemoyer, 
L., and Levy, O. SpanBERT: Improving pre-training 
by representing and predicting spans. arXiv preprint 
arXiv:1907.10529, 2019.
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    -   Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mo-
hamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 
BART: Denoising sequence-to-sequence pre-training for 
natural language generation, translation, and comprehen-
sion. arXiv preprint arXiv:1910.13461, 2019.
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    -   Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., 
Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 
RoBERTa: A robustly optimized BERT pretraining ap-
proach. arXiv preprint arXiv:1907.11692, 2019.
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    -   Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., 
Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring 
the limits of transfer learning with a unified text-to-text 
transformer. arXiv preprint arXiv:1910.10683, 2019.
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    -   Liu, Y. and Lapata, M. Text summarization with pretrained 
encoders. In Proceedings of the 2019 Conference on Em-
pirical Methods in Natural Language Processing and the 
9th International Joint Conference on Natural Language 
Processing, pp. 3730-3740, Hong Kong, China, 2019.
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    -   Taylor, W. L. Cloze procedure: A new tool for measuring 
readability. Journalism Bulletin, 30(4):415-433, 1953. 
UNILMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training 
A. Hyperparameters for Pre-Training 
C. Hyperparameters for NLU Fine-Tuning 
As shown in Table 7, we present the hyperparameters used 
for pre-training UNILMv2 BASE . We use the same Word-
Piece (Wu et al., 2016) vocabulary and model size as 
BERT BASE (Devlin et al., 2018). We follow the optimiza-
tion hyperparameters of RoBERTa BASE (Liu et al., 2019) for 
comparisons.
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    -   Song, K., Tan, X., Qin, T., Lu, J., and Liu, T.-Y. Mass: 
Masked sequence to sequence pre-training for language 
generation. arXiv preprint arXiv:1905.02450, 2019.
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    -   Joshi, M., Chen, D., Liu, Y., Weld, D. S., Zettlemoyer, 
L., and Levy, O. SpanBERT: Improving pre-training 
by representing and predicting spans. arXiv preprint 
arXiv:1907.10529, 2019.
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    -   Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mo-
hamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. 
BART: Denoising sequence-to-sequence pre-training for 
natural language generation, translation, and comprehen-
sion. arXiv preprint arXiv:1910.13461, 2019.
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    -   Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., 
Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. 
RoBERTa: A robustly optimized BERT pretraining ap-
proach. arXiv preprint arXiv:1907.11692, 2019.
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:14.33 [INFO ] ReferenceMarkerMatcher    -   Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., 
Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring 
the limits of transfer learning with a unified text-to-text 
transformer. arXiv preprint arXiv:1910.10683, 2019.
30 Dec 2020 09:14.42 [ERROR] FullTextParser            - DocumentPointer for block 73 points to 61 token, but block token size is 59
30 Dec 2020 09:14.45 [ERROR] FullTextParser            - DocumentPointer for block 73 points to 61 token, but block token size is 59
30 Dec 2020 09:14.46 [ERROR] FullTextParser            - DocumentPointer for block 73 points to 61 token, but block token size is 59
30 Dec 2020 09:18.44 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:18.44 [INFO ] ReferenceMarkerMatcher    -   Tang, H., Huang, Y., Jing, M., Fan, Y., Zeng, X.: Very deep residual network for 
image matting. In: Proceedings of the IEEE International Conference on Image 
Processing (September 2019)
30 Dec 2020 09:18.44 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:18.44 [INFO ] ReferenceMarkerMatcher    -   Tang, J., Aksoy, Y., Oztireli, C., Gross, M., Aydin, T.O.: Learning-based sampling 
for natural image matting. In: Proceedings of the IEEE Conference on Computer 
Vision and Pattern Recognition (June 2019)
30 Dec 2020 09:18.47 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:18.47 [INFO ] ReferenceMarkerMatcher    -   Tang, H., Huang, Y., Jing, M., Fan, Y., Zeng, X.: Very deep residual network for 
image matting. In: Proceedings of the IEEE International Conference on Image 
Processing (September 2019)
30 Dec 2020 09:18.47 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:18.47 [INFO ] ReferenceMarkerMatcher    -   Tang, J., Aksoy, Y., Oztireli, C., Gross, M., Aydin, T.O.: Learning-based sampling 
for natural image matting. In: Proceedings of the IEEE Conference on Computer 
Vision and Pattern Recognition (June 2019)
30 Dec 2020 09:21.15 [WARN ] CybozuLanguageDetector    - Cannot detect language because of: com.cybozu.labs.langdetect.LangDetectException: no features in text
30 Dec 2020 09:21.16 [WARN ] CybozuLanguageDetector    - Cannot detect language because of: com.cybozu.labs.langdetect.LangDetectException: no features in text
30 Dec 2020 09:21.18 [WARN ] CybozuLanguageDetector    - Cannot detect language because of: com.cybozu.labs.langdetect.LangDetectException: no features in text
30 Dec 2020 09:21.19 [WARN ] CybozuLanguageDetector    - Cannot detect language because of: com.cybozu.labs.langdetect.LangDetectException: no features in text
30 Dec 2020 09:26.34 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:26.34 [INFO ] ReferenceMarkerMatcher    -   Wang, X. and Gupta, A. Unsupervised learning of visual 
representations using videos. In Proceedings of the IEEE 
International Conference on Computer Vision, pp. 2794-
2802, 2015.
30 Dec 2020 09:26.34 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:26.34 [INFO ] ReferenceMarkerMatcher    -   Wang, Z., Schaul, T., Hessel, M., Van Hasselt, H., Lanc-
tot, M., and De Freitas, N. Dueling network architec-
tures for deep reinforcement learning. arXiv preprint 
arXiv:1511.06581, 2015.
30 Dec 2020 09:26.39 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:26.39 [INFO ] ReferenceMarkerMatcher    -   Wang, X. and Gupta, A. Unsupervised learning of visual 
representations using videos. In Proceedings of the IEEE 
International Conference on Computer Vision, pp. 2794-
2802, 2015.
30 Dec 2020 09:26.39 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:26.39 [INFO ] ReferenceMarkerMatcher    -   Wang, Z., Schaul, T., Hessel, M., Van Hasselt, H., Lanc-
tot, M., and De Freitas, N. Dueling network architec-
tures for deep reinforcement learning. arXiv preprint 
arXiv:1511.06581, 2015.
30 Dec 2020 09:30.36 [ERROR] FullTextParser            - DocumentPointer for block 609 points to 395 token, but block token size is 395
30 Dec 2020 09:30.40 [ERROR] FullTextParser            - DocumentPointer for block 609 points to 395 token, but block token size is 395
30 Dec 2020 09:30.42 [ERROR] FullTextParser            - DocumentPointer for block 609 points to 395 token, but block token size is 395
30 Dec 2020 09:40.26 [ERROR] FullTextParser            - DocumentPointer for block 108 points to 73 token, but block token size is 69
30 Dec 2020 09:40.30 [ERROR] FullTextParser            - DocumentPointer for block 108 points to 73 token, but block token size is 69
30 Dec 2020 09:40.31 [ERROR] FullTextParser            - DocumentPointer for block 108 points to 73 token, but block token size is 69
30 Dec 2020 09:42.50 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:42.50 [INFO ] ReferenceMarkerMatcher    -   Li, C. and Goldwasser, D. Encoding social information with 
graph convolutional networks forpolitical perspective de-
tection in news media. In ACL, 2019.
30 Dec 2020 09:42.50 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:42.50 [INFO ] ReferenceMarkerMatcher    -   Liu, Z., Chen, C., Li, L., Zhou, J., Li, X., Song, L., and 
Qi, Y. Geniepath: Graph neural networks with adaptive 
receptive paths. In AAAI, 2019.
30 Dec 2020 09:42.50 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:42.50 [INFO ] ReferenceMarkerMatcher    -   Shang, J., Xiao, C., Ma, T., Li, H., and Sun, J. Gamenet: 
Graph augmented memory networks for recommending 
medication combination. In AAAI, 2019.
30 Dec 2020 09:42.50 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:42.50 [INFO ] ReferenceMarkerMatcher    -   Ma, J., Wen, J., Zhong, M., Chen, W., and Li, X. MMM: 
multi-source multi-net micro-video recommendation with 
clustered hidden item representation learning. Data Sci-
ence and Engineering, 4(3):240-253, 2019.
30 Dec 2020 09:42.50 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:42.50 [INFO ] ReferenceMarkerMatcher    -   Li, J., Han, Z., Cheng, H., Su, J., Wang, P., Zhang, J., and 
Pan, L. Predicting path failure in time-evolving graphs. 
In KDD. ACM, 2019.
30 Dec 2020 09:42.50 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:42.50 [INFO ] ReferenceMarkerMatcher    -   Zhang, J. and Ghanem, B. 
Ista-net: Interpretable 
optimization-inspired deep network for image compres-
sive sensing. In CVPR, pp. 1828-1837, 2018.
30 Dec 2020 09:42.50 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:42.50 [INFO ] ReferenceMarkerMatcher    -   Huang, W., Zhang, T., Rong, Y., and Huang, J. Adaptive 
sampling towards fast graph representation learning. In 
NeurIPS, pp. 4563-4572, 2018.
30 Dec 2020 09:42.50 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:42.50 [INFO ] ReferenceMarkerMatcher    -   Zhang, J., Shi, X., Xie, J., Ma, H., King, I., and Yeung, D. 
Gaan: Gated attention networks for learning on large and 
spatiotemporal graphs. In UAI, 2018.
30 Dec 2020 09:42.53 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:42.53 [INFO ] ReferenceMarkerMatcher    -   Li, C. and Goldwasser, D. Encoding social information with 
graph convolutional networks forpolitical perspective de-
tection in news media. In ACL, 2019.
30 Dec 2020 09:42.53 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:42.53 [INFO ] ReferenceMarkerMatcher    -   Liu, Z., Chen, C., Li, L., Zhou, J., Li, X., Song, L., and 
Qi, Y. Geniepath: Graph neural networks with adaptive 
receptive paths. In AAAI, 2019.
30 Dec 2020 09:42.53 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:42.53 [INFO ] ReferenceMarkerMatcher    -   Shang, J., Xiao, C., Ma, T., Li, H., and Sun, J. Gamenet: 
Graph augmented memory networks for recommending 
medication combination. In AAAI, 2019.
30 Dec 2020 09:42.53 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:42.53 [INFO ] ReferenceMarkerMatcher    -   Ma, J., Wen, J., Zhong, M., Chen, W., and Li, X. MMM: 
multi-source multi-net micro-video recommendation with 
clustered hidden item representation learning. Data Sci-
ence and Engineering, 4(3):240-253, 2019.
30 Dec 2020 09:42.53 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:42.53 [INFO ] ReferenceMarkerMatcher    -   Li, J., Han, Z., Cheng, H., Su, J., Wang, P., Zhang, J., and 
Pan, L. Predicting path failure in time-evolving graphs. 
In KDD. ACM, 2019.
30 Dec 2020 09:42.53 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:42.53 [INFO ] ReferenceMarkerMatcher    -   Zhang, J. and Ghanem, B. 
Ista-net: Interpretable 
optimization-inspired deep network for image compres-
sive sensing. In CVPR, pp. 1828-1837, 2018.
30 Dec 2020 09:42.53 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:42.53 [INFO ] ReferenceMarkerMatcher    -   Huang, W., Zhang, T., Rong, Y., and Huang, J. Adaptive 
sampling towards fast graph representation learning. In 
NeurIPS, pp. 4563-4572, 2018.
30 Dec 2020 09:42.53 [INFO ] ReferenceMarkerMatcher    - +++++
30 Dec 2020 09:42.53 [INFO ] ReferenceMarkerMatcher    -   Zhang, J., Shi, X., Xie, J., Ma, H., King, I., and Yeung, D. 
Gaan: Gated attention networks for learning on large and 
spatiotemporal graphs. In UAI, 2018.
