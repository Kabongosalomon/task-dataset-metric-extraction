Directory ../data/ibm/exp/few-shot-setup/NLP-TDMS/paperVersion//models/BERT/ Exist
WARNING:tensorflow:From /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/BERT/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From run_classifier_sci.py:1079: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.

WARNING:tensorflow:From run_classifier_sci.py:865: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

W0622 09:42:32.877676 140630242117440 module_wrapper.py:139] From run_classifier_sci.py:865: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

WARNING:tensorflow:From run_classifier_sci.py:865: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

W0622 09:42:32.877968 140630242117440 module_wrapper.py:139] From run_classifier_sci.py:865: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

WARNING:tensorflow:From /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/BERT/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.

W0622 09:42:32.878518 140630242117440 module_wrapper.py:139] From /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/BERT/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.

WARNING:tensorflow:From run_classifier_sci.py:890: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.

W0622 09:42:32.881338 140630242117440 module_wrapper.py:139] From run_classifier_sci.py:890: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.

WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

W0622 09:42:32.987523 140630242117440 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fe67e5e4730>) includes params argument, but params are not passed to Estimator.
W0622 09:42:35.269286 140630242117440 estimator.py:1994] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7fe67e5e4730>) includes params argument, but params are not passed to Estimator.
INFO:tensorflow:Using config: {'_model_dir': '../data/ibm/exp/few-shot-setup/NLP-TDMS/paperVersion//models/BERT/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe67e5a4860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}
I0622 09:42:35.271247 140630242117440 estimator.py:212] Using config: {'_model_dir': '../data/ibm/exp/few-shot-setup/NLP-TDMS/paperVersion//models/BERT/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe67e5a4860>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}
INFO:tensorflow:_TPUContext: eval_on_tpu True
I0622 09:42:35.271999 140630242117440 tpu_context.py:220] _TPUContext: eval_on_tpu True
WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.
W0622 09:42:35.272485 140630242117440 tpu_context.py:222] eval_on_tpu ignored because use_tpu is False.
WARNING:tensorflow:From run_classifier_sci.py:561: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.

W0622 09:42:35.274172 140630242117440 module_wrapper.py:139] From run_classifier_sci.py:561: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.

WARNING:tensorflow:From run_classifier_sci.py:565: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.

W0622 09:42:35.275827 140630242117440 module_wrapper.py:139] From run_classifier_sci.py:565: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.

INFO:tensorflow:Writing example 0 of 13306
I0622 09:42:35.275949 140630242117440 run_classifier_sci.py:565] Writing example 0 of 13306
INFO:tensorflow:*** Example ***
I0622 09:42:35.282030 140630242117440 run_classifier_sci.py:538] *** Example ***
INFO:tensorflow:guid: train-0
I0622 09:42:35.282170 140630242117440 run_classifier_sci.py:539] guid: train-0
INFO:tensorflow:tokens: [CLS] un ##k ##now [SEP] " multi - view response selection for human - computer conversation in this paper , we study the task of response selection for multi - turn human - computer conversation . previous approaches take word as a unit and view context and response as sequences of words . this kind of approaches do not explicitly take each utter ##ance as a unit , therefore it is difficult to catch utter ##ance - level discourse information and depend ##encies . in this paper , we propose a multi - view response selection model that integrate ##s information from two different views , i . e . , word sequence view and utter ##ance sequence view . we jointly model the two views via deep ne ##u - ra ##l networks . experimental results on a public corpus for context - sensitive response selection demonstrate the effectiveness of the proposed multi - view model , which significantly out ##per - forms other single - view baseline ##s . our model is evaluated on the public u ##bu ##nt ##u corpus ( , designed for response selection study of multi - turn human - computer conversation ( ) the data ##set contains 0 . 93 million human - human dialogues crawled from an internet chatting room for u ##bu ##nt ##u troubles ##hoot ##ing the validation set and testing set are constructed in a similar way to the training set , with one notable difference that for each context and the corresponding positive response , 9 negative responses are randomly selected for further evaluation following the work of , the evaluation metric is 1 in m recall @ k ( denoted 1 in m r @ k ) , where a response selection model is designed to select k most likely responses among m candidates , and it gets the score " " 1 " " if the correct response is in the k selected ones this metric can be seen as an adaptation of the precision and recall metric ##s previously applied to dialogue data ##set ##s ( ) table 1 : performance comparison between our models and baseline models . in the table , word - se ##q - l ##st ##m is the experiment 10 % 20 % 1 in 2 r @ 1 1 ##in 10 r @ 5 50 % 1 in 10 r @ 1 1 in 10 r @ 2 " [SEP]
I0622 09:42:35.282394 140630242117440 run_classifier_sci.py:541] tokens: [CLS] un ##k ##now [SEP] " multi - view response selection for human - computer conversation in this paper , we study the task of response selection for multi - turn human - computer conversation . previous approaches take word as a unit and view context and response as sequences of words . this kind of approaches do not explicitly take each utter ##ance as a unit , therefore it is difficult to catch utter ##ance - level discourse information and depend ##encies . in this paper , we propose a multi - view response selection model that integrate ##s information from two different views , i . e . , word sequence view and utter ##ance sequence view . we jointly model the two views via deep ne ##u - ra ##l networks . experimental results on a public corpus for context - sensitive response selection demonstrate the effectiveness of the proposed multi - view model , which significantly out ##per - forms other single - view baseline ##s . our model is evaluated on the public u ##bu ##nt ##u corpus ( , designed for response selection study of multi - turn human - computer conversation ( ) the data ##set contains 0 . 93 million human - human dialogues crawled from an internet chatting room for u ##bu ##nt ##u troubles ##hoot ##ing the validation set and testing set are constructed in a similar way to the training set , with one notable difference that for each context and the corresponding positive response , 9 negative responses are randomly selected for further evaluation following the work of , the evaluation metric is 1 in m recall @ k ( denoted 1 in m r @ k ) , where a response selection model is designed to select k most likely responses among m candidates , and it gets the score " " 1 " " if the correct response is in the k selected ones this metric can be seen as an adaptation of the precision and recall metric ##s previously applied to dialogue data ##set ##s ( ) table 1 : performance comparison between our models and baseline models . in the table , word - se ##q - l ##st ##m is the experiment 10 % 20 % 1 in 2 r @ 1 1 ##in 10 r @ 5 50 % 1 in 10 r @ 1 1 in 10 r @ 2 " [SEP]
INFO:tensorflow:input_ids: 101 4895 2243 19779 102 1000 4800 1011 3193 3433 4989 2005 2529 1011 3274 4512 1999 2023 3259 1010 2057 2817 1996 4708 1997 3433 4989 2005 4800 1011 2735 2529 1011 3274 4512 1012 3025 8107 2202 2773 2004 1037 3131 1998 3193 6123 1998 3433 2004 10071 1997 2616 1012 2023 2785 1997 8107 2079 2025 12045 2202 2169 14395 6651 2004 1037 3131 1010 3568 2009 2003 3697 2000 4608 14395 6651 1011 2504 15152 2592 1998 12530 15266 1012 1999 2023 3259 1010 2057 16599 1037 4800 1011 3193 3433 4989 2944 2008 17409 2015 2592 2013 2048 2367 5328 1010 1045 1012 1041 1012 1010 2773 5537 3193 1998 14395 6651 5537 3193 1012 2057 10776 2944 1996 2048 5328 3081 2784 11265 2226 1011 10958 2140 6125 1012 6388 3463 2006 1037 2270 13931 2005 6123 1011 7591 3433 4989 10580 1996 12353 1997 1996 3818 4800 1011 3193 2944 1010 2029 6022 2041 4842 1011 3596 2060 2309 1011 3193 26163 2015 1012 2256 2944 2003 16330 2006 1996 2270 1057 8569 3372 2226 13931 1006 1010 2881 2005 3433 4989 2817 1997 4800 1011 2735 2529 1011 3274 4512 1006 1007 1996 2951 13462 3397 1014 1012 6109 2454 2529 1011 2529 22580 12425 2013 2019 4274 22331 2282 2005 1057 8569 3372 2226 13460 23416 2075 1996 27354 2275 1998 5604 2275 2024 3833 1999 1037 2714 2126 2000 1996 2731 2275 1010 2007 2028 3862 4489 2008 2005 2169 6123 1998 1996 7978 3893 3433 1010 1023 4997 10960 2024 18154 3479 2005 2582 9312 2206 1996 2147 1997 1010 1996 9312 12046 2003 1015 1999 1049 9131 1030 1047 1006 19537 1015 1999 1049 1054 1030 1047 1007 1010 2073 1037 3433 4989 2944 2003 2881 2000 7276 1047 2087 3497 10960 2426 1049 5347 1010 1998 2009 4152 1996 3556 1000 1000 1015 1000 1000 2065 1996 6149 3433 2003 1999 1996 1047 3479 3924 2023 12046 2064 2022 2464 2004 2019 6789 1997 1996 11718 1998 9131 12046 2015 3130 4162 2000 7982 2951 13462 2015 1006 1007 2795 1015 1024 2836 7831 2090 2256 4275 1998 26163 4275 1012 1999 1996 2795 1010 2773 1011 7367 4160 1011 1048 3367 2213 2003 1996 7551 2184 1003 2322 1003 1015 1999 1016 1054 1030 1015 1015 2378 2184 1054 1030 1019 2753 1003 1015 1999 2184 1054 1030 1015 1015 1999 2184 1054 1030 1016 1000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0622 09:42:35.282614 140630242117440 run_classifier_sci.py:542] input_ids: 101 4895 2243 19779 102 1000 4800 1011 3193 3433 4989 2005 2529 1011 3274 4512 1999 2023 3259 1010 2057 2817 1996 4708 1997 3433 4989 2005 4800 1011 2735 2529 1011 3274 4512 1012 3025 8107 2202 2773 2004 1037 3131 1998 3193 6123 1998 3433 2004 10071 1997 2616 1012 2023 2785 1997 8107 2079 2025 12045 2202 2169 14395 6651 2004 1037 3131 1010 3568 2009 2003 3697 2000 4608 14395 6651 1011 2504 15152 2592 1998 12530 15266 1012 1999 2023 3259 1010 2057 16599 1037 4800 1011 3193 3433 4989 2944 2008 17409 2015 2592 2013 2048 2367 5328 1010 1045 1012 1041 1012 1010 2773 5537 3193 1998 14395 6651 5537 3193 1012 2057 10776 2944 1996 2048 5328 3081 2784 11265 2226 1011 10958 2140 6125 1012 6388 3463 2006 1037 2270 13931 2005 6123 1011 7591 3433 4989 10580 1996 12353 1997 1996 3818 4800 1011 3193 2944 1010 2029 6022 2041 4842 1011 3596 2060 2309 1011 3193 26163 2015 1012 2256 2944 2003 16330 2006 1996 2270 1057 8569 3372 2226 13931 1006 1010 2881 2005 3433 4989 2817 1997 4800 1011 2735 2529 1011 3274 4512 1006 1007 1996 2951 13462 3397 1014 1012 6109 2454 2529 1011 2529 22580 12425 2013 2019 4274 22331 2282 2005 1057 8569 3372 2226 13460 23416 2075 1996 27354 2275 1998 5604 2275 2024 3833 1999 1037 2714 2126 2000 1996 2731 2275 1010 2007 2028 3862 4489 2008 2005 2169 6123 1998 1996 7978 3893 3433 1010 1023 4997 10960 2024 18154 3479 2005 2582 9312 2206 1996 2147 1997 1010 1996 9312 12046 2003 1015 1999 1049 9131 1030 1047 1006 19537 1015 1999 1049 1054 1030 1047 1007 1010 2073 1037 3433 4989 2944 2003 2881 2000 7276 1047 2087 3497 10960 2426 1049 5347 1010 1998 2009 4152 1996 3556 1000 1000 1015 1000 1000 2065 1996 6149 3433 2003 1999 1996 1047 3479 3924 2023 12046 2064 2022 2464 2004 2019 6789 1997 1996 11718 1998 9131 12046 2015 3130 4162 2000 7982 2951 13462 2015 1006 1007 2795 1015 1024 2836 7831 2090 2256 4275 1998 26163 4275 1012 1999 1996 2795 1010 2773 1011 7367 4160 1011 1048 3367 2213 2003 1996 7551 2184 1003 2322 1003 1015 1999 1016 1054 1030 1015 1015 2378 2184 1054 1030 1019 2753 1003 1015 1999 2184 1054 1030 1015 1015 1999 2184 1054 1030 1016 1000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0622 09:42:35.282842 140630242117440 run_classifier_sci.py:543] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:segment_ids: 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0622 09:42:35.283019 140630242117440 run_classifier_sci.py:544] segment_ids: 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:label: true (id = 0)
I0622 09:42:35.283088 140630242117440 run_classifier_sci.py:545] label: true (id = 0)
INFO:tensorflow:*** Example ***
I0622 09:42:35.289251 140630242117440 run_classifier_sci.py:538] *** Example ***
INFO:tensorflow:guid: train-1
I0622 09:42:35.289363 140630242117440 run_classifier_sci.py:539] guid: train-1
INFO:tensorflow:tokens: [CLS] question answering ; squad ; f1 [SEP] " multi - view response selection for human - computer conversation in this paper , we study the task of response selection for multi - turn human - computer conversation . previous approaches take word as a unit and view context and response as sequences of words . this kind of approaches do not explicitly take each utter ##ance as a unit , therefore it is difficult to catch utter ##ance - level discourse information and depend ##encies . in this paper , we propose a multi - view response selection model that integrate ##s information from two different views , i . e . , word sequence view and utter ##ance sequence view . we jointly model the two views via deep ne ##u - ra ##l networks . experimental results on a public corpus for context - sensitive response selection demonstrate the effectiveness of the proposed multi - view model , which significantly out ##per - forms other single - view baseline ##s . our model is evaluated on the public u ##bu ##nt ##u corpus ( , designed for response selection study of multi - turn human - computer conversation ( ) the data ##set contains 0 . 93 million human - human dialogues crawled from an internet chatting room for u ##bu ##nt ##u troubles ##hoot ##ing the validation set and testing set are constructed in a similar way to the training set , with one notable difference that for each context and the corresponding positive response , 9 negative responses are randomly selected for further evaluation following the work of , the evaluation metric is 1 in m recall @ k ( denoted 1 in m r @ k ) , where a response selection model is designed to select k most likely responses among m candidates , and it gets the score " " 1 " " if the correct response is in the k selected ones this metric can be seen as an adaptation of the precision and recall metric ##s previously applied to dialogue data ##set ##s ( ) table 1 : performance comparison between our models and baseline models . in the table , word - se ##q - l ##st ##m is the experiment 10 % 20 % 1 in 2 r @ 1 1 ##in 10 r @ 5 50 % 1 in 10 r @ 1 1 in 10 r @ 2 " [SEP]
I0622 09:42:35.289564 140630242117440 run_classifier_sci.py:541] tokens: [CLS] question answering ; squad ; f1 [SEP] " multi - view response selection for human - computer conversation in this paper , we study the task of response selection for multi - turn human - computer conversation . previous approaches take word as a unit and view context and response as sequences of words . this kind of approaches do not explicitly take each utter ##ance as a unit , therefore it is difficult to catch utter ##ance - level discourse information and depend ##encies . in this paper , we propose a multi - view response selection model that integrate ##s information from two different views , i . e . , word sequence view and utter ##ance sequence view . we jointly model the two views via deep ne ##u - ra ##l networks . experimental results on a public corpus for context - sensitive response selection demonstrate the effectiveness of the proposed multi - view model , which significantly out ##per - forms other single - view baseline ##s . our model is evaluated on the public u ##bu ##nt ##u corpus ( , designed for response selection study of multi - turn human - computer conversation ( ) the data ##set contains 0 . 93 million human - human dialogues crawled from an internet chatting room for u ##bu ##nt ##u troubles ##hoot ##ing the validation set and testing set are constructed in a similar way to the training set , with one notable difference that for each context and the corresponding positive response , 9 negative responses are randomly selected for further evaluation following the work of , the evaluation metric is 1 in m recall @ k ( denoted 1 in m r @ k ) , where a response selection model is designed to select k most likely responses among m candidates , and it gets the score " " 1 " " if the correct response is in the k selected ones this metric can be seen as an adaptation of the precision and recall metric ##s previously applied to dialogue data ##set ##s ( ) table 1 : performance comparison between our models and baseline models . in the table , word - se ##q - l ##st ##m is the experiment 10 % 20 % 1 in 2 r @ 1 1 ##in 10 r @ 5 50 % 1 in 10 r @ 1 1 in 10 r @ 2 " [SEP]
INFO:tensorflow:input_ids: 101 3160 10739 1025 4686 1025 20069 102 1000 4800 1011 3193 3433 4989 2005 2529 1011 3274 4512 1999 2023 3259 1010 2057 2817 1996 4708 1997 3433 4989 2005 4800 1011 2735 2529 1011 3274 4512 1012 3025 8107 2202 2773 2004 1037 3131 1998 3193 6123 1998 3433 2004 10071 1997 2616 1012 2023 2785 1997 8107 2079 2025 12045 2202 2169 14395 6651 2004 1037 3131 1010 3568 2009 2003 3697 2000 4608 14395 6651 1011 2504 15152 2592 1998 12530 15266 1012 1999 2023 3259 1010 2057 16599 1037 4800 1011 3193 3433 4989 2944 2008 17409 2015 2592 2013 2048 2367 5328 1010 1045 1012 1041 1012 1010 2773 5537 3193 1998 14395 6651 5537 3193 1012 2057 10776 2944 1996 2048 5328 3081 2784 11265 2226 1011 10958 2140 6125 1012 6388 3463 2006 1037 2270 13931 2005 6123 1011 7591 3433 4989 10580 1996 12353 1997 1996 3818 4800 1011 3193 2944 1010 2029 6022 2041 4842 1011 3596 2060 2309 1011 3193 26163 2015 1012 2256 2944 2003 16330 2006 1996 2270 1057 8569 3372 2226 13931 1006 1010 2881 2005 3433 4989 2817 1997 4800 1011 2735 2529 1011 3274 4512 1006 1007 1996 2951 13462 3397 1014 1012 6109 2454 2529 1011 2529 22580 12425 2013 2019 4274 22331 2282 2005 1057 8569 3372 2226 13460 23416 2075 1996 27354 2275 1998 5604 2275 2024 3833 1999 1037 2714 2126 2000 1996 2731 2275 1010 2007 2028 3862 4489 2008 2005 2169 6123 1998 1996 7978 3893 3433 1010 1023 4997 10960 2024 18154 3479 2005 2582 9312 2206 1996 2147 1997 1010 1996 9312 12046 2003 1015 1999 1049 9131 1030 1047 1006 19537 1015 1999 1049 1054 1030 1047 1007 1010 2073 1037 3433 4989 2944 2003 2881 2000 7276 1047 2087 3497 10960 2426 1049 5347 1010 1998 2009 4152 1996 3556 1000 1000 1015 1000 1000 2065 1996 6149 3433 2003 1999 1996 1047 3479 3924 2023 12046 2064 2022 2464 2004 2019 6789 1997 1996 11718 1998 9131 12046 2015 3130 4162 2000 7982 2951 13462 2015 1006 1007 2795 1015 1024 2836 7831 2090 2256 4275 1998 26163 4275 1012 1999 1996 2795 1010 2773 1011 7367 4160 1011 1048 3367 2213 2003 1996 7551 2184 1003 2322 1003 1015 1999 1016 1054 1030 1015 1015 2378 2184 1054 1030 1019 2753 1003 1015 1999 2184 1054 1030 1015 1015 1999 2184 1054 1030 1016 1000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0622 09:42:35.289780 140630242117440 run_classifier_sci.py:542] input_ids: 101 3160 10739 1025 4686 1025 20069 102 1000 4800 1011 3193 3433 4989 2005 2529 1011 3274 4512 1999 2023 3259 1010 2057 2817 1996 4708 1997 3433 4989 2005 4800 1011 2735 2529 1011 3274 4512 1012 3025 8107 2202 2773 2004 1037 3131 1998 3193 6123 1998 3433 2004 10071 1997 2616 1012 2023 2785 1997 8107 2079 2025 12045 2202 2169 14395 6651 2004 1037 3131 1010 3568 2009 2003 3697 2000 4608 14395 6651 1011 2504 15152 2592 1998 12530 15266 1012 1999 2023 3259 1010 2057 16599 1037 4800 1011 3193 3433 4989 2944 2008 17409 2015 2592 2013 2048 2367 5328 1010 1045 1012 1041 1012 1010 2773 5537 3193 1998 14395 6651 5537 3193 1012 2057 10776 2944 1996 2048 5328 3081 2784 11265 2226 1011 10958 2140 6125 1012 6388 3463 2006 1037 2270 13931 2005 6123 1011 7591 3433 4989 10580 1996 12353 1997 1996 3818 4800 1011 3193 2944 1010 2029 6022 2041 4842 1011 3596 2060 2309 1011 3193 26163 2015 1012 2256 2944 2003 16330 2006 1996 2270 1057 8569 3372 2226 13931 1006 1010 2881 2005 3433 4989 2817 1997 4800 1011 2735 2529 1011 3274 4512 1006 1007 1996 2951 13462 3397 1014 1012 6109 2454 2529 1011 2529 22580 12425 2013 2019 4274 22331 2282 2005 1057 8569 3372 2226 13460 23416 2075 1996 27354 2275 1998 5604 2275 2024 3833 1999 1037 2714 2126 2000 1996 2731 2275 1010 2007 2028 3862 4489 2008 2005 2169 6123 1998 1996 7978 3893 3433 1010 1023 4997 10960 2024 18154 3479 2005 2582 9312 2206 1996 2147 1997 1010 1996 9312 12046 2003 1015 1999 1049 9131 1030 1047 1006 19537 1015 1999 1049 1054 1030 1047 1007 1010 2073 1037 3433 4989 2944 2003 2881 2000 7276 1047 2087 3497 10960 2426 1049 5347 1010 1998 2009 4152 1996 3556 1000 1000 1015 1000 1000 2065 1996 6149 3433 2003 1999 1996 1047 3479 3924 2023 12046 2064 2022 2464 2004 2019 6789 1997 1996 11718 1998 9131 12046 2015 3130 4162 2000 7982 2951 13462 2015 1006 1007 2795 1015 1024 2836 7831 2090 2256 4275 1998 26163 4275 1012 1999 1996 2795 1010 2773 1011 7367 4160 1011 1048 3367 2213 2003 1996 7551 2184 1003 2322 1003 1015 1999 1016 1054 1030 1015 1015 2378 2184 1054 1030 1019 2753 1003 1015 1999 2184 1054 1030 1015 1015 1999 2184 1054 1030 1016 1000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0622 09:42:35.289993 140630242117440 run_classifier_sci.py:543] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0622 09:42:35.290164 140630242117440 run_classifier_sci.py:544] segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:label: false (id = 1)
I0622 09:42:35.290233 140630242117440 run_classifier_sci.py:545] label: false (id = 1)
INFO:tensorflow:*** Example ***
I0622 09:42:35.296359 140630242117440 run_classifier_sci.py:538] *** Example ***
INFO:tensorflow:guid: train-2
I0622 09:42:35.296465 140630242117440 run_classifier_sci.py:539] guid: train-2
INFO:tensorflow:tokens: [CLS] relation prediction ; f ##b ##15 ##k - 237 ; h @ 1 [SEP] " multi - view response selection for human - computer conversation in this paper , we study the task of response selection for multi - turn human - computer conversation . previous approaches take word as a unit and view context and response as sequences of words . this kind of approaches do not explicitly take each utter ##ance as a unit , therefore it is difficult to catch utter ##ance - level discourse information and depend ##encies . in this paper , we propose a multi - view response selection model that integrate ##s information from two different views , i . e . , word sequence view and utter ##ance sequence view . we jointly model the two views via deep ne ##u - ra ##l networks . experimental results on a public corpus for context - sensitive response selection demonstrate the effectiveness of the proposed multi - view model , which significantly out ##per - forms other single - view baseline ##s . our model is evaluated on the public u ##bu ##nt ##u corpus ( , designed for response selection study of multi - turn human - computer conversation ( ) the data ##set contains 0 . 93 million human - human dialogues crawled from an internet chatting room for u ##bu ##nt ##u troubles ##hoot ##ing the validation set and testing set are constructed in a similar way to the training set , with one notable difference that for each context and the corresponding positive response , 9 negative responses are randomly selected for further evaluation following the work of , the evaluation metric is 1 in m recall @ k ( denoted 1 in m r @ k ) , where a response selection model is designed to select k most likely responses among m candidates , and it gets the score " " 1 " " if the correct response is in the k selected ones this metric can be seen as an adaptation of the precision and recall metric ##s previously applied to dialogue data ##set ##s ( ) table 1 : performance comparison between our models and baseline models . in the table , word - se ##q - l ##st ##m is the experiment 10 % 20 % 1 in 2 r @ 1 1 ##in 10 r @ 5 50 % 1 in 10 r @ 1 1 in 10 r @ 2 " [SEP]
I0622 09:42:35.296649 140630242117440 run_classifier_sci.py:541] tokens: [CLS] relation prediction ; f ##b ##15 ##k - 237 ; h @ 1 [SEP] " multi - view response selection for human - computer conversation in this paper , we study the task of response selection for multi - turn human - computer conversation . previous approaches take word as a unit and view context and response as sequences of words . this kind of approaches do not explicitly take each utter ##ance as a unit , therefore it is difficult to catch utter ##ance - level discourse information and depend ##encies . in this paper , we propose a multi - view response selection model that integrate ##s information from two different views , i . e . , word sequence view and utter ##ance sequence view . we jointly model the two views via deep ne ##u - ra ##l networks . experimental results on a public corpus for context - sensitive response selection demonstrate the effectiveness of the proposed multi - view model , which significantly out ##per - forms other single - view baseline ##s . our model is evaluated on the public u ##bu ##nt ##u corpus ( , designed for response selection study of multi - turn human - computer conversation ( ) the data ##set contains 0 . 93 million human - human dialogues crawled from an internet chatting room for u ##bu ##nt ##u troubles ##hoot ##ing the validation set and testing set are constructed in a similar way to the training set , with one notable difference that for each context and the corresponding positive response , 9 negative responses are randomly selected for further evaluation following the work of , the evaluation metric is 1 in m recall @ k ( denoted 1 in m r @ k ) , where a response selection model is designed to select k most likely responses among m candidates , and it gets the score " " 1 " " if the correct response is in the k selected ones this metric can be seen as an adaptation of the precision and recall metric ##s previously applied to dialogue data ##set ##s ( ) table 1 : performance comparison between our models and baseline models . in the table , word - se ##q - l ##st ##m is the experiment 10 % 20 % 1 in 2 r @ 1 1 ##in 10 r @ 5 50 % 1 in 10 r @ 1 1 in 10 r @ 2 " [SEP]
INFO:tensorflow:input_ids: 101 7189 17547 1025 1042 2497 16068 2243 1011 23297 1025 1044 1030 1015 102 1000 4800 1011 3193 3433 4989 2005 2529 1011 3274 4512 1999 2023 3259 1010 2057 2817 1996 4708 1997 3433 4989 2005 4800 1011 2735 2529 1011 3274 4512 1012 3025 8107 2202 2773 2004 1037 3131 1998 3193 6123 1998 3433 2004 10071 1997 2616 1012 2023 2785 1997 8107 2079 2025 12045 2202 2169 14395 6651 2004 1037 3131 1010 3568 2009 2003 3697 2000 4608 14395 6651 1011 2504 15152 2592 1998 12530 15266 1012 1999 2023 3259 1010 2057 16599 1037 4800 1011 3193 3433 4989 2944 2008 17409 2015 2592 2013 2048 2367 5328 1010 1045 1012 1041 1012 1010 2773 5537 3193 1998 14395 6651 5537 3193 1012 2057 10776 2944 1996 2048 5328 3081 2784 11265 2226 1011 10958 2140 6125 1012 6388 3463 2006 1037 2270 13931 2005 6123 1011 7591 3433 4989 10580 1996 12353 1997 1996 3818 4800 1011 3193 2944 1010 2029 6022 2041 4842 1011 3596 2060 2309 1011 3193 26163 2015 1012 2256 2944 2003 16330 2006 1996 2270 1057 8569 3372 2226 13931 1006 1010 2881 2005 3433 4989 2817 1997 4800 1011 2735 2529 1011 3274 4512 1006 1007 1996 2951 13462 3397 1014 1012 6109 2454 2529 1011 2529 22580 12425 2013 2019 4274 22331 2282 2005 1057 8569 3372 2226 13460 23416 2075 1996 27354 2275 1998 5604 2275 2024 3833 1999 1037 2714 2126 2000 1996 2731 2275 1010 2007 2028 3862 4489 2008 2005 2169 6123 1998 1996 7978 3893 3433 1010 1023 4997 10960 2024 18154 3479 2005 2582 9312 2206 1996 2147 1997 1010 1996 9312 12046 2003 1015 1999 1049 9131 1030 1047 1006 19537 1015 1999 1049 1054 1030 1047 1007 1010 2073 1037 3433 4989 2944 2003 2881 2000 7276 1047 2087 3497 10960 2426 1049 5347 1010 1998 2009 4152 1996 3556 1000 1000 1015 1000 1000 2065 1996 6149 3433 2003 1999 1996 1047 3479 3924 2023 12046 2064 2022 2464 2004 2019 6789 1997 1996 11718 1998 9131 12046 2015 3130 4162 2000 7982 2951 13462 2015 1006 1007 2795 1015 1024 2836 7831 2090 2256 4275 1998 26163 4275 1012 1999 1996 2795 1010 2773 1011 7367 4160 1011 1048 3367 2213 2003 1996 7551 2184 1003 2322 1003 1015 1999 1016 1054 1030 1015 1015 2378 2184 1054 1030 1019 2753 1003 1015 1999 2184 1054 1030 1015 1015 1999 2184 1054 1030 1016 1000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0622 09:42:35.296848 140630242117440 run_classifier_sci.py:542] input_ids: 101 7189 17547 1025 1042 2497 16068 2243 1011 23297 1025 1044 1030 1015 102 1000 4800 1011 3193 3433 4989 2005 2529 1011 3274 4512 1999 2023 3259 1010 2057 2817 1996 4708 1997 3433 4989 2005 4800 1011 2735 2529 1011 3274 4512 1012 3025 8107 2202 2773 2004 1037 3131 1998 3193 6123 1998 3433 2004 10071 1997 2616 1012 2023 2785 1997 8107 2079 2025 12045 2202 2169 14395 6651 2004 1037 3131 1010 3568 2009 2003 3697 2000 4608 14395 6651 1011 2504 15152 2592 1998 12530 15266 1012 1999 2023 3259 1010 2057 16599 1037 4800 1011 3193 3433 4989 2944 2008 17409 2015 2592 2013 2048 2367 5328 1010 1045 1012 1041 1012 1010 2773 5537 3193 1998 14395 6651 5537 3193 1012 2057 10776 2944 1996 2048 5328 3081 2784 11265 2226 1011 10958 2140 6125 1012 6388 3463 2006 1037 2270 13931 2005 6123 1011 7591 3433 4989 10580 1996 12353 1997 1996 3818 4800 1011 3193 2944 1010 2029 6022 2041 4842 1011 3596 2060 2309 1011 3193 26163 2015 1012 2256 2944 2003 16330 2006 1996 2270 1057 8569 3372 2226 13931 1006 1010 2881 2005 3433 4989 2817 1997 4800 1011 2735 2529 1011 3274 4512 1006 1007 1996 2951 13462 3397 1014 1012 6109 2454 2529 1011 2529 22580 12425 2013 2019 4274 22331 2282 2005 1057 8569 3372 2226 13460 23416 2075 1996 27354 2275 1998 5604 2275 2024 3833 1999 1037 2714 2126 2000 1996 2731 2275 1010 2007 2028 3862 4489 2008 2005 2169 6123 1998 1996 7978 3893 3433 1010 1023 4997 10960 2024 18154 3479 2005 2582 9312 2206 1996 2147 1997 1010 1996 9312 12046 2003 1015 1999 1049 9131 1030 1047 1006 19537 1015 1999 1049 1054 1030 1047 1007 1010 2073 1037 3433 4989 2944 2003 2881 2000 7276 1047 2087 3497 10960 2426 1049 5347 1010 1998 2009 4152 1996 3556 1000 1000 1015 1000 1000 2065 1996 6149 3433 2003 1999 1996 1047 3479 3924 2023 12046 2064 2022 2464 2004 2019 6789 1997 1996 11718 1998 9131 12046 2015 3130 4162 2000 7982 2951 13462 2015 1006 1007 2795 1015 1024 2836 7831 2090 2256 4275 1998 26163 4275 1012 1999 1996 2795 1010 2773 1011 7367 4160 1011 1048 3367 2213 2003 1996 7551 2184 1003 2322 1003 1015 1999 1016 1054 1030 1015 1015 2378 2184 1054 1030 1019 2753 1003 1015 1999 2184 1054 1030 1015 1015 1999 2184 1054 1030 1016 1000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0622 09:42:35.297075 140630242117440 run_classifier_sci.py:543] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0622 09:42:35.297427 140630242117440 run_classifier_sci.py:544] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:label: false (id = 1)
I0622 09:42:35.297596 140630242117440 run_classifier_sci.py:545] label: false (id = 1)
INFO:tensorflow:*** Example ***
I0622 09:42:35.303004 140630242117440 run_classifier_sci.py:538] *** Example ***
INFO:tensorflow:guid: train-3
I0622 09:42:35.303154 140630242117440 run_classifier_sci.py:539] guid: train-3
INFO:tensorflow:tokens: [CLS] word sense di ##sam ##bi ##gua ##tion ; se ##me ##val 2013 ; f1 [SEP] " multi - view response selection for human - computer conversation in this paper , we study the task of response selection for multi - turn human - computer conversation . previous approaches take word as a unit and view context and response as sequences of words . this kind of approaches do not explicitly take each utter ##ance as a unit , therefore it is difficult to catch utter ##ance - level discourse information and depend ##encies . in this paper , we propose a multi - view response selection model that integrate ##s information from two different views , i . e . , word sequence view and utter ##ance sequence view . we jointly model the two views via deep ne ##u - ra ##l networks . experimental results on a public corpus for context - sensitive response selection demonstrate the effectiveness of the proposed multi - view model , which significantly out ##per - forms other single - view baseline ##s . our model is evaluated on the public u ##bu ##nt ##u corpus ( , designed for response selection study of multi - turn human - computer conversation ( ) the data ##set contains 0 . 93 million human - human dialogues crawled from an internet chatting room for u ##bu ##nt ##u troubles ##hoot ##ing the validation set and testing set are constructed in a similar way to the training set , with one notable difference that for each context and the corresponding positive response , 9 negative responses are randomly selected for further evaluation following the work of , the evaluation metric is 1 in m recall @ k ( denoted 1 in m r @ k ) , where a response selection model is designed to select k most likely responses among m candidates , and it gets the score " " 1 " " if the correct response is in the k selected ones this metric can be seen as an adaptation of the precision and recall metric ##s previously applied to dialogue data ##set ##s ( ) table 1 : performance comparison between our models and baseline models . in the table , word - se ##q - l ##st ##m is the experiment 10 % 20 % 1 in 2 r @ 1 1 ##in 10 r @ 5 50 % 1 in 10 r @ 1 1 in 10 r @ 2 " [SEP]
I0622 09:42:35.303375 140630242117440 run_classifier_sci.py:541] tokens: [CLS] word sense di ##sam ##bi ##gua ##tion ; se ##me ##val 2013 ; f1 [SEP] " multi - view response selection for human - computer conversation in this paper , we study the task of response selection for multi - turn human - computer conversation . previous approaches take word as a unit and view context and response as sequences of words . this kind of approaches do not explicitly take each utter ##ance as a unit , therefore it is difficult to catch utter ##ance - level discourse information and depend ##encies . in this paper , we propose a multi - view response selection model that integrate ##s information from two different views , i . e . , word sequence view and utter ##ance sequence view . we jointly model the two views via deep ne ##u - ra ##l networks . experimental results on a public corpus for context - sensitive response selection demonstrate the effectiveness of the proposed multi - view model , which significantly out ##per - forms other single - view baseline ##s . our model is evaluated on the public u ##bu ##nt ##u corpus ( , designed for response selection study of multi - turn human - computer conversation ( ) the data ##set contains 0 . 93 million human - human dialogues crawled from an internet chatting room for u ##bu ##nt ##u troubles ##hoot ##ing the validation set and testing set are constructed in a similar way to the training set , with one notable difference that for each context and the corresponding positive response , 9 negative responses are randomly selected for further evaluation following the work of , the evaluation metric is 1 in m recall @ k ( denoted 1 in m r @ k ) , where a response selection model is designed to select k most likely responses among m candidates , and it gets the score " " 1 " " if the correct response is in the k selected ones this metric can be seen as an adaptation of the precision and recall metric ##s previously applied to dialogue data ##set ##s ( ) table 1 : performance comparison between our models and baseline models . in the table , word - se ##q - l ##st ##m is the experiment 10 % 20 % 1 in 2 r @ 1 1 ##in 10 r @ 5 50 % 1 in 10 r @ 1 1 in 10 r @ 2 " [SEP]
INFO:tensorflow:input_ids: 101 2773 3168 4487 21559 5638 19696 3508 1025 7367 4168 10175 2286 1025 20069 102 1000 4800 1011 3193 3433 4989 2005 2529 1011 3274 4512 1999 2023 3259 1010 2057 2817 1996 4708 1997 3433 4989 2005 4800 1011 2735 2529 1011 3274 4512 1012 3025 8107 2202 2773 2004 1037 3131 1998 3193 6123 1998 3433 2004 10071 1997 2616 1012 2023 2785 1997 8107 2079 2025 12045 2202 2169 14395 6651 2004 1037 3131 1010 3568 2009 2003 3697 2000 4608 14395 6651 1011 2504 15152 2592 1998 12530 15266 1012 1999 2023 3259 1010 2057 16599 1037 4800 1011 3193 3433 4989 2944 2008 17409 2015 2592 2013 2048 2367 5328 1010 1045 1012 1041 1012 1010 2773 5537 3193 1998 14395 6651 5537 3193 1012 2057 10776 2944 1996 2048 5328 3081 2784 11265 2226 1011 10958 2140 6125 1012 6388 3463 2006 1037 2270 13931 2005 6123 1011 7591 3433 4989 10580 1996 12353 1997 1996 3818 4800 1011 3193 2944 1010 2029 6022 2041 4842 1011 3596 2060 2309 1011 3193 26163 2015 1012 2256 2944 2003 16330 2006 1996 2270 1057 8569 3372 2226 13931 1006 1010 2881 2005 3433 4989 2817 1997 4800 1011 2735 2529 1011 3274 4512 1006 1007 1996 2951 13462 3397 1014 1012 6109 2454 2529 1011 2529 22580 12425 2013 2019 4274 22331 2282 2005 1057 8569 3372 2226 13460 23416 2075 1996 27354 2275 1998 5604 2275 2024 3833 1999 1037 2714 2126 2000 1996 2731 2275 1010 2007 2028 3862 4489 2008 2005 2169 6123 1998 1996 7978 3893 3433 1010 1023 4997 10960 2024 18154 3479 2005 2582 9312 2206 1996 2147 1997 1010 1996 9312 12046 2003 1015 1999 1049 9131 1030 1047 1006 19537 1015 1999 1049 1054 1030 1047 1007 1010 2073 1037 3433 4989 2944 2003 2881 2000 7276 1047 2087 3497 10960 2426 1049 5347 1010 1998 2009 4152 1996 3556 1000 1000 1015 1000 1000 2065 1996 6149 3433 2003 1999 1996 1047 3479 3924 2023 12046 2064 2022 2464 2004 2019 6789 1997 1996 11718 1998 9131 12046 2015 3130 4162 2000 7982 2951 13462 2015 1006 1007 2795 1015 1024 2836 7831 2090 2256 4275 1998 26163 4275 1012 1999 1996 2795 1010 2773 1011 7367 4160 1011 1048 3367 2213 2003 1996 7551 2184 1003 2322 1003 1015 1999 1016 1054 1030 1015 1015 2378 2184 1054 1030 1019 2753 1003 1015 1999 2184 1054 1030 1015 1015 1999 2184 1054 1030 1016 1000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0622 09:42:35.303612 140630242117440 run_classifier_sci.py:542] input_ids: 101 2773 3168 4487 21559 5638 19696 3508 1025 7367 4168 10175 2286 1025 20069 102 1000 4800 1011 3193 3433 4989 2005 2529 1011 3274 4512 1999 2023 3259 1010 2057 2817 1996 4708 1997 3433 4989 2005 4800 1011 2735 2529 1011 3274 4512 1012 3025 8107 2202 2773 2004 1037 3131 1998 3193 6123 1998 3433 2004 10071 1997 2616 1012 2023 2785 1997 8107 2079 2025 12045 2202 2169 14395 6651 2004 1037 3131 1010 3568 2009 2003 3697 2000 4608 14395 6651 1011 2504 15152 2592 1998 12530 15266 1012 1999 2023 3259 1010 2057 16599 1037 4800 1011 3193 3433 4989 2944 2008 17409 2015 2592 2013 2048 2367 5328 1010 1045 1012 1041 1012 1010 2773 5537 3193 1998 14395 6651 5537 3193 1012 2057 10776 2944 1996 2048 5328 3081 2784 11265 2226 1011 10958 2140 6125 1012 6388 3463 2006 1037 2270 13931 2005 6123 1011 7591 3433 4989 10580 1996 12353 1997 1996 3818 4800 1011 3193 2944 1010 2029 6022 2041 4842 1011 3596 2060 2309 1011 3193 26163 2015 1012 2256 2944 2003 16330 2006 1996 2270 1057 8569 3372 2226 13931 1006 1010 2881 2005 3433 4989 2817 1997 4800 1011 2735 2529 1011 3274 4512 1006 1007 1996 2951 13462 3397 1014 1012 6109 2454 2529 1011 2529 22580 12425 2013 2019 4274 22331 2282 2005 1057 8569 3372 2226 13460 23416 2075 1996 27354 2275 1998 5604 2275 2024 3833 1999 1037 2714 2126 2000 1996 2731 2275 1010 2007 2028 3862 4489 2008 2005 2169 6123 1998 1996 7978 3893 3433 1010 1023 4997 10960 2024 18154 3479 2005 2582 9312 2206 1996 2147 1997 1010 1996 9312 12046 2003 1015 1999 1049 9131 1030 1047 1006 19537 1015 1999 1049 1054 1030 1047 1007 1010 2073 1037 3433 4989 2944 2003 2881 2000 7276 1047 2087 3497 10960 2426 1049 5347 1010 1998 2009 4152 1996 3556 1000 1000 1015 1000 1000 2065 1996 6149 3433 2003 1999 1996 1047 3479 3924 2023 12046 2064 2022 2464 2004 2019 6789 1997 1996 11718 1998 9131 12046 2015 3130 4162 2000 7982 2951 13462 2015 1006 1007 2795 1015 1024 2836 7831 2090 2256 4275 1998 26163 4275 1012 1999 1996 2795 1010 2773 1011 7367 4160 1011 1048 3367 2213 2003 1996 7551 2184 1003 2322 1003 1015 1999 1016 1054 1030 1015 1015 2378 2184 1054 1030 1019 2753 1003 1015 1999 2184 1054 1030 1015 1015 1999 2184 1054 1030 1016 1000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0622 09:42:35.303868 140630242117440 run_classifier_sci.py:543] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0622 09:42:35.304077 140630242117440 run_classifier_sci.py:544] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:label: false (id = 1)
I0622 09:42:35.304151 140630242117440 run_classifier_sci.py:545] label: false (id = 1)
INFO:tensorflow:*** Example ***
I0622 09:42:35.309716 140630242117440 run_classifier_sci.py:538] *** Example ***
INFO:tensorflow:guid: train-4
I0622 09:42:35.309870 140630242117440 run_classifier_sci.py:539] guid: train-4
INFO:tensorflow:tokens: [CLS] language modeling ; 1b words / google billion word bench ##mark ; test per ##plex ##ity [SEP] " multi - view response selection for human - computer conversation in this paper , we study the task of response selection for multi - turn human - computer conversation . previous approaches take word as a unit and view context and response as sequences of words . this kind of approaches do not explicitly take each utter ##ance as a unit , therefore it is difficult to catch utter ##ance - level discourse information and depend ##encies . in this paper , we propose a multi - view response selection model that integrate ##s information from two different views , i . e . , word sequence view and utter ##ance sequence view . we jointly model the two views via deep ne ##u - ra ##l networks . experimental results on a public corpus for context - sensitive response selection demonstrate the effectiveness of the proposed multi - view model , which significantly out ##per - forms other single - view baseline ##s . our model is evaluated on the public u ##bu ##nt ##u corpus ( , designed for response selection study of multi - turn human - computer conversation ( ) the data ##set contains 0 . 93 million human - human dialogues crawled from an internet chatting room for u ##bu ##nt ##u troubles ##hoot ##ing the validation set and testing set are constructed in a similar way to the training set , with one notable difference that for each context and the corresponding positive response , 9 negative responses are randomly selected for further evaluation following the work of , the evaluation metric is 1 in m recall @ k ( denoted 1 in m r @ k ) , where a response selection model is designed to select k most likely responses among m candidates , and it gets the score " " 1 " " if the correct response is in the k selected ones this metric can be seen as an adaptation of the precision and recall metric ##s previously applied to dialogue data ##set ##s ( ) table 1 : performance comparison between our models and baseline models . in the table , word - se ##q - l ##st ##m is the experiment 10 % 20 % 1 in 2 r @ 1 1 ##in 10 r @ 5 50 % 1 in 10 r @ 1 1 in 10 r @ 2 " [SEP]
I0622 09:42:35.310091 140630242117440 run_classifier_sci.py:541] tokens: [CLS] language modeling ; 1b words / google billion word bench ##mark ; test per ##plex ##ity [SEP] " multi - view response selection for human - computer conversation in this paper , we study the task of response selection for multi - turn human - computer conversation . previous approaches take word as a unit and view context and response as sequences of words . this kind of approaches do not explicitly take each utter ##ance as a unit , therefore it is difficult to catch utter ##ance - level discourse information and depend ##encies . in this paper , we propose a multi - view response selection model that integrate ##s information from two different views , i . e . , word sequence view and utter ##ance sequence view . we jointly model the two views via deep ne ##u - ra ##l networks . experimental results on a public corpus for context - sensitive response selection demonstrate the effectiveness of the proposed multi - view model , which significantly out ##per - forms other single - view baseline ##s . our model is evaluated on the public u ##bu ##nt ##u corpus ( , designed for response selection study of multi - turn human - computer conversation ( ) the data ##set contains 0 . 93 million human - human dialogues crawled from an internet chatting room for u ##bu ##nt ##u troubles ##hoot ##ing the validation set and testing set are constructed in a similar way to the training set , with one notable difference that for each context and the corresponding positive response , 9 negative responses are randomly selected for further evaluation following the work of , the evaluation metric is 1 in m recall @ k ( denoted 1 in m r @ k ) , where a response selection model is designed to select k most likely responses among m candidates , and it gets the score " " 1 " " if the correct response is in the k selected ones this metric can be seen as an adaptation of the precision and recall metric ##s previously applied to dialogue data ##set ##s ( ) table 1 : performance comparison between our models and baseline models . in the table , word - se ##q - l ##st ##m is the experiment 10 % 20 % 1 in 2 r @ 1 1 ##in 10 r @ 5 50 % 1 in 10 r @ 1 1 in 10 r @ 2 " [SEP]
INFO:tensorflow:input_ids: 101 2653 11643 1025 26314 2616 1013 8224 4551 2773 6847 10665 1025 3231 2566 19386 3012 102 1000 4800 1011 3193 3433 4989 2005 2529 1011 3274 4512 1999 2023 3259 1010 2057 2817 1996 4708 1997 3433 4989 2005 4800 1011 2735 2529 1011 3274 4512 1012 3025 8107 2202 2773 2004 1037 3131 1998 3193 6123 1998 3433 2004 10071 1997 2616 1012 2023 2785 1997 8107 2079 2025 12045 2202 2169 14395 6651 2004 1037 3131 1010 3568 2009 2003 3697 2000 4608 14395 6651 1011 2504 15152 2592 1998 12530 15266 1012 1999 2023 3259 1010 2057 16599 1037 4800 1011 3193 3433 4989 2944 2008 17409 2015 2592 2013 2048 2367 5328 1010 1045 1012 1041 1012 1010 2773 5537 3193 1998 14395 6651 5537 3193 1012 2057 10776 2944 1996 2048 5328 3081 2784 11265 2226 1011 10958 2140 6125 1012 6388 3463 2006 1037 2270 13931 2005 6123 1011 7591 3433 4989 10580 1996 12353 1997 1996 3818 4800 1011 3193 2944 1010 2029 6022 2041 4842 1011 3596 2060 2309 1011 3193 26163 2015 1012 2256 2944 2003 16330 2006 1996 2270 1057 8569 3372 2226 13931 1006 1010 2881 2005 3433 4989 2817 1997 4800 1011 2735 2529 1011 3274 4512 1006 1007 1996 2951 13462 3397 1014 1012 6109 2454 2529 1011 2529 22580 12425 2013 2019 4274 22331 2282 2005 1057 8569 3372 2226 13460 23416 2075 1996 27354 2275 1998 5604 2275 2024 3833 1999 1037 2714 2126 2000 1996 2731 2275 1010 2007 2028 3862 4489 2008 2005 2169 6123 1998 1996 7978 3893 3433 1010 1023 4997 10960 2024 18154 3479 2005 2582 9312 2206 1996 2147 1997 1010 1996 9312 12046 2003 1015 1999 1049 9131 1030 1047 1006 19537 1015 1999 1049 1054 1030 1047 1007 1010 2073 1037 3433 4989 2944 2003 2881 2000 7276 1047 2087 3497 10960 2426 1049 5347 1010 1998 2009 4152 1996 3556 1000 1000 1015 1000 1000 2065 1996 6149 3433 2003 1999 1996 1047 3479 3924 2023 12046 2064 2022 2464 2004 2019 6789 1997 1996 11718 1998 9131 12046 2015 3130 4162 2000 7982 2951 13462 2015 1006 1007 2795 1015 1024 2836 7831 2090 2256 4275 1998 26163 4275 1012 1999 1996 2795 1010 2773 1011 7367 4160 1011 1048 3367 2213 2003 1996 7551 2184 1003 2322 1003 1015 1999 1016 1054 1030 1015 1015 2378 2184 1054 1030 1019 2753 1003 1015 1999 2184 1054 1030 1015 1015 1999 2184 1054 1030 1016 1000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0622 09:42:35.310330 140630242117440 run_classifier_sci.py:542] input_ids: 101 2653 11643 1025 26314 2616 1013 8224 4551 2773 6847 10665 1025 3231 2566 19386 3012 102 1000 4800 1011 3193 3433 4989 2005 2529 1011 3274 4512 1999 2023 3259 1010 2057 2817 1996 4708 1997 3433 4989 2005 4800 1011 2735 2529 1011 3274 4512 1012 3025 8107 2202 2773 2004 1037 3131 1998 3193 6123 1998 3433 2004 10071 1997 2616 1012 2023 2785 1997 8107 2079 2025 12045 2202 2169 14395 6651 2004 1037 3131 1010 3568 2009 2003 3697 2000 4608 14395 6651 1011 2504 15152 2592 1998 12530 15266 1012 1999 2023 3259 1010 2057 16599 1037 4800 1011 3193 3433 4989 2944 2008 17409 2015 2592 2013 2048 2367 5328 1010 1045 1012 1041 1012 1010 2773 5537 3193 1998 14395 6651 5537 3193 1012 2057 10776 2944 1996 2048 5328 3081 2784 11265 2226 1011 10958 2140 6125 1012 6388 3463 2006 1037 2270 13931 2005 6123 1011 7591 3433 4989 10580 1996 12353 1997 1996 3818 4800 1011 3193 2944 1010 2029 6022 2041 4842 1011 3596 2060 2309 1011 3193 26163 2015 1012 2256 2944 2003 16330 2006 1996 2270 1057 8569 3372 2226 13931 1006 1010 2881 2005 3433 4989 2817 1997 4800 1011 2735 2529 1011 3274 4512 1006 1007 1996 2951 13462 3397 1014 1012 6109 2454 2529 1011 2529 22580 12425 2013 2019 4274 22331 2282 2005 1057 8569 3372 2226 13460 23416 2075 1996 27354 2275 1998 5604 2275 2024 3833 1999 1037 2714 2126 2000 1996 2731 2275 1010 2007 2028 3862 4489 2008 2005 2169 6123 1998 1996 7978 3893 3433 1010 1023 4997 10960 2024 18154 3479 2005 2582 9312 2206 1996 2147 1997 1010 1996 9312 12046 2003 1015 1999 1049 9131 1030 1047 1006 19537 1015 1999 1049 1054 1030 1047 1007 1010 2073 1037 3433 4989 2944 2003 2881 2000 7276 1047 2087 3497 10960 2426 1049 5347 1010 1998 2009 4152 1996 3556 1000 1000 1015 1000 1000 2065 1996 6149 3433 2003 1999 1996 1047 3479 3924 2023 12046 2064 2022 2464 2004 2019 6789 1997 1996 11718 1998 9131 12046 2015 3130 4162 2000 7982 2951 13462 2015 1006 1007 2795 1015 1024 2836 7831 2090 2256 4275 1998 26163 4275 1012 1999 1996 2795 1010 2773 1011 7367 4160 1011 1048 3367 2213 2003 1996 7551 2184 1003 2322 1003 1015 1999 1016 1054 1030 1015 1015 2378 2184 1054 1030 1019 2753 1003 1015 1999 2184 1054 1030 1015 1015 1999 2184 1054 1030 1016 1000 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0622 09:42:35.310562 140630242117440 run_classifier_sci.py:543] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
I0622 09:42:35.310748 140630242117440 run_classifier_sci.py:544] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:label: false (id = 1)
I0622 09:42:35.310815 140630242117440 run_classifier_sci.py:545] label: false (id = 1)
INFO:tensorflow:Writing example 10000 of 13306
I0622 09:43:29.130040 140630242117440 run_classifier_sci.py:565] Writing example 10000 of 13306
INFO:tensorflow:***** Running training *****
I0622 09:43:47.463642 140630242117440 run_classifier_sci.py:960] ***** Running training *****
INFO:tensorflow:  Num examples = 13306
I0622 09:43:47.463956 140630242117440 run_classifier_sci.py:961]   Num examples = 13306
INFO:tensorflow:  Batch size = 6
I0622 09:43:47.464575 140630242117440 run_classifier_sci.py:962]   Batch size = 6
INFO:tensorflow:  Num steps = 6653
I0622 09:43:47.464688 140630242117440 run_classifier_sci.py:963]   Num steps = 6653
WARNING:tensorflow:From run_classifier_sci.py:592: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.

W0622 09:43:47.464882 140630242117440 module_wrapper.py:139] From run_classifier_sci.py:592: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.

INFO:tensorflow:Skipping training since max_steps has already saved.
I0622 09:43:47.501152 140630242117440 estimator.py:363] Skipping training since max_steps has already saved.
INFO:tensorflow:training_loop marked as finished
I0622 09:43:47.501339 140630242117440 error_handling.py:101] training_loop marked as finished
INFO:tensorflow:Writing example 0 of 13071
I0622 09:43:47.960882 140630242117440 run_classifier_sci.py:565] Writing example 0 of 13071
INFO:tensorflow:*** Example ***
I0622 09:43:47.967326 140630242117440 run_classifier_sci.py:538] *** Example ***
INFO:tensorflow:guid: test-0
I0622 09:43:47.967482 140630242117440 run_classifier_sci.py:539] guid: test-0
INFO:tensorflow:tokens: [CLS] sentiment analysis ; sub ##j ; accuracy [SEP] universal sentence en ##code ##r we present models for encoding sentences into em ##bed ##ding vectors that specifically target transfer learning to other nl ##p tasks . the models are efficient and result in accurate performance on diverse transfer tasks . two variants of the encoding models allow for trade - offs between accuracy and compute resources . for both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance . comparisons are made with base - lines that use word level transfer learning via pre ##train ##ed word em ##bed ##ding ##s as well as baseline ##s do not use any transfer learning . we find that transfer learning using sentence em ##bed ##ding ##s tends to out ##per ##form word level transfer . with transfer learning via sentence em ##bed ##ding ##s , we observe surprisingly good performance with minimal amounts of supervised training data for ##a transfer task . we obtain encouraging results on word em ##bed ##ding association tests ( we ##at ) targeted at detecting model bias . our pre - trained sentence encoding models are made freely available for download and on t ##f hub . otherwise , hyper ##para ##meter ##s are tuned by cross ##val ##ida ##tion on the task training data when available or the evaluation test data when neither training nor dev data are provided training repeats ten times for each transfer task model with different randomly initial ##ized weights and we report evaluation results by averaging across runs to assess bias in our encoding models , we evaluate the strength of various associations learned by our model on we ##at word lists table 1 : transfer task evaluation sets 1 , 82 ##1 dev test 1 , 37 ##9 ) . table 2 : model performance on transfer tasks . use t is the universal sentence en ##code ##r ( use ) using transform ##er . use d is the universal en ##code ##r dan model . models tagged with baseline ##s with no transfer learning sentence & word em ##bed ##ding transfer learning - sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning mr ss ##t sts bench mp ##qa tre ##c sub ##j cr table 3 : task performance on ss ##t for varying amounts of training data . ss ##t 67 . 3 ##k represents the full training set . using only 1 , 000 examples for training , transfer learning from use t is able to obtain performance that rivals many of the other models trained on the full 67 . 3 thousand example training set . baseline ##s with no transfer learning sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning sentence & word em ##bed ##ding transfer learning table 4 : word em ##bed ##ding association tests ( we ##at ) for glove [SEP]
I0622 09:43:47.967732 140630242117440 run_classifier_sci.py:541] tokens: [CLS] sentiment analysis ; sub ##j ; accuracy [SEP] universal sentence en ##code ##r we present models for encoding sentences into em ##bed ##ding vectors that specifically target transfer learning to other nl ##p tasks . the models are efficient and result in accurate performance on diverse transfer tasks . two variants of the encoding models allow for trade - offs between accuracy and compute resources . for both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance . comparisons are made with base - lines that use word level transfer learning via pre ##train ##ed word em ##bed ##ding ##s as well as baseline ##s do not use any transfer learning . we find that transfer learning using sentence em ##bed ##ding ##s tends to out ##per ##form word level transfer . with transfer learning via sentence em ##bed ##ding ##s , we observe surprisingly good performance with minimal amounts of supervised training data for ##a transfer task . we obtain encouraging results on word em ##bed ##ding association tests ( we ##at ) targeted at detecting model bias . our pre - trained sentence encoding models are made freely available for download and on t ##f hub . otherwise , hyper ##para ##meter ##s are tuned by cross ##val ##ida ##tion on the task training data when available or the evaluation test data when neither training nor dev data are provided training repeats ten times for each transfer task model with different randomly initial ##ized weights and we report evaluation results by averaging across runs to assess bias in our encoding models , we evaluate the strength of various associations learned by our model on we ##at word lists table 1 : transfer task evaluation sets 1 , 82 ##1 dev test 1 , 37 ##9 ) . table 2 : model performance on transfer tasks . use t is the universal sentence en ##code ##r ( use ) using transform ##er . use d is the universal en ##code ##r dan model . models tagged with baseline ##s with no transfer learning sentence & word em ##bed ##ding transfer learning - sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning mr ss ##t sts bench mp ##qa tre ##c sub ##j cr table 3 : task performance on ss ##t for varying amounts of training data . ss ##t 67 . 3 ##k represents the full training set . using only 1 , 000 examples for training , transfer learning from use t is able to obtain performance that rivals many of the other models trained on the full 67 . 3 thousand example training set . baseline ##s with no transfer learning sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning sentence & word em ##bed ##ding transfer learning table 4 : word em ##bed ##ding association tests ( we ##at ) for glove [SEP]
INFO:tensorflow:input_ids: 101 15792 4106 1025 4942 3501 1025 10640 102 5415 6251 4372 16044 2099 2057 2556 4275 2005 17181 11746 2046 7861 8270 4667 19019 2008 4919 4539 4651 4083 2000 2060 17953 2361 8518 1012 1996 4275 2024 8114 1998 2765 1999 8321 2836 2006 7578 4651 8518 1012 2048 10176 1997 1996 17181 4275 3499 2005 3119 1011 12446 2090 10640 1998 24134 4219 1012 2005 2119 10176 1010 2057 8556 1998 3189 1996 3276 2090 2944 11619 1010 7692 8381 1010 1996 11343 1997 4651 4708 2731 2951 1010 1998 4708 2836 1012 18539 2024 2081 2007 2918 1011 3210 2008 2224 2773 2504 4651 4083 3081 3653 23654 2098 2773 7861 8270 4667 2015 2004 2092 2004 26163 2015 2079 2025 2224 2151 4651 4083 1012 2057 2424 2008 4651 4083 2478 6251 7861 8270 4667 2015 12102 2000 2041 4842 14192 2773 2504 4651 1012 2007 4651 4083 3081 6251 7861 8270 4667 2015 1010 2057 11949 10889 2204 2836 2007 10124 8310 1997 13588 2731 2951 2005 2050 4651 4708 1012 2057 6855 11434 3463 2006 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 9416 2012 25952 2944 13827 1012 2256 3653 1011 4738 6251 17181 4275 2024 2081 10350 2800 2005 8816 1998 2006 1056 2546 9594 1012 4728 1010 23760 28689 22828 2015 2024 15757 2011 2892 10175 8524 3508 2006 1996 4708 2731 2951 2043 2800 2030 1996 9312 3231 2951 2043 4445 2731 4496 16475 2951 2024 3024 2731 17993 2702 2335 2005 2169 4651 4708 2944 2007 2367 18154 3988 3550 15871 1998 2057 3189 9312 3463 2011 14985 2408 3216 2000 14358 13827 1999 2256 17181 4275 1010 2057 16157 1996 3997 1997 2536 8924 4342 2011 2256 2944 2006 2057 4017 2773 7201 2795 1015 1024 4651 4708 9312 4520 1015 1010 6445 2487 16475 3231 1015 1010 4261 2683 1007 1012 2795 1016 1024 2944 2836 2006 4651 8518 1012 2224 1056 2003 1996 5415 6251 4372 16044 2099 1006 2224 1007 2478 10938 2121 1012 2224 1040 2003 1996 5415 4372 16044 2099 4907 2944 1012 4275 26610 2007 26163 2015 2007 2053 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 1011 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 2720 7020 2102 8541 6847 6131 19062 29461 2278 4942 3501 13675 2795 1017 1024 4708 2836 2006 7020 2102 2005 9671 8310 1997 2731 2951 1012 7020 2102 6163 1012 1017 2243 5836 1996 2440 2731 2275 1012 2478 2069 1015 1010 2199 4973 2005 2731 1010 4651 4083 2013 2224 1056 2003 2583 2000 6855 2836 2008 9169 2116 1997 1996 2060 4275 4738 2006 1996 2440 6163 1012 1017 4595 2742 2731 2275 1012 26163 2015 2007 2053 4651 4083 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 2795 1018 1024 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 2005 15913 102 0 0 0 0 0 0 0 0 0 0
I0622 09:43:47.967980 140630242117440 run_classifier_sci.py:542] input_ids: 101 15792 4106 1025 4942 3501 1025 10640 102 5415 6251 4372 16044 2099 2057 2556 4275 2005 17181 11746 2046 7861 8270 4667 19019 2008 4919 4539 4651 4083 2000 2060 17953 2361 8518 1012 1996 4275 2024 8114 1998 2765 1999 8321 2836 2006 7578 4651 8518 1012 2048 10176 1997 1996 17181 4275 3499 2005 3119 1011 12446 2090 10640 1998 24134 4219 1012 2005 2119 10176 1010 2057 8556 1998 3189 1996 3276 2090 2944 11619 1010 7692 8381 1010 1996 11343 1997 4651 4708 2731 2951 1010 1998 4708 2836 1012 18539 2024 2081 2007 2918 1011 3210 2008 2224 2773 2504 4651 4083 3081 3653 23654 2098 2773 7861 8270 4667 2015 2004 2092 2004 26163 2015 2079 2025 2224 2151 4651 4083 1012 2057 2424 2008 4651 4083 2478 6251 7861 8270 4667 2015 12102 2000 2041 4842 14192 2773 2504 4651 1012 2007 4651 4083 3081 6251 7861 8270 4667 2015 1010 2057 11949 10889 2204 2836 2007 10124 8310 1997 13588 2731 2951 2005 2050 4651 4708 1012 2057 6855 11434 3463 2006 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 9416 2012 25952 2944 13827 1012 2256 3653 1011 4738 6251 17181 4275 2024 2081 10350 2800 2005 8816 1998 2006 1056 2546 9594 1012 4728 1010 23760 28689 22828 2015 2024 15757 2011 2892 10175 8524 3508 2006 1996 4708 2731 2951 2043 2800 2030 1996 9312 3231 2951 2043 4445 2731 4496 16475 2951 2024 3024 2731 17993 2702 2335 2005 2169 4651 4708 2944 2007 2367 18154 3988 3550 15871 1998 2057 3189 9312 3463 2011 14985 2408 3216 2000 14358 13827 1999 2256 17181 4275 1010 2057 16157 1996 3997 1997 2536 8924 4342 2011 2256 2944 2006 2057 4017 2773 7201 2795 1015 1024 4651 4708 9312 4520 1015 1010 6445 2487 16475 3231 1015 1010 4261 2683 1007 1012 2795 1016 1024 2944 2836 2006 4651 8518 1012 2224 1056 2003 1996 5415 6251 4372 16044 2099 1006 2224 1007 2478 10938 2121 1012 2224 1040 2003 1996 5415 4372 16044 2099 4907 2944 1012 4275 26610 2007 26163 2015 2007 2053 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 1011 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 2720 7020 2102 8541 6847 6131 19062 29461 2278 4942 3501 13675 2795 1017 1024 4708 2836 2006 7020 2102 2005 9671 8310 1997 2731 2951 1012 7020 2102 6163 1012 1017 2243 5836 1996 2440 2731 2275 1012 2478 2069 1015 1010 2199 4973 2005 2731 1010 4651 4083 2013 2224 1056 2003 2583 2000 6855 2836 2008 9169 2116 1997 1996 2060 4275 4738 2006 1996 2440 6163 1012 1017 4595 2742 2731 2275 1012 26163 2015 2007 2053 4651 4083 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 2795 1018 1024 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 2005 15913 102 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0
I0622 09:43:47.968262 140630242117440 run_classifier_sci.py:543] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0
I0622 09:43:47.968455 140630242117440 run_classifier_sci.py:544] segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:label: true (id = 0)
I0622 09:43:47.968524 140630242117440 run_classifier_sci.py:545] label: true (id = 0)
INFO:tensorflow:*** Example ***
I0622 09:43:47.975285 140630242117440 run_classifier_sci.py:538] *** Example ***
INFO:tensorflow:guid: test-1
I0622 09:43:47.975435 140630242117440 run_classifier_sci.py:539] guid: test-1
INFO:tensorflow:tokens: [CLS] text classification ; tre ##c ; error [SEP] universal sentence en ##code ##r we present models for encoding sentences into em ##bed ##ding vectors that specifically target transfer learning to other nl ##p tasks . the models are efficient and result in accurate performance on diverse transfer tasks . two variants of the encoding models allow for trade - offs between accuracy and compute resources . for both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance . comparisons are made with base - lines that use word level transfer learning via pre ##train ##ed word em ##bed ##ding ##s as well as baseline ##s do not use any transfer learning . we find that transfer learning using sentence em ##bed ##ding ##s tends to out ##per ##form word level transfer . with transfer learning via sentence em ##bed ##ding ##s , we observe surprisingly good performance with minimal amounts of supervised training data for ##a transfer task . we obtain encouraging results on word em ##bed ##ding association tests ( we ##at ) targeted at detecting model bias . our pre - trained sentence encoding models are made freely available for download and on t ##f hub . otherwise , hyper ##para ##meter ##s are tuned by cross ##val ##ida ##tion on the task training data when available or the evaluation test data when neither training nor dev data are provided training repeats ten times for each transfer task model with different randomly initial ##ized weights and we report evaluation results by averaging across runs to assess bias in our encoding models , we evaluate the strength of various associations learned by our model on we ##at word lists table 1 : transfer task evaluation sets 1 , 82 ##1 dev test 1 , 37 ##9 ) . table 2 : model performance on transfer tasks . use t is the universal sentence en ##code ##r ( use ) using transform ##er . use d is the universal en ##code ##r dan model . models tagged with baseline ##s with no transfer learning sentence & word em ##bed ##ding transfer learning - sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning mr ss ##t sts bench mp ##qa tre ##c sub ##j cr table 3 : task performance on ss ##t for varying amounts of training data . ss ##t 67 . 3 ##k represents the full training set . using only 1 , 000 examples for training , transfer learning from use t is able to obtain performance that rivals many of the other models trained on the full 67 . 3 thousand example training set . baseline ##s with no transfer learning sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning sentence & word em ##bed ##ding transfer learning table 4 : word em ##bed ##ding association tests ( we ##at ) for glove [SEP]
I0622 09:43:47.975681 140630242117440 run_classifier_sci.py:541] tokens: [CLS] text classification ; tre ##c ; error [SEP] universal sentence en ##code ##r we present models for encoding sentences into em ##bed ##ding vectors that specifically target transfer learning to other nl ##p tasks . the models are efficient and result in accurate performance on diverse transfer tasks . two variants of the encoding models allow for trade - offs between accuracy and compute resources . for both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance . comparisons are made with base - lines that use word level transfer learning via pre ##train ##ed word em ##bed ##ding ##s as well as baseline ##s do not use any transfer learning . we find that transfer learning using sentence em ##bed ##ding ##s tends to out ##per ##form word level transfer . with transfer learning via sentence em ##bed ##ding ##s , we observe surprisingly good performance with minimal amounts of supervised training data for ##a transfer task . we obtain encouraging results on word em ##bed ##ding association tests ( we ##at ) targeted at detecting model bias . our pre - trained sentence encoding models are made freely available for download and on t ##f hub . otherwise , hyper ##para ##meter ##s are tuned by cross ##val ##ida ##tion on the task training data when available or the evaluation test data when neither training nor dev data are provided training repeats ten times for each transfer task model with different randomly initial ##ized weights and we report evaluation results by averaging across runs to assess bias in our encoding models , we evaluate the strength of various associations learned by our model on we ##at word lists table 1 : transfer task evaluation sets 1 , 82 ##1 dev test 1 , 37 ##9 ) . table 2 : model performance on transfer tasks . use t is the universal sentence en ##code ##r ( use ) using transform ##er . use d is the universal en ##code ##r dan model . models tagged with baseline ##s with no transfer learning sentence & word em ##bed ##ding transfer learning - sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning mr ss ##t sts bench mp ##qa tre ##c sub ##j cr table 3 : task performance on ss ##t for varying amounts of training data . ss ##t 67 . 3 ##k represents the full training set . using only 1 , 000 examples for training , transfer learning from use t is able to obtain performance that rivals many of the other models trained on the full 67 . 3 thousand example training set . baseline ##s with no transfer learning sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning sentence & word em ##bed ##ding transfer learning table 4 : word em ##bed ##ding association tests ( we ##at ) for glove [SEP]
INFO:tensorflow:input_ids: 101 3793 5579 1025 29461 2278 1025 7561 102 5415 6251 4372 16044 2099 2057 2556 4275 2005 17181 11746 2046 7861 8270 4667 19019 2008 4919 4539 4651 4083 2000 2060 17953 2361 8518 1012 1996 4275 2024 8114 1998 2765 1999 8321 2836 2006 7578 4651 8518 1012 2048 10176 1997 1996 17181 4275 3499 2005 3119 1011 12446 2090 10640 1998 24134 4219 1012 2005 2119 10176 1010 2057 8556 1998 3189 1996 3276 2090 2944 11619 1010 7692 8381 1010 1996 11343 1997 4651 4708 2731 2951 1010 1998 4708 2836 1012 18539 2024 2081 2007 2918 1011 3210 2008 2224 2773 2504 4651 4083 3081 3653 23654 2098 2773 7861 8270 4667 2015 2004 2092 2004 26163 2015 2079 2025 2224 2151 4651 4083 1012 2057 2424 2008 4651 4083 2478 6251 7861 8270 4667 2015 12102 2000 2041 4842 14192 2773 2504 4651 1012 2007 4651 4083 3081 6251 7861 8270 4667 2015 1010 2057 11949 10889 2204 2836 2007 10124 8310 1997 13588 2731 2951 2005 2050 4651 4708 1012 2057 6855 11434 3463 2006 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 9416 2012 25952 2944 13827 1012 2256 3653 1011 4738 6251 17181 4275 2024 2081 10350 2800 2005 8816 1998 2006 1056 2546 9594 1012 4728 1010 23760 28689 22828 2015 2024 15757 2011 2892 10175 8524 3508 2006 1996 4708 2731 2951 2043 2800 2030 1996 9312 3231 2951 2043 4445 2731 4496 16475 2951 2024 3024 2731 17993 2702 2335 2005 2169 4651 4708 2944 2007 2367 18154 3988 3550 15871 1998 2057 3189 9312 3463 2011 14985 2408 3216 2000 14358 13827 1999 2256 17181 4275 1010 2057 16157 1996 3997 1997 2536 8924 4342 2011 2256 2944 2006 2057 4017 2773 7201 2795 1015 1024 4651 4708 9312 4520 1015 1010 6445 2487 16475 3231 1015 1010 4261 2683 1007 1012 2795 1016 1024 2944 2836 2006 4651 8518 1012 2224 1056 2003 1996 5415 6251 4372 16044 2099 1006 2224 1007 2478 10938 2121 1012 2224 1040 2003 1996 5415 4372 16044 2099 4907 2944 1012 4275 26610 2007 26163 2015 2007 2053 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 1011 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 2720 7020 2102 8541 6847 6131 19062 29461 2278 4942 3501 13675 2795 1017 1024 4708 2836 2006 7020 2102 2005 9671 8310 1997 2731 2951 1012 7020 2102 6163 1012 1017 2243 5836 1996 2440 2731 2275 1012 2478 2069 1015 1010 2199 4973 2005 2731 1010 4651 4083 2013 2224 1056 2003 2583 2000 6855 2836 2008 9169 2116 1997 1996 2060 4275 4738 2006 1996 2440 6163 1012 1017 4595 2742 2731 2275 1012 26163 2015 2007 2053 4651 4083 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 2795 1018 1024 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 2005 15913 102 0 0 0 0 0 0 0 0 0 0
I0622 09:43:47.975925 140630242117440 run_classifier_sci.py:542] input_ids: 101 3793 5579 1025 29461 2278 1025 7561 102 5415 6251 4372 16044 2099 2057 2556 4275 2005 17181 11746 2046 7861 8270 4667 19019 2008 4919 4539 4651 4083 2000 2060 17953 2361 8518 1012 1996 4275 2024 8114 1998 2765 1999 8321 2836 2006 7578 4651 8518 1012 2048 10176 1997 1996 17181 4275 3499 2005 3119 1011 12446 2090 10640 1998 24134 4219 1012 2005 2119 10176 1010 2057 8556 1998 3189 1996 3276 2090 2944 11619 1010 7692 8381 1010 1996 11343 1997 4651 4708 2731 2951 1010 1998 4708 2836 1012 18539 2024 2081 2007 2918 1011 3210 2008 2224 2773 2504 4651 4083 3081 3653 23654 2098 2773 7861 8270 4667 2015 2004 2092 2004 26163 2015 2079 2025 2224 2151 4651 4083 1012 2057 2424 2008 4651 4083 2478 6251 7861 8270 4667 2015 12102 2000 2041 4842 14192 2773 2504 4651 1012 2007 4651 4083 3081 6251 7861 8270 4667 2015 1010 2057 11949 10889 2204 2836 2007 10124 8310 1997 13588 2731 2951 2005 2050 4651 4708 1012 2057 6855 11434 3463 2006 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 9416 2012 25952 2944 13827 1012 2256 3653 1011 4738 6251 17181 4275 2024 2081 10350 2800 2005 8816 1998 2006 1056 2546 9594 1012 4728 1010 23760 28689 22828 2015 2024 15757 2011 2892 10175 8524 3508 2006 1996 4708 2731 2951 2043 2800 2030 1996 9312 3231 2951 2043 4445 2731 4496 16475 2951 2024 3024 2731 17993 2702 2335 2005 2169 4651 4708 2944 2007 2367 18154 3988 3550 15871 1998 2057 3189 9312 3463 2011 14985 2408 3216 2000 14358 13827 1999 2256 17181 4275 1010 2057 16157 1996 3997 1997 2536 8924 4342 2011 2256 2944 2006 2057 4017 2773 7201 2795 1015 1024 4651 4708 9312 4520 1015 1010 6445 2487 16475 3231 1015 1010 4261 2683 1007 1012 2795 1016 1024 2944 2836 2006 4651 8518 1012 2224 1056 2003 1996 5415 6251 4372 16044 2099 1006 2224 1007 2478 10938 2121 1012 2224 1040 2003 1996 5415 4372 16044 2099 4907 2944 1012 4275 26610 2007 26163 2015 2007 2053 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 1011 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 2720 7020 2102 8541 6847 6131 19062 29461 2278 4942 3501 13675 2795 1017 1024 4708 2836 2006 7020 2102 2005 9671 8310 1997 2731 2951 1012 7020 2102 6163 1012 1017 2243 5836 1996 2440 2731 2275 1012 2478 2069 1015 1010 2199 4973 2005 2731 1010 4651 4083 2013 2224 1056 2003 2583 2000 6855 2836 2008 9169 2116 1997 1996 2060 4275 4738 2006 1996 2440 6163 1012 1017 4595 2742 2731 2275 1012 26163 2015 2007 2053 4651 4083 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 2795 1018 1024 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 2005 15913 102 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0
I0622 09:43:47.976248 140630242117440 run_classifier_sci.py:543] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0
I0622 09:43:47.976436 140630242117440 run_classifier_sci.py:544] segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:label: true (id = 0)
I0622 09:43:47.976503 140630242117440 run_classifier_sci.py:545] label: true (id = 0)
INFO:tensorflow:*** Example ***
I0622 09:43:47.984496 140630242117440 run_classifier_sci.py:538] *** Example ***
INFO:tensorflow:guid: test-2
I0622 09:43:47.984601 140630242117440 run_classifier_sci.py:539] guid: test-2
INFO:tensorflow:tokens: [CLS] question answering ; squad ; f1 [SEP] universal sentence en ##code ##r we present models for encoding sentences into em ##bed ##ding vectors that specifically target transfer learning to other nl ##p tasks . the models are efficient and result in accurate performance on diverse transfer tasks . two variants of the encoding models allow for trade - offs between accuracy and compute resources . for both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance . comparisons are made with base - lines that use word level transfer learning via pre ##train ##ed word em ##bed ##ding ##s as well as baseline ##s do not use any transfer learning . we find that transfer learning using sentence em ##bed ##ding ##s tends to out ##per ##form word level transfer . with transfer learning via sentence em ##bed ##ding ##s , we observe surprisingly good performance with minimal amounts of supervised training data for ##a transfer task . we obtain encouraging results on word em ##bed ##ding association tests ( we ##at ) targeted at detecting model bias . our pre - trained sentence encoding models are made freely available for download and on t ##f hub . otherwise , hyper ##para ##meter ##s are tuned by cross ##val ##ida ##tion on the task training data when available or the evaluation test data when neither training nor dev data are provided training repeats ten times for each transfer task model with different randomly initial ##ized weights and we report evaluation results by averaging across runs to assess bias in our encoding models , we evaluate the strength of various associations learned by our model on we ##at word lists table 1 : transfer task evaluation sets 1 , 82 ##1 dev test 1 , 37 ##9 ) . table 2 : model performance on transfer tasks . use t is the universal sentence en ##code ##r ( use ) using transform ##er . use d is the universal en ##code ##r dan model . models tagged with baseline ##s with no transfer learning sentence & word em ##bed ##ding transfer learning - sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning mr ss ##t sts bench mp ##qa tre ##c sub ##j cr table 3 : task performance on ss ##t for varying amounts of training data . ss ##t 67 . 3 ##k represents the full training set . using only 1 , 000 examples for training , transfer learning from use t is able to obtain performance that rivals many of the other models trained on the full 67 . 3 thousand example training set . baseline ##s with no transfer learning sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning sentence & word em ##bed ##ding transfer learning table 4 : word em ##bed ##ding association tests ( we ##at ) for glove [SEP]
I0622 09:43:47.984859 140630242117440 run_classifier_sci.py:541] tokens: [CLS] question answering ; squad ; f1 [SEP] universal sentence en ##code ##r we present models for encoding sentences into em ##bed ##ding vectors that specifically target transfer learning to other nl ##p tasks . the models are efficient and result in accurate performance on diverse transfer tasks . two variants of the encoding models allow for trade - offs between accuracy and compute resources . for both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance . comparisons are made with base - lines that use word level transfer learning via pre ##train ##ed word em ##bed ##ding ##s as well as baseline ##s do not use any transfer learning . we find that transfer learning using sentence em ##bed ##ding ##s tends to out ##per ##form word level transfer . with transfer learning via sentence em ##bed ##ding ##s , we observe surprisingly good performance with minimal amounts of supervised training data for ##a transfer task . we obtain encouraging results on word em ##bed ##ding association tests ( we ##at ) targeted at detecting model bias . our pre - trained sentence encoding models are made freely available for download and on t ##f hub . otherwise , hyper ##para ##meter ##s are tuned by cross ##val ##ida ##tion on the task training data when available or the evaluation test data when neither training nor dev data are provided training repeats ten times for each transfer task model with different randomly initial ##ized weights and we report evaluation results by averaging across runs to assess bias in our encoding models , we evaluate the strength of various associations learned by our model on we ##at word lists table 1 : transfer task evaluation sets 1 , 82 ##1 dev test 1 , 37 ##9 ) . table 2 : model performance on transfer tasks . use t is the universal sentence en ##code ##r ( use ) using transform ##er . use d is the universal en ##code ##r dan model . models tagged with baseline ##s with no transfer learning sentence & word em ##bed ##ding transfer learning - sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning mr ss ##t sts bench mp ##qa tre ##c sub ##j cr table 3 : task performance on ss ##t for varying amounts of training data . ss ##t 67 . 3 ##k represents the full training set . using only 1 , 000 examples for training , transfer learning from use t is able to obtain performance that rivals many of the other models trained on the full 67 . 3 thousand example training set . baseline ##s with no transfer learning sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning sentence & word em ##bed ##ding transfer learning table 4 : word em ##bed ##ding association tests ( we ##at ) for glove [SEP]
INFO:tensorflow:input_ids: 101 3160 10739 1025 4686 1025 20069 102 5415 6251 4372 16044 2099 2057 2556 4275 2005 17181 11746 2046 7861 8270 4667 19019 2008 4919 4539 4651 4083 2000 2060 17953 2361 8518 1012 1996 4275 2024 8114 1998 2765 1999 8321 2836 2006 7578 4651 8518 1012 2048 10176 1997 1996 17181 4275 3499 2005 3119 1011 12446 2090 10640 1998 24134 4219 1012 2005 2119 10176 1010 2057 8556 1998 3189 1996 3276 2090 2944 11619 1010 7692 8381 1010 1996 11343 1997 4651 4708 2731 2951 1010 1998 4708 2836 1012 18539 2024 2081 2007 2918 1011 3210 2008 2224 2773 2504 4651 4083 3081 3653 23654 2098 2773 7861 8270 4667 2015 2004 2092 2004 26163 2015 2079 2025 2224 2151 4651 4083 1012 2057 2424 2008 4651 4083 2478 6251 7861 8270 4667 2015 12102 2000 2041 4842 14192 2773 2504 4651 1012 2007 4651 4083 3081 6251 7861 8270 4667 2015 1010 2057 11949 10889 2204 2836 2007 10124 8310 1997 13588 2731 2951 2005 2050 4651 4708 1012 2057 6855 11434 3463 2006 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 9416 2012 25952 2944 13827 1012 2256 3653 1011 4738 6251 17181 4275 2024 2081 10350 2800 2005 8816 1998 2006 1056 2546 9594 1012 4728 1010 23760 28689 22828 2015 2024 15757 2011 2892 10175 8524 3508 2006 1996 4708 2731 2951 2043 2800 2030 1996 9312 3231 2951 2043 4445 2731 4496 16475 2951 2024 3024 2731 17993 2702 2335 2005 2169 4651 4708 2944 2007 2367 18154 3988 3550 15871 1998 2057 3189 9312 3463 2011 14985 2408 3216 2000 14358 13827 1999 2256 17181 4275 1010 2057 16157 1996 3997 1997 2536 8924 4342 2011 2256 2944 2006 2057 4017 2773 7201 2795 1015 1024 4651 4708 9312 4520 1015 1010 6445 2487 16475 3231 1015 1010 4261 2683 1007 1012 2795 1016 1024 2944 2836 2006 4651 8518 1012 2224 1056 2003 1996 5415 6251 4372 16044 2099 1006 2224 1007 2478 10938 2121 1012 2224 1040 2003 1996 5415 4372 16044 2099 4907 2944 1012 4275 26610 2007 26163 2015 2007 2053 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 1011 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 2720 7020 2102 8541 6847 6131 19062 29461 2278 4942 3501 13675 2795 1017 1024 4708 2836 2006 7020 2102 2005 9671 8310 1997 2731 2951 1012 7020 2102 6163 1012 1017 2243 5836 1996 2440 2731 2275 1012 2478 2069 1015 1010 2199 4973 2005 2731 1010 4651 4083 2013 2224 1056 2003 2583 2000 6855 2836 2008 9169 2116 1997 1996 2060 4275 4738 2006 1996 2440 6163 1012 1017 4595 2742 2731 2275 1012 26163 2015 2007 2053 4651 4083 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 2795 1018 1024 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 2005 15913 102 0 0 0 0 0 0 0 0 0 0 0
I0622 09:43:47.985097 140630242117440 run_classifier_sci.py:542] input_ids: 101 3160 10739 1025 4686 1025 20069 102 5415 6251 4372 16044 2099 2057 2556 4275 2005 17181 11746 2046 7861 8270 4667 19019 2008 4919 4539 4651 4083 2000 2060 17953 2361 8518 1012 1996 4275 2024 8114 1998 2765 1999 8321 2836 2006 7578 4651 8518 1012 2048 10176 1997 1996 17181 4275 3499 2005 3119 1011 12446 2090 10640 1998 24134 4219 1012 2005 2119 10176 1010 2057 8556 1998 3189 1996 3276 2090 2944 11619 1010 7692 8381 1010 1996 11343 1997 4651 4708 2731 2951 1010 1998 4708 2836 1012 18539 2024 2081 2007 2918 1011 3210 2008 2224 2773 2504 4651 4083 3081 3653 23654 2098 2773 7861 8270 4667 2015 2004 2092 2004 26163 2015 2079 2025 2224 2151 4651 4083 1012 2057 2424 2008 4651 4083 2478 6251 7861 8270 4667 2015 12102 2000 2041 4842 14192 2773 2504 4651 1012 2007 4651 4083 3081 6251 7861 8270 4667 2015 1010 2057 11949 10889 2204 2836 2007 10124 8310 1997 13588 2731 2951 2005 2050 4651 4708 1012 2057 6855 11434 3463 2006 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 9416 2012 25952 2944 13827 1012 2256 3653 1011 4738 6251 17181 4275 2024 2081 10350 2800 2005 8816 1998 2006 1056 2546 9594 1012 4728 1010 23760 28689 22828 2015 2024 15757 2011 2892 10175 8524 3508 2006 1996 4708 2731 2951 2043 2800 2030 1996 9312 3231 2951 2043 4445 2731 4496 16475 2951 2024 3024 2731 17993 2702 2335 2005 2169 4651 4708 2944 2007 2367 18154 3988 3550 15871 1998 2057 3189 9312 3463 2011 14985 2408 3216 2000 14358 13827 1999 2256 17181 4275 1010 2057 16157 1996 3997 1997 2536 8924 4342 2011 2256 2944 2006 2057 4017 2773 7201 2795 1015 1024 4651 4708 9312 4520 1015 1010 6445 2487 16475 3231 1015 1010 4261 2683 1007 1012 2795 1016 1024 2944 2836 2006 4651 8518 1012 2224 1056 2003 1996 5415 6251 4372 16044 2099 1006 2224 1007 2478 10938 2121 1012 2224 1040 2003 1996 5415 4372 16044 2099 4907 2944 1012 4275 26610 2007 26163 2015 2007 2053 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 1011 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 2720 7020 2102 8541 6847 6131 19062 29461 2278 4942 3501 13675 2795 1017 1024 4708 2836 2006 7020 2102 2005 9671 8310 1997 2731 2951 1012 7020 2102 6163 1012 1017 2243 5836 1996 2440 2731 2275 1012 2478 2069 1015 1010 2199 4973 2005 2731 1010 4651 4083 2013 2224 1056 2003 2583 2000 6855 2836 2008 9169 2116 1997 1996 2060 4275 4738 2006 1996 2440 6163 1012 1017 4595 2742 2731 2275 1012 26163 2015 2007 2053 4651 4083 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 2795 1018 1024 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 2005 15913 102 0 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0
I0622 09:43:47.985414 140630242117440 run_classifier_sci.py:543] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0
I0622 09:43:47.985599 140630242117440 run_classifier_sci.py:544] segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:label: true (id = 0)
I0622 09:43:47.985665 140630242117440 run_classifier_sci.py:545] label: true (id = 0)
INFO:tensorflow:*** Example ***
I0622 09:43:47.992361 140630242117440 run_classifier_sci.py:538] *** Example ***
INFO:tensorflow:guid: test-3
I0622 09:43:47.992464 140630242117440 run_classifier_sci.py:539] guid: test-3
INFO:tensorflow:tokens: [CLS] relation prediction ; f ##b ##15 ##k - 237 ; h @ 1 [SEP] universal sentence en ##code ##r we present models for encoding sentences into em ##bed ##ding vectors that specifically target transfer learning to other nl ##p tasks . the models are efficient and result in accurate performance on diverse transfer tasks . two variants of the encoding models allow for trade - offs between accuracy and compute resources . for both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance . comparisons are made with base - lines that use word level transfer learning via pre ##train ##ed word em ##bed ##ding ##s as well as baseline ##s do not use any transfer learning . we find that transfer learning using sentence em ##bed ##ding ##s tends to out ##per ##form word level transfer . with transfer learning via sentence em ##bed ##ding ##s , we observe surprisingly good performance with minimal amounts of supervised training data for ##a transfer task . we obtain encouraging results on word em ##bed ##ding association tests ( we ##at ) targeted at detecting model bias . our pre - trained sentence encoding models are made freely available for download and on t ##f hub . otherwise , hyper ##para ##meter ##s are tuned by cross ##val ##ida ##tion on the task training data when available or the evaluation test data when neither training nor dev data are provided training repeats ten times for each transfer task model with different randomly initial ##ized weights and we report evaluation results by averaging across runs to assess bias in our encoding models , we evaluate the strength of various associations learned by our model on we ##at word lists table 1 : transfer task evaluation sets 1 , 82 ##1 dev test 1 , 37 ##9 ) . table 2 : model performance on transfer tasks . use t is the universal sentence en ##code ##r ( use ) using transform ##er . use d is the universal en ##code ##r dan model . models tagged with baseline ##s with no transfer learning sentence & word em ##bed ##ding transfer learning - sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning mr ss ##t sts bench mp ##qa tre ##c sub ##j cr table 3 : task performance on ss ##t for varying amounts of training data . ss ##t 67 . 3 ##k represents the full training set . using only 1 , 000 examples for training , transfer learning from use t is able to obtain performance that rivals many of the other models trained on the full 67 . 3 thousand example training set . baseline ##s with no transfer learning sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning sentence & word em ##bed ##ding transfer learning table 4 : word em ##bed ##ding association tests ( we ##at ) for glove [SEP]
I0622 09:43:47.992724 140630242117440 run_classifier_sci.py:541] tokens: [CLS] relation prediction ; f ##b ##15 ##k - 237 ; h @ 1 [SEP] universal sentence en ##code ##r we present models for encoding sentences into em ##bed ##ding vectors that specifically target transfer learning to other nl ##p tasks . the models are efficient and result in accurate performance on diverse transfer tasks . two variants of the encoding models allow for trade - offs between accuracy and compute resources . for both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance . comparisons are made with base - lines that use word level transfer learning via pre ##train ##ed word em ##bed ##ding ##s as well as baseline ##s do not use any transfer learning . we find that transfer learning using sentence em ##bed ##ding ##s tends to out ##per ##form word level transfer . with transfer learning via sentence em ##bed ##ding ##s , we observe surprisingly good performance with minimal amounts of supervised training data for ##a transfer task . we obtain encouraging results on word em ##bed ##ding association tests ( we ##at ) targeted at detecting model bias . our pre - trained sentence encoding models are made freely available for download and on t ##f hub . otherwise , hyper ##para ##meter ##s are tuned by cross ##val ##ida ##tion on the task training data when available or the evaluation test data when neither training nor dev data are provided training repeats ten times for each transfer task model with different randomly initial ##ized weights and we report evaluation results by averaging across runs to assess bias in our encoding models , we evaluate the strength of various associations learned by our model on we ##at word lists table 1 : transfer task evaluation sets 1 , 82 ##1 dev test 1 , 37 ##9 ) . table 2 : model performance on transfer tasks . use t is the universal sentence en ##code ##r ( use ) using transform ##er . use d is the universal en ##code ##r dan model . models tagged with baseline ##s with no transfer learning sentence & word em ##bed ##ding transfer learning - sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning mr ss ##t sts bench mp ##qa tre ##c sub ##j cr table 3 : task performance on ss ##t for varying amounts of training data . ss ##t 67 . 3 ##k represents the full training set . using only 1 , 000 examples for training , transfer learning from use t is able to obtain performance that rivals many of the other models trained on the full 67 . 3 thousand example training set . baseline ##s with no transfer learning sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning sentence & word em ##bed ##ding transfer learning table 4 : word em ##bed ##ding association tests ( we ##at ) for glove [SEP]
INFO:tensorflow:input_ids: 101 7189 17547 1025 1042 2497 16068 2243 1011 23297 1025 1044 1030 1015 102 5415 6251 4372 16044 2099 2057 2556 4275 2005 17181 11746 2046 7861 8270 4667 19019 2008 4919 4539 4651 4083 2000 2060 17953 2361 8518 1012 1996 4275 2024 8114 1998 2765 1999 8321 2836 2006 7578 4651 8518 1012 2048 10176 1997 1996 17181 4275 3499 2005 3119 1011 12446 2090 10640 1998 24134 4219 1012 2005 2119 10176 1010 2057 8556 1998 3189 1996 3276 2090 2944 11619 1010 7692 8381 1010 1996 11343 1997 4651 4708 2731 2951 1010 1998 4708 2836 1012 18539 2024 2081 2007 2918 1011 3210 2008 2224 2773 2504 4651 4083 3081 3653 23654 2098 2773 7861 8270 4667 2015 2004 2092 2004 26163 2015 2079 2025 2224 2151 4651 4083 1012 2057 2424 2008 4651 4083 2478 6251 7861 8270 4667 2015 12102 2000 2041 4842 14192 2773 2504 4651 1012 2007 4651 4083 3081 6251 7861 8270 4667 2015 1010 2057 11949 10889 2204 2836 2007 10124 8310 1997 13588 2731 2951 2005 2050 4651 4708 1012 2057 6855 11434 3463 2006 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 9416 2012 25952 2944 13827 1012 2256 3653 1011 4738 6251 17181 4275 2024 2081 10350 2800 2005 8816 1998 2006 1056 2546 9594 1012 4728 1010 23760 28689 22828 2015 2024 15757 2011 2892 10175 8524 3508 2006 1996 4708 2731 2951 2043 2800 2030 1996 9312 3231 2951 2043 4445 2731 4496 16475 2951 2024 3024 2731 17993 2702 2335 2005 2169 4651 4708 2944 2007 2367 18154 3988 3550 15871 1998 2057 3189 9312 3463 2011 14985 2408 3216 2000 14358 13827 1999 2256 17181 4275 1010 2057 16157 1996 3997 1997 2536 8924 4342 2011 2256 2944 2006 2057 4017 2773 7201 2795 1015 1024 4651 4708 9312 4520 1015 1010 6445 2487 16475 3231 1015 1010 4261 2683 1007 1012 2795 1016 1024 2944 2836 2006 4651 8518 1012 2224 1056 2003 1996 5415 6251 4372 16044 2099 1006 2224 1007 2478 10938 2121 1012 2224 1040 2003 1996 5415 4372 16044 2099 4907 2944 1012 4275 26610 2007 26163 2015 2007 2053 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 1011 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 2720 7020 2102 8541 6847 6131 19062 29461 2278 4942 3501 13675 2795 1017 1024 4708 2836 2006 7020 2102 2005 9671 8310 1997 2731 2951 1012 7020 2102 6163 1012 1017 2243 5836 1996 2440 2731 2275 1012 2478 2069 1015 1010 2199 4973 2005 2731 1010 4651 4083 2013 2224 1056 2003 2583 2000 6855 2836 2008 9169 2116 1997 1996 2060 4275 4738 2006 1996 2440 6163 1012 1017 4595 2742 2731 2275 1012 26163 2015 2007 2053 4651 4083 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 2795 1018 1024 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 2005 15913 102 0 0 0 0
I0622 09:43:47.992960 140630242117440 run_classifier_sci.py:542] input_ids: 101 7189 17547 1025 1042 2497 16068 2243 1011 23297 1025 1044 1030 1015 102 5415 6251 4372 16044 2099 2057 2556 4275 2005 17181 11746 2046 7861 8270 4667 19019 2008 4919 4539 4651 4083 2000 2060 17953 2361 8518 1012 1996 4275 2024 8114 1998 2765 1999 8321 2836 2006 7578 4651 8518 1012 2048 10176 1997 1996 17181 4275 3499 2005 3119 1011 12446 2090 10640 1998 24134 4219 1012 2005 2119 10176 1010 2057 8556 1998 3189 1996 3276 2090 2944 11619 1010 7692 8381 1010 1996 11343 1997 4651 4708 2731 2951 1010 1998 4708 2836 1012 18539 2024 2081 2007 2918 1011 3210 2008 2224 2773 2504 4651 4083 3081 3653 23654 2098 2773 7861 8270 4667 2015 2004 2092 2004 26163 2015 2079 2025 2224 2151 4651 4083 1012 2057 2424 2008 4651 4083 2478 6251 7861 8270 4667 2015 12102 2000 2041 4842 14192 2773 2504 4651 1012 2007 4651 4083 3081 6251 7861 8270 4667 2015 1010 2057 11949 10889 2204 2836 2007 10124 8310 1997 13588 2731 2951 2005 2050 4651 4708 1012 2057 6855 11434 3463 2006 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 9416 2012 25952 2944 13827 1012 2256 3653 1011 4738 6251 17181 4275 2024 2081 10350 2800 2005 8816 1998 2006 1056 2546 9594 1012 4728 1010 23760 28689 22828 2015 2024 15757 2011 2892 10175 8524 3508 2006 1996 4708 2731 2951 2043 2800 2030 1996 9312 3231 2951 2043 4445 2731 4496 16475 2951 2024 3024 2731 17993 2702 2335 2005 2169 4651 4708 2944 2007 2367 18154 3988 3550 15871 1998 2057 3189 9312 3463 2011 14985 2408 3216 2000 14358 13827 1999 2256 17181 4275 1010 2057 16157 1996 3997 1997 2536 8924 4342 2011 2256 2944 2006 2057 4017 2773 7201 2795 1015 1024 4651 4708 9312 4520 1015 1010 6445 2487 16475 3231 1015 1010 4261 2683 1007 1012 2795 1016 1024 2944 2836 2006 4651 8518 1012 2224 1056 2003 1996 5415 6251 4372 16044 2099 1006 2224 1007 2478 10938 2121 1012 2224 1040 2003 1996 5415 4372 16044 2099 4907 2944 1012 4275 26610 2007 26163 2015 2007 2053 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 1011 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 2720 7020 2102 8541 6847 6131 19062 29461 2278 4942 3501 13675 2795 1017 1024 4708 2836 2006 7020 2102 2005 9671 8310 1997 2731 2951 1012 7020 2102 6163 1012 1017 2243 5836 1996 2440 2731 2275 1012 2478 2069 1015 1010 2199 4973 2005 2731 1010 4651 4083 2013 2224 1056 2003 2583 2000 6855 2836 2008 9169 2116 1997 1996 2060 4275 4738 2006 1996 2440 6163 1012 1017 4595 2742 2731 2275 1012 26163 2015 2007 2053 4651 4083 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 2795 1018 1024 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 2005 15913 102 0 0 0 0
INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0
I0622 09:43:47.993280 140630242117440 run_classifier_sci.py:543] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0
INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0
I0622 09:43:47.993464 140630242117440 run_classifier_sci.py:544] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0
INFO:tensorflow:label: true (id = 0)
I0622 09:43:47.993531 140630242117440 run_classifier_sci.py:545] label: true (id = 0)
INFO:tensorflow:*** Example ***
I0622 09:43:48.000295 140630242117440 run_classifier_sci.py:538] *** Example ***
INFO:tensorflow:guid: test-4
I0622 09:43:48.000396 140630242117440 run_classifier_sci.py:539] guid: test-4
INFO:tensorflow:tokens: [CLS] word sense di ##sam ##bi ##gua ##tion ; se ##me ##val 2013 ; f1 [SEP] universal sentence en ##code ##r we present models for encoding sentences into em ##bed ##ding vectors that specifically target transfer learning to other nl ##p tasks . the models are efficient and result in accurate performance on diverse transfer tasks . two variants of the encoding models allow for trade - offs between accuracy and compute resources . for both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance . comparisons are made with base - lines that use word level transfer learning via pre ##train ##ed word em ##bed ##ding ##s as well as baseline ##s do not use any transfer learning . we find that transfer learning using sentence em ##bed ##ding ##s tends to out ##per ##form word level transfer . with transfer learning via sentence em ##bed ##ding ##s , we observe surprisingly good performance with minimal amounts of supervised training data for ##a transfer task . we obtain encouraging results on word em ##bed ##ding association tests ( we ##at ) targeted at detecting model bias . our pre - trained sentence encoding models are made freely available for download and on t ##f hub . otherwise , hyper ##para ##meter ##s are tuned by cross ##val ##ida ##tion on the task training data when available or the evaluation test data when neither training nor dev data are provided training repeats ten times for each transfer task model with different randomly initial ##ized weights and we report evaluation results by averaging across runs to assess bias in our encoding models , we evaluate the strength of various associations learned by our model on we ##at word lists table 1 : transfer task evaluation sets 1 , 82 ##1 dev test 1 , 37 ##9 ) . table 2 : model performance on transfer tasks . use t is the universal sentence en ##code ##r ( use ) using transform ##er . use d is the universal en ##code ##r dan model . models tagged with baseline ##s with no transfer learning sentence & word em ##bed ##ding transfer learning - sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning mr ss ##t sts bench mp ##qa tre ##c sub ##j cr table 3 : task performance on ss ##t for varying amounts of training data . ss ##t 67 . 3 ##k represents the full training set . using only 1 , 000 examples for training , transfer learning from use t is able to obtain performance that rivals many of the other models trained on the full 67 . 3 thousand example training set . baseline ##s with no transfer learning sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning sentence & word em ##bed ##ding transfer learning table 4 : word em ##bed ##ding association tests ( we ##at ) for glove [SEP]
I0622 09:43:48.000646 140630242117440 run_classifier_sci.py:541] tokens: [CLS] word sense di ##sam ##bi ##gua ##tion ; se ##me ##val 2013 ; f1 [SEP] universal sentence en ##code ##r we present models for encoding sentences into em ##bed ##ding vectors that specifically target transfer learning to other nl ##p tasks . the models are efficient and result in accurate performance on diverse transfer tasks . two variants of the encoding models allow for trade - offs between accuracy and compute resources . for both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance . comparisons are made with base - lines that use word level transfer learning via pre ##train ##ed word em ##bed ##ding ##s as well as baseline ##s do not use any transfer learning . we find that transfer learning using sentence em ##bed ##ding ##s tends to out ##per ##form word level transfer . with transfer learning via sentence em ##bed ##ding ##s , we observe surprisingly good performance with minimal amounts of supervised training data for ##a transfer task . we obtain encouraging results on word em ##bed ##ding association tests ( we ##at ) targeted at detecting model bias . our pre - trained sentence encoding models are made freely available for download and on t ##f hub . otherwise , hyper ##para ##meter ##s are tuned by cross ##val ##ida ##tion on the task training data when available or the evaluation test data when neither training nor dev data are provided training repeats ten times for each transfer task model with different randomly initial ##ized weights and we report evaluation results by averaging across runs to assess bias in our encoding models , we evaluate the strength of various associations learned by our model on we ##at word lists table 1 : transfer task evaluation sets 1 , 82 ##1 dev test 1 , 37 ##9 ) . table 2 : model performance on transfer tasks . use t is the universal sentence en ##code ##r ( use ) using transform ##er . use d is the universal en ##code ##r dan model . models tagged with baseline ##s with no transfer learning sentence & word em ##bed ##ding transfer learning - sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning mr ss ##t sts bench mp ##qa tre ##c sub ##j cr table 3 : task performance on ss ##t for varying amounts of training data . ss ##t 67 . 3 ##k represents the full training set . using only 1 , 000 examples for training , transfer learning from use t is able to obtain performance that rivals many of the other models trained on the full 67 . 3 thousand example training set . baseline ##s with no transfer learning sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning sentence & word em ##bed ##ding transfer learning table 4 : word em ##bed ##ding association tests ( we ##at ) for glove [SEP]
INFO:tensorflow:input_ids: 101 2773 3168 4487 21559 5638 19696 3508 1025 7367 4168 10175 2286 1025 20069 102 5415 6251 4372 16044 2099 2057 2556 4275 2005 17181 11746 2046 7861 8270 4667 19019 2008 4919 4539 4651 4083 2000 2060 17953 2361 8518 1012 1996 4275 2024 8114 1998 2765 1999 8321 2836 2006 7578 4651 8518 1012 2048 10176 1997 1996 17181 4275 3499 2005 3119 1011 12446 2090 10640 1998 24134 4219 1012 2005 2119 10176 1010 2057 8556 1998 3189 1996 3276 2090 2944 11619 1010 7692 8381 1010 1996 11343 1997 4651 4708 2731 2951 1010 1998 4708 2836 1012 18539 2024 2081 2007 2918 1011 3210 2008 2224 2773 2504 4651 4083 3081 3653 23654 2098 2773 7861 8270 4667 2015 2004 2092 2004 26163 2015 2079 2025 2224 2151 4651 4083 1012 2057 2424 2008 4651 4083 2478 6251 7861 8270 4667 2015 12102 2000 2041 4842 14192 2773 2504 4651 1012 2007 4651 4083 3081 6251 7861 8270 4667 2015 1010 2057 11949 10889 2204 2836 2007 10124 8310 1997 13588 2731 2951 2005 2050 4651 4708 1012 2057 6855 11434 3463 2006 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 9416 2012 25952 2944 13827 1012 2256 3653 1011 4738 6251 17181 4275 2024 2081 10350 2800 2005 8816 1998 2006 1056 2546 9594 1012 4728 1010 23760 28689 22828 2015 2024 15757 2011 2892 10175 8524 3508 2006 1996 4708 2731 2951 2043 2800 2030 1996 9312 3231 2951 2043 4445 2731 4496 16475 2951 2024 3024 2731 17993 2702 2335 2005 2169 4651 4708 2944 2007 2367 18154 3988 3550 15871 1998 2057 3189 9312 3463 2011 14985 2408 3216 2000 14358 13827 1999 2256 17181 4275 1010 2057 16157 1996 3997 1997 2536 8924 4342 2011 2256 2944 2006 2057 4017 2773 7201 2795 1015 1024 4651 4708 9312 4520 1015 1010 6445 2487 16475 3231 1015 1010 4261 2683 1007 1012 2795 1016 1024 2944 2836 2006 4651 8518 1012 2224 1056 2003 1996 5415 6251 4372 16044 2099 1006 2224 1007 2478 10938 2121 1012 2224 1040 2003 1996 5415 4372 16044 2099 4907 2944 1012 4275 26610 2007 26163 2015 2007 2053 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 1011 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 2720 7020 2102 8541 6847 6131 19062 29461 2278 4942 3501 13675 2795 1017 1024 4708 2836 2006 7020 2102 2005 9671 8310 1997 2731 2951 1012 7020 2102 6163 1012 1017 2243 5836 1996 2440 2731 2275 1012 2478 2069 1015 1010 2199 4973 2005 2731 1010 4651 4083 2013 2224 1056 2003 2583 2000 6855 2836 2008 9169 2116 1997 1996 2060 4275 4738 2006 1996 2440 6163 1012 1017 4595 2742 2731 2275 1012 26163 2015 2007 2053 4651 4083 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 2795 1018 1024 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 2005 15913 102 0 0 0
I0622 09:43:48.000884 140630242117440 run_classifier_sci.py:542] input_ids: 101 2773 3168 4487 21559 5638 19696 3508 1025 7367 4168 10175 2286 1025 20069 102 5415 6251 4372 16044 2099 2057 2556 4275 2005 17181 11746 2046 7861 8270 4667 19019 2008 4919 4539 4651 4083 2000 2060 17953 2361 8518 1012 1996 4275 2024 8114 1998 2765 1999 8321 2836 2006 7578 4651 8518 1012 2048 10176 1997 1996 17181 4275 3499 2005 3119 1011 12446 2090 10640 1998 24134 4219 1012 2005 2119 10176 1010 2057 8556 1998 3189 1996 3276 2090 2944 11619 1010 7692 8381 1010 1996 11343 1997 4651 4708 2731 2951 1010 1998 4708 2836 1012 18539 2024 2081 2007 2918 1011 3210 2008 2224 2773 2504 4651 4083 3081 3653 23654 2098 2773 7861 8270 4667 2015 2004 2092 2004 26163 2015 2079 2025 2224 2151 4651 4083 1012 2057 2424 2008 4651 4083 2478 6251 7861 8270 4667 2015 12102 2000 2041 4842 14192 2773 2504 4651 1012 2007 4651 4083 3081 6251 7861 8270 4667 2015 1010 2057 11949 10889 2204 2836 2007 10124 8310 1997 13588 2731 2951 2005 2050 4651 4708 1012 2057 6855 11434 3463 2006 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 9416 2012 25952 2944 13827 1012 2256 3653 1011 4738 6251 17181 4275 2024 2081 10350 2800 2005 8816 1998 2006 1056 2546 9594 1012 4728 1010 23760 28689 22828 2015 2024 15757 2011 2892 10175 8524 3508 2006 1996 4708 2731 2951 2043 2800 2030 1996 9312 3231 2951 2043 4445 2731 4496 16475 2951 2024 3024 2731 17993 2702 2335 2005 2169 4651 4708 2944 2007 2367 18154 3988 3550 15871 1998 2057 3189 9312 3463 2011 14985 2408 3216 2000 14358 13827 1999 2256 17181 4275 1010 2057 16157 1996 3997 1997 2536 8924 4342 2011 2256 2944 2006 2057 4017 2773 7201 2795 1015 1024 4651 4708 9312 4520 1015 1010 6445 2487 16475 3231 1015 1010 4261 2683 1007 1012 2795 1016 1024 2944 2836 2006 4651 8518 1012 2224 1056 2003 1996 5415 6251 4372 16044 2099 1006 2224 1007 2478 10938 2121 1012 2224 1040 2003 1996 5415 4372 16044 2099 4907 2944 1012 4275 26610 2007 26163 2015 2007 2053 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 1011 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 2720 7020 2102 8541 6847 6131 19062 29461 2278 4942 3501 13675 2795 1017 1024 4708 2836 2006 7020 2102 2005 9671 8310 1997 2731 2951 1012 7020 2102 6163 1012 1017 2243 5836 1996 2440 2731 2275 1012 2478 2069 1015 1010 2199 4973 2005 2731 1010 4651 4083 2013 2224 1056 2003 2583 2000 6855 2836 2008 9169 2116 1997 1996 2060 4275 4738 2006 1996 2440 6163 1012 1017 4595 2742 2731 2275 1012 26163 2015 2007 2053 4651 4083 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 2795 1018 1024 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 2005 15913 102 0 0 0
INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0
I0622 09:43:48.001128 140630242117440 run_classifier_sci.py:543] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0
INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0
I0622 09:43:48.001322 140630242117440 run_classifier_sci.py:544] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0
INFO:tensorflow:label: true (id = 0)
I0622 09:43:48.001389 140630242117440 run_classifier_sci.py:545] label: true (id = 0)
INFO:tensorflow:Writing example 10000 of 13071
I0622 09:44:41.474362 140630242117440 run_classifier_sci.py:565] Writing example 10000 of 13071
INFO:tensorflow:***** Running prediction*****
I0622 09:44:58.126897 140630242117440 run_classifier_sci.py:1040] ***** Running prediction*****
INFO:tensorflow:  Num examples = 13071 (13071 actual, 0 padding)
I0622 09:44:58.127099 140630242117440 run_classifier_sci.py:1043]   Num examples = 13071 (13071 actual, 0 padding)
INFO:tensorflow:  Batch size = 6
I0622 09:44:58.127255 140630242117440 run_classifier_sci.py:1044]   Batch size = 6
INFO:tensorflow:***** Predict results *****
I0622 09:44:58.127448 140630242117440 run_classifier_sci.py:1059] ***** Predict results *****
WARNING:tensorflow:From /opt/conda/envs/python3/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
W0622 09:44:58.153101 140630242117440 deprecation.py:506] From /opt/conda/envs/python3/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From run_classifier_sci.py:628: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.experimental.map_and_batch(...)`.
W0622 09:44:58.179409 140630242117440 deprecation.py:323] From run_classifier_sci.py:628: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.experimental.map_and_batch(...)`.
WARNING:tensorflow:From /opt/conda/envs/python3/lib/python3.6/site-packages/tensorflow_core/contrib/data/python/ops/batching.py:276: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.
W0622 09:44:58.179585 140630242117440 deprecation.py:323] From /opt/conda/envs/python3/lib/python3.6/site-packages/tensorflow_core/contrib/data/python/ops/batching.py:276: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.
WARNING:tensorflow:From /opt/conda/envs/python3/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.

W0622 09:44:58.242268 140630242117440 module_wrapper.py:139] From /opt/conda/envs/python3/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.

WARNING:tensorflow:From run_classifier_sci.py:608: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
W0622 09:44:58.341008 140630242117440 deprecation.py:323] From run_classifier_sci.py:608: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
INFO:tensorflow:Calling model_fn.
I0622 09:44:58.355283 140630242117440 estimator.py:1148] Calling model_fn.
INFO:tensorflow:Running infer on CPU
I0622 09:44:58.355504 140630242117440 tpu_estimator.py:3124] Running infer on CPU
INFO:tensorflow:*** Features ***
I0622 09:44:58.355849 140630242117440 run_classifier_sci.py:708] *** Features ***
INFO:tensorflow:  name = input_ids, shape = (?, 512)
I0622 09:44:58.355992 140630242117440 run_classifier_sci.py:710]   name = input_ids, shape = (?, 512)
INFO:tensorflow:  name = input_mask, shape = (?, 512)
I0622 09:44:58.356137 140630242117440 run_classifier_sci.py:710]   name = input_mask, shape = (?, 512)
INFO:tensorflow:  name = is_real_example, shape = (?,)
I0622 09:44:58.356261 140630242117440 run_classifier_sci.py:710]   name = is_real_example, shape = (?,)
INFO:tensorflow:  name = label_ids, shape = (?,)
I0622 09:44:58.356377 140630242117440 run_classifier_sci.py:710]   name = label_ids, shape = (?,)
INFO:tensorflow:  name = segment_ids, shape = (?, 512)
I0622 09:44:58.356494 140630242117440 run_classifier_sci.py:710]   name = segment_ids, shape = (?, 512)
WARNING:tensorflow:From /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/BERT/modeling.py:172: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W0622 09:44:58.362360 140630242117440 module_wrapper.py:139] From /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/BERT/modeling.py:172: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/BERT/modeling.py:410: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

W0622 09:44:58.363715 140630242117440 module_wrapper.py:139] From /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/BERT/modeling.py:410: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/BERT/modeling.py:491: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.

W0622 09:44:58.388545 140630242117440 module_wrapper.py:139] From /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/BERT/modeling.py:491: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.

WARNING:tensorflow:From /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/BERT/modeling.py:672: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
W0622 09:44:58.432861 140630242117440 deprecation.py:323] From /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/BERT/modeling.py:672: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
WARNING:tensorflow:From /opt/conda/envs/python3/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
W0622 09:44:58.434067 140630242117440 deprecation.py:323] From /opt/conda/envs/python3/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From run_classifier_sci.py:728: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.

W0622 09:44:59.849704 140630242117440 module_wrapper.py:139] From run_classifier_sci.py:728: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.

WARNING:tensorflow:From run_classifier_sci.py:742: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.

W0622 09:44:59.856102 140630242117440 module_wrapper.py:139] From run_classifier_sci.py:742: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.

INFO:tensorflow:**** Trainable Variables ****
I0622 09:45:00.322351 140630242117440 run_classifier_sci.py:744] **** Trainable Variables ****
INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (30522, 768), *INIT_FROM_CKPT*
I0622 09:45:00.322533 140630242117440 run_classifier_sci.py:750]   name = bert/embeddings/word_embeddings:0, shape = (30522, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*
I0622 09:45:00.322705 140630242117440 run_classifier_sci.py:750]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*
I0622 09:45:00.322838 140630242117440 run_classifier_sci.py:750]   name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.322957 140630242117440 run_classifier_sci.py:750]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.323070 140630242117440 run_classifier_sci.py:750]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.323178 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.323291 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.323397 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.323508 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.323615 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.323724 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.323835 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.323949 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.324054 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.324158 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
I0622 09:45:00.324262 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
I0622 09:45:00.324375 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
I0622 09:45:00.324480 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.324588 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.324692 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.324794 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.324896 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.325004 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.325107 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.325214 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.325317 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.325425 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.325528 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.325635 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.325738 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.325841 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
I0622 09:45:00.325947 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
I0622 09:45:00.326056 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
I0622 09:45:00.326159 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.326267 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.326370 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.326472 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.326575 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.326682 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.326785 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.326893 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.326995 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.327102 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.327205 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.327313 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.327416 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.327521 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
I0622 09:45:00.327625 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
I0622 09:45:00.327732 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
I0622 09:45:00.327841 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.327965 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.328069 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.328172 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.328274 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.328383 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.328486 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.328594 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.328696 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.328804 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.328908 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.329015 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.329120 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.329227 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
I0622 09:45:00.329330 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
I0622 09:45:00.329439 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
I0622 09:45:00.329542 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.329648 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.329751 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.329853 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.329955 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.330063 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.330165 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.330272 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.330375 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.330482 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.330584 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.330692 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.330796 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.330900 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
I0622 09:45:00.331002 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
I0622 09:45:00.331110 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
I0622 09:45:00.331213 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.331321 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.331423 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.331525 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.331627 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.331734 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.331842 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.331953 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.332057 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.332165 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.332268 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.332378 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.332483 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.332586 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
I0622 09:45:00.332690 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
I0622 09:45:00.332797 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
I0622 09:45:00.332900 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.333007 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.333109 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.333216 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.333318 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.333425 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.333527 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.333635 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.333738 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.333845 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.333949 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.334058 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.334161 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.334263 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
I0622 09:45:00.334365 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
I0622 09:45:00.334472 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
I0622 09:45:00.334575 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.334683 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.334785 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.334887 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.334990 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.335097 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.335199 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.335305 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.335408 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.335516 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.335621 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.335730 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.335839 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.335944 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
I0622 09:45:00.336048 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
I0622 09:45:00.336155 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
I0622 09:45:00.336259 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.336366 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.336468 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.336570 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.336671 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.336779 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.336881 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.336988 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.337090 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.337200 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.337304 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.337411 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.337514 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.337616 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
I0622 09:45:00.337718 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
I0622 09:45:00.337825 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
I0622 09:45:00.337929 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.338035 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.338138 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.338240 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.338342 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.338450 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.338552 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.338659 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.338764 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.338874 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.338978 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.339085 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.339188 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.339291 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
I0622 09:45:00.339392 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
I0622 09:45:00.339500 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
I0622 09:45:00.339601 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.339708 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.339809 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.339920 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.340023 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.340132 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.340236 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.340346 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.340451 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.340559 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.340662 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.340742 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.340796 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.340843 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
I0622 09:45:00.340890 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
I0622 09:45:00.340941 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
I0622 09:45:00.340988 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.341039 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.341085 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.341131 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.341178 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.341227 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.341273 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.341322 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.341368 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.341417 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.341466 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.341515 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.341560 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.341606 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
I0622 09:45:00.341651 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
I0622 09:45:00.341700 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
I0622 09:45:00.341747 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.341796 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.341843 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.341889 140630242117440 run_classifier_sci.py:750]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0622 09:45:00.341934 140630242117440 run_classifier_sci.py:750]   name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0622 09:45:00.341984 140630242117440 run_classifier_sci.py:750]   name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = output_weights:0, shape = (2, 768)
I0622 09:45:00.342048 140630242117440 run_classifier_sci.py:750]   name = output_weights:0, shape = (2, 768)
INFO:tensorflow:  name = output_bias:0, shape = (2,)
I0622 09:45:00.342110 140630242117440 run_classifier_sci.py:750]   name = output_bias:0, shape = (2,)
INFO:tensorflow:Done calling model_fn.
I0622 09:45:00.342680 140630242117440 estimator.py:1150] Done calling model_fn.
WARNING:tensorflow:From /opt/conda/envs/python3/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0622 09:45:00.482192 140630242117440 deprecation.py:323] From /opt/conda/envs/python3/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
INFO:tensorflow:Graph was finalized.
I0622 09:45:00.850584 140630242117440 monitored_session.py:240] Graph was finalized.
2021-06-22 09:45:00.850985: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2021-06-22 09:45:00.892264: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2021-06-22 09:45:00.897084: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5650bb2740d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-06-22 09:45:00.897132: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-06-22 09:45:00.902952: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2021-06-22 09:45:02.165937: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5650bb2828e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-06-22 09:45:02.165965: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
2021-06-22 09:45:02.166677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:3d:00.0
2021-06-22 09:45:02.170758: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-22 09:45:02.210828: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-06-22 09:45:02.232988: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-06-22 09:45:02.244998: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-06-22 09:45:02.291209: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-06-22 09:45:02.320825: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-06-22 09:45:02.408480: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-06-22 09:45:02.409729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2021-06-22 09:45:02.412258: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-22 09:45:02.413445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-06-22 09:45:02.413463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2021-06-22 09:45:02.413470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2021-06-22 09:45:02.415984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10320 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:3d:00.0, compute capability: 7.5)
INFO:tensorflow:Restoring parameters from ../data/ibm/exp/few-shot-setup/NLP-TDMS/paperVersion//models/BERT/model.ckpt-6653
I0622 09:45:02.454019 140630242117440 saver.py:1284] Restoring parameters from ../data/ibm/exp/few-shot-setup/NLP-TDMS/paperVersion//models/BERT/model.ckpt-6653
INFO:tensorflow:Running local_init_op.
I0622 09:45:06.773982 140630242117440 session_manager.py:500] Running local_init_op.
INFO:tensorflow:Done running local_init_op.
I0622 09:45:06.834291 140630242117440 session_manager.py:502] Done running local_init_op.
2021-06-22 09:45:07.616877: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
INFO:tensorflow:prediction_loop marked as finished
I0622 09:48:50.104615 140630242117440 error_handling.py:101] prediction_loop marked as finished
INFO:tensorflow:prediction_loop marked as finished
I0622 09:48:50.105175 140630242117440 error_handling.py:101] prediction_loop marked as finished
