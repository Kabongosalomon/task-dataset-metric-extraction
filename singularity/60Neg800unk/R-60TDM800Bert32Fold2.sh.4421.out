Directory ../data/paperwithcode/60Neg800unk/twofoldwithunk/fold2/models/uncased_L-12_H-768_A-12_BERT/ Exist
WARNING:tensorflow:From /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/BERT/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

WARNING:tensorflow:From run_classifier_sci.py:1079: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.

WARNING:tensorflow:From run_classifier_sci.py:865: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

W0620 00:07:06.362665 140250234066752 module_wrapper.py:139] From run_classifier_sci.py:865: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.

WARNING:tensorflow:From run_classifier_sci.py:865: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

W0620 00:07:06.362870 140250234066752 module_wrapper.py:139] From run_classifier_sci.py:865: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.

WARNING:tensorflow:From /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/BERT/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.

W0620 00:07:06.363305 140250234066752 module_wrapper.py:139] From /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/BERT/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.

WARNING:tensorflow:From run_classifier_sci.py:890: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.

W0620 00:07:06.364921 140250234066752 module_wrapper.py:139] From run_classifier_sci.py:890: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.

WARNING:tensorflow:
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

W0620 00:07:06.446525 140250234066752 lazy_loader.py:50] 
The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
  * https://github.com/tensorflow/io (for I/O related ops)
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f8e105132f0>) includes params argument, but params are not passed to Estimator.
W0620 00:07:08.002594 140250234066752 estimator.py:1994] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f8e105132f0>) includes params argument, but params are not passed to Estimator.
INFO:tensorflow:Using config: {'_model_dir': '../data/paperwithcode/60Neg800unk/twofoldwithunk/fold2/models/uncased_L-12_H-768_A-12_BERT/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8e05839e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}
I0620 00:07:08.004523 140250234066752 estimator.py:212] Using config: {'_model_dir': '../data/paperwithcode/60Neg800unk/twofoldwithunk/fold2/models/uncased_L-12_H-768_A-12_BERT/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8e05839e10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}
INFO:tensorflow:_TPUContext: eval_on_tpu True
I0620 00:07:08.005242 140250234066752 tpu_context.py:220] _TPUContext: eval_on_tpu True
WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.
W0620 00:07:08.005763 140250234066752 tpu_context.py:222] eval_on_tpu ignored because use_tpu is False.
WARNING:tensorflow:From run_classifier_sci.py:561: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.

W0620 00:07:08.371294 140250234066752 module_wrapper.py:139] From run_classifier_sci.py:561: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.

WARNING:tensorflow:From run_classifier_sci.py:565: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.

W0620 00:07:08.372496 140250234066752 module_wrapper.py:139] From run_classifier_sci.py:565: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.

INFO:tensorflow:Writing example 0 of 13071
I0620 00:07:08.372670 140250234066752 run_classifier_sci.py:565] Writing example 0 of 13071
INFO:tensorflow:*** Example ***
I0620 00:07:08.380282 140250234066752 run_classifier_sci.py:538] *** Example ***
INFO:tensorflow:guid: test-0
I0620 00:07:08.380467 140250234066752 run_classifier_sci.py:539] guid: test-0
INFO:tensorflow:tokens: [CLS] sentiment analysis ; sub ##j ; accuracy [SEP] universal sentence en ##code ##r we present models for encoding sentences into em ##bed ##ding vectors that specifically target transfer learning to other nl ##p tasks . the models are efficient and result in accurate performance on diverse transfer tasks . two variants of the encoding models allow for trade - offs between accuracy and compute resources . for both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance . comparisons are made with base - lines that use word level transfer learning via pre ##train ##ed word em ##bed ##ding ##s as well as baseline ##s do not use any transfer learning . we find that transfer learning using sentence em ##bed ##ding ##s tends to out ##per ##form word level transfer . with transfer learning via sentence em ##bed ##ding ##s , we observe surprisingly good performance with minimal amounts of supervised training data for ##a transfer task . we obtain encouraging results on word em ##bed ##ding association tests ( we ##at ) targeted at detecting model bias . our pre - trained sentence encoding models are made freely available for download and on t ##f hub . otherwise , hyper ##para ##meter ##s are tuned by cross ##val ##ida ##tion on the task training data when available or the evaluation test data when neither training nor dev data are provided training repeats ten times for each transfer task model with different randomly initial ##ized weights and we report evaluation results by averaging across runs to assess bias in our encoding models , we evaluate the strength of various associations learned by our model on we ##at word lists table 1 : transfer task evaluation sets 1 , 82 ##1 dev test 1 , 37 ##9 ) . table 2 : model performance on transfer tasks . use t is the universal sentence en ##code ##r ( use ) using transform ##er . use d is the universal en ##code ##r dan model . models tagged with baseline ##s with no transfer learning sentence & word em ##bed ##ding transfer learning - sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning mr ss ##t sts bench mp ##qa tre ##c sub ##j cr table 3 : task performance on ss ##t for varying amounts of training data . ss ##t 67 . 3 ##k represents the full training set . using only 1 , 000 examples for training , transfer learning from use t is able to obtain performance that rivals many of the other models trained on the full 67 . 3 thousand example training set . baseline ##s with no transfer learning sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning sentence & word em ##bed ##ding transfer learning table 4 : word em ##bed ##ding association tests ( we ##at ) for glove [SEP]
I0620 00:07:08.380731 140250234066752 run_classifier_sci.py:541] tokens: [CLS] sentiment analysis ; sub ##j ; accuracy [SEP] universal sentence en ##code ##r we present models for encoding sentences into em ##bed ##ding vectors that specifically target transfer learning to other nl ##p tasks . the models are efficient and result in accurate performance on diverse transfer tasks . two variants of the encoding models allow for trade - offs between accuracy and compute resources . for both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance . comparisons are made with base - lines that use word level transfer learning via pre ##train ##ed word em ##bed ##ding ##s as well as baseline ##s do not use any transfer learning . we find that transfer learning using sentence em ##bed ##ding ##s tends to out ##per ##form word level transfer . with transfer learning via sentence em ##bed ##ding ##s , we observe surprisingly good performance with minimal amounts of supervised training data for ##a transfer task . we obtain encouraging results on word em ##bed ##ding association tests ( we ##at ) targeted at detecting model bias . our pre - trained sentence encoding models are made freely available for download and on t ##f hub . otherwise , hyper ##para ##meter ##s are tuned by cross ##val ##ida ##tion on the task training data when available or the evaluation test data when neither training nor dev data are provided training repeats ten times for each transfer task model with different randomly initial ##ized weights and we report evaluation results by averaging across runs to assess bias in our encoding models , we evaluate the strength of various associations learned by our model on we ##at word lists table 1 : transfer task evaluation sets 1 , 82 ##1 dev test 1 , 37 ##9 ) . table 2 : model performance on transfer tasks . use t is the universal sentence en ##code ##r ( use ) using transform ##er . use d is the universal en ##code ##r dan model . models tagged with baseline ##s with no transfer learning sentence & word em ##bed ##ding transfer learning - sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning mr ss ##t sts bench mp ##qa tre ##c sub ##j cr table 3 : task performance on ss ##t for varying amounts of training data . ss ##t 67 . 3 ##k represents the full training set . using only 1 , 000 examples for training , transfer learning from use t is able to obtain performance that rivals many of the other models trained on the full 67 . 3 thousand example training set . baseline ##s with no transfer learning sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning sentence & word em ##bed ##ding transfer learning table 4 : word em ##bed ##ding association tests ( we ##at ) for glove [SEP]
INFO:tensorflow:input_ids: 101 15792 4106 1025 4942 3501 1025 10640 102 5415 6251 4372 16044 2099 2057 2556 4275 2005 17181 11746 2046 7861 8270 4667 19019 2008 4919 4539 4651 4083 2000 2060 17953 2361 8518 1012 1996 4275 2024 8114 1998 2765 1999 8321 2836 2006 7578 4651 8518 1012 2048 10176 1997 1996 17181 4275 3499 2005 3119 1011 12446 2090 10640 1998 24134 4219 1012 2005 2119 10176 1010 2057 8556 1998 3189 1996 3276 2090 2944 11619 1010 7692 8381 1010 1996 11343 1997 4651 4708 2731 2951 1010 1998 4708 2836 1012 18539 2024 2081 2007 2918 1011 3210 2008 2224 2773 2504 4651 4083 3081 3653 23654 2098 2773 7861 8270 4667 2015 2004 2092 2004 26163 2015 2079 2025 2224 2151 4651 4083 1012 2057 2424 2008 4651 4083 2478 6251 7861 8270 4667 2015 12102 2000 2041 4842 14192 2773 2504 4651 1012 2007 4651 4083 3081 6251 7861 8270 4667 2015 1010 2057 11949 10889 2204 2836 2007 10124 8310 1997 13588 2731 2951 2005 2050 4651 4708 1012 2057 6855 11434 3463 2006 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 9416 2012 25952 2944 13827 1012 2256 3653 1011 4738 6251 17181 4275 2024 2081 10350 2800 2005 8816 1998 2006 1056 2546 9594 1012 4728 1010 23760 28689 22828 2015 2024 15757 2011 2892 10175 8524 3508 2006 1996 4708 2731 2951 2043 2800 2030 1996 9312 3231 2951 2043 4445 2731 4496 16475 2951 2024 3024 2731 17993 2702 2335 2005 2169 4651 4708 2944 2007 2367 18154 3988 3550 15871 1998 2057 3189 9312 3463 2011 14985 2408 3216 2000 14358 13827 1999 2256 17181 4275 1010 2057 16157 1996 3997 1997 2536 8924 4342 2011 2256 2944 2006 2057 4017 2773 7201 2795 1015 1024 4651 4708 9312 4520 1015 1010 6445 2487 16475 3231 1015 1010 4261 2683 1007 1012 2795 1016 1024 2944 2836 2006 4651 8518 1012 2224 1056 2003 1996 5415 6251 4372 16044 2099 1006 2224 1007 2478 10938 2121 1012 2224 1040 2003 1996 5415 4372 16044 2099 4907 2944 1012 4275 26610 2007 26163 2015 2007 2053 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 1011 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 2720 7020 2102 8541 6847 6131 19062 29461 2278 4942 3501 13675 2795 1017 1024 4708 2836 2006 7020 2102 2005 9671 8310 1997 2731 2951 1012 7020 2102 6163 1012 1017 2243 5836 1996 2440 2731 2275 1012 2478 2069 1015 1010 2199 4973 2005 2731 1010 4651 4083 2013 2224 1056 2003 2583 2000 6855 2836 2008 9169 2116 1997 1996 2060 4275 4738 2006 1996 2440 6163 1012 1017 4595 2742 2731 2275 1012 26163 2015 2007 2053 4651 4083 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 2795 1018 1024 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 2005 15913 102 0 0 0 0 0 0 0 0 0 0
I0620 00:07:08.381017 140250234066752 run_classifier_sci.py:542] input_ids: 101 15792 4106 1025 4942 3501 1025 10640 102 5415 6251 4372 16044 2099 2057 2556 4275 2005 17181 11746 2046 7861 8270 4667 19019 2008 4919 4539 4651 4083 2000 2060 17953 2361 8518 1012 1996 4275 2024 8114 1998 2765 1999 8321 2836 2006 7578 4651 8518 1012 2048 10176 1997 1996 17181 4275 3499 2005 3119 1011 12446 2090 10640 1998 24134 4219 1012 2005 2119 10176 1010 2057 8556 1998 3189 1996 3276 2090 2944 11619 1010 7692 8381 1010 1996 11343 1997 4651 4708 2731 2951 1010 1998 4708 2836 1012 18539 2024 2081 2007 2918 1011 3210 2008 2224 2773 2504 4651 4083 3081 3653 23654 2098 2773 7861 8270 4667 2015 2004 2092 2004 26163 2015 2079 2025 2224 2151 4651 4083 1012 2057 2424 2008 4651 4083 2478 6251 7861 8270 4667 2015 12102 2000 2041 4842 14192 2773 2504 4651 1012 2007 4651 4083 3081 6251 7861 8270 4667 2015 1010 2057 11949 10889 2204 2836 2007 10124 8310 1997 13588 2731 2951 2005 2050 4651 4708 1012 2057 6855 11434 3463 2006 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 9416 2012 25952 2944 13827 1012 2256 3653 1011 4738 6251 17181 4275 2024 2081 10350 2800 2005 8816 1998 2006 1056 2546 9594 1012 4728 1010 23760 28689 22828 2015 2024 15757 2011 2892 10175 8524 3508 2006 1996 4708 2731 2951 2043 2800 2030 1996 9312 3231 2951 2043 4445 2731 4496 16475 2951 2024 3024 2731 17993 2702 2335 2005 2169 4651 4708 2944 2007 2367 18154 3988 3550 15871 1998 2057 3189 9312 3463 2011 14985 2408 3216 2000 14358 13827 1999 2256 17181 4275 1010 2057 16157 1996 3997 1997 2536 8924 4342 2011 2256 2944 2006 2057 4017 2773 7201 2795 1015 1024 4651 4708 9312 4520 1015 1010 6445 2487 16475 3231 1015 1010 4261 2683 1007 1012 2795 1016 1024 2944 2836 2006 4651 8518 1012 2224 1056 2003 1996 5415 6251 4372 16044 2099 1006 2224 1007 2478 10938 2121 1012 2224 1040 2003 1996 5415 4372 16044 2099 4907 2944 1012 4275 26610 2007 26163 2015 2007 2053 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 1011 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 2720 7020 2102 8541 6847 6131 19062 29461 2278 4942 3501 13675 2795 1017 1024 4708 2836 2006 7020 2102 2005 9671 8310 1997 2731 2951 1012 7020 2102 6163 1012 1017 2243 5836 1996 2440 2731 2275 1012 2478 2069 1015 1010 2199 4973 2005 2731 1010 4651 4083 2013 2224 1056 2003 2583 2000 6855 2836 2008 9169 2116 1997 1996 2060 4275 4738 2006 1996 2440 6163 1012 1017 4595 2742 2731 2275 1012 26163 2015 2007 2053 4651 4083 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 2795 1018 1024 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 2005 15913 102 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0
I0620 00:07:08.381269 140250234066752 run_classifier_sci.py:543] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0
I0620 00:07:08.381443 140250234066752 run_classifier_sci.py:544] segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:label: true (id = 0)
I0620 00:07:08.381520 140250234066752 run_classifier_sci.py:545] label: true (id = 0)
INFO:tensorflow:*** Example ***
I0620 00:07:08.388461 140250234066752 run_classifier_sci.py:538] *** Example ***
INFO:tensorflow:guid: test-1
I0620 00:07:08.388572 140250234066752 run_classifier_sci.py:539] guid: test-1
INFO:tensorflow:tokens: [CLS] text classification ; tre ##c ; error [SEP] universal sentence en ##code ##r we present models for encoding sentences into em ##bed ##ding vectors that specifically target transfer learning to other nl ##p tasks . the models are efficient and result in accurate performance on diverse transfer tasks . two variants of the encoding models allow for trade - offs between accuracy and compute resources . for both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance . comparisons are made with base - lines that use word level transfer learning via pre ##train ##ed word em ##bed ##ding ##s as well as baseline ##s do not use any transfer learning . we find that transfer learning using sentence em ##bed ##ding ##s tends to out ##per ##form word level transfer . with transfer learning via sentence em ##bed ##ding ##s , we observe surprisingly good performance with minimal amounts of supervised training data for ##a transfer task . we obtain encouraging results on word em ##bed ##ding association tests ( we ##at ) targeted at detecting model bias . our pre - trained sentence encoding models are made freely available for download and on t ##f hub . otherwise , hyper ##para ##meter ##s are tuned by cross ##val ##ida ##tion on the task training data when available or the evaluation test data when neither training nor dev data are provided training repeats ten times for each transfer task model with different randomly initial ##ized weights and we report evaluation results by averaging across runs to assess bias in our encoding models , we evaluate the strength of various associations learned by our model on we ##at word lists table 1 : transfer task evaluation sets 1 , 82 ##1 dev test 1 , 37 ##9 ) . table 2 : model performance on transfer tasks . use t is the universal sentence en ##code ##r ( use ) using transform ##er . use d is the universal en ##code ##r dan model . models tagged with baseline ##s with no transfer learning sentence & word em ##bed ##ding transfer learning - sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning mr ss ##t sts bench mp ##qa tre ##c sub ##j cr table 3 : task performance on ss ##t for varying amounts of training data . ss ##t 67 . 3 ##k represents the full training set . using only 1 , 000 examples for training , transfer learning from use t is able to obtain performance that rivals many of the other models trained on the full 67 . 3 thousand example training set . baseline ##s with no transfer learning sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning sentence & word em ##bed ##ding transfer learning table 4 : word em ##bed ##ding association tests ( we ##at ) for glove [SEP]
I0620 00:07:08.388785 140250234066752 run_classifier_sci.py:541] tokens: [CLS] text classification ; tre ##c ; error [SEP] universal sentence en ##code ##r we present models for encoding sentences into em ##bed ##ding vectors that specifically target transfer learning to other nl ##p tasks . the models are efficient and result in accurate performance on diverse transfer tasks . two variants of the encoding models allow for trade - offs between accuracy and compute resources . for both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance . comparisons are made with base - lines that use word level transfer learning via pre ##train ##ed word em ##bed ##ding ##s as well as baseline ##s do not use any transfer learning . we find that transfer learning using sentence em ##bed ##ding ##s tends to out ##per ##form word level transfer . with transfer learning via sentence em ##bed ##ding ##s , we observe surprisingly good performance with minimal amounts of supervised training data for ##a transfer task . we obtain encouraging results on word em ##bed ##ding association tests ( we ##at ) targeted at detecting model bias . our pre - trained sentence encoding models are made freely available for download and on t ##f hub . otherwise , hyper ##para ##meter ##s are tuned by cross ##val ##ida ##tion on the task training data when available or the evaluation test data when neither training nor dev data are provided training repeats ten times for each transfer task model with different randomly initial ##ized weights and we report evaluation results by averaging across runs to assess bias in our encoding models , we evaluate the strength of various associations learned by our model on we ##at word lists table 1 : transfer task evaluation sets 1 , 82 ##1 dev test 1 , 37 ##9 ) . table 2 : model performance on transfer tasks . use t is the universal sentence en ##code ##r ( use ) using transform ##er . use d is the universal en ##code ##r dan model . models tagged with baseline ##s with no transfer learning sentence & word em ##bed ##ding transfer learning - sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning mr ss ##t sts bench mp ##qa tre ##c sub ##j cr table 3 : task performance on ss ##t for varying amounts of training data . ss ##t 67 . 3 ##k represents the full training set . using only 1 , 000 examples for training , transfer learning from use t is able to obtain performance that rivals many of the other models trained on the full 67 . 3 thousand example training set . baseline ##s with no transfer learning sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning sentence & word em ##bed ##ding transfer learning table 4 : word em ##bed ##ding association tests ( we ##at ) for glove [SEP]
INFO:tensorflow:input_ids: 101 3793 5579 1025 29461 2278 1025 7561 102 5415 6251 4372 16044 2099 2057 2556 4275 2005 17181 11746 2046 7861 8270 4667 19019 2008 4919 4539 4651 4083 2000 2060 17953 2361 8518 1012 1996 4275 2024 8114 1998 2765 1999 8321 2836 2006 7578 4651 8518 1012 2048 10176 1997 1996 17181 4275 3499 2005 3119 1011 12446 2090 10640 1998 24134 4219 1012 2005 2119 10176 1010 2057 8556 1998 3189 1996 3276 2090 2944 11619 1010 7692 8381 1010 1996 11343 1997 4651 4708 2731 2951 1010 1998 4708 2836 1012 18539 2024 2081 2007 2918 1011 3210 2008 2224 2773 2504 4651 4083 3081 3653 23654 2098 2773 7861 8270 4667 2015 2004 2092 2004 26163 2015 2079 2025 2224 2151 4651 4083 1012 2057 2424 2008 4651 4083 2478 6251 7861 8270 4667 2015 12102 2000 2041 4842 14192 2773 2504 4651 1012 2007 4651 4083 3081 6251 7861 8270 4667 2015 1010 2057 11949 10889 2204 2836 2007 10124 8310 1997 13588 2731 2951 2005 2050 4651 4708 1012 2057 6855 11434 3463 2006 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 9416 2012 25952 2944 13827 1012 2256 3653 1011 4738 6251 17181 4275 2024 2081 10350 2800 2005 8816 1998 2006 1056 2546 9594 1012 4728 1010 23760 28689 22828 2015 2024 15757 2011 2892 10175 8524 3508 2006 1996 4708 2731 2951 2043 2800 2030 1996 9312 3231 2951 2043 4445 2731 4496 16475 2951 2024 3024 2731 17993 2702 2335 2005 2169 4651 4708 2944 2007 2367 18154 3988 3550 15871 1998 2057 3189 9312 3463 2011 14985 2408 3216 2000 14358 13827 1999 2256 17181 4275 1010 2057 16157 1996 3997 1997 2536 8924 4342 2011 2256 2944 2006 2057 4017 2773 7201 2795 1015 1024 4651 4708 9312 4520 1015 1010 6445 2487 16475 3231 1015 1010 4261 2683 1007 1012 2795 1016 1024 2944 2836 2006 4651 8518 1012 2224 1056 2003 1996 5415 6251 4372 16044 2099 1006 2224 1007 2478 10938 2121 1012 2224 1040 2003 1996 5415 4372 16044 2099 4907 2944 1012 4275 26610 2007 26163 2015 2007 2053 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 1011 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 2720 7020 2102 8541 6847 6131 19062 29461 2278 4942 3501 13675 2795 1017 1024 4708 2836 2006 7020 2102 2005 9671 8310 1997 2731 2951 1012 7020 2102 6163 1012 1017 2243 5836 1996 2440 2731 2275 1012 2478 2069 1015 1010 2199 4973 2005 2731 1010 4651 4083 2013 2224 1056 2003 2583 2000 6855 2836 2008 9169 2116 1997 1996 2060 4275 4738 2006 1996 2440 6163 1012 1017 4595 2742 2731 2275 1012 26163 2015 2007 2053 4651 4083 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 2795 1018 1024 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 2005 15913 102 0 0 0 0 0 0 0 0 0 0
I0620 00:07:08.389020 140250234066752 run_classifier_sci.py:542] input_ids: 101 3793 5579 1025 29461 2278 1025 7561 102 5415 6251 4372 16044 2099 2057 2556 4275 2005 17181 11746 2046 7861 8270 4667 19019 2008 4919 4539 4651 4083 2000 2060 17953 2361 8518 1012 1996 4275 2024 8114 1998 2765 1999 8321 2836 2006 7578 4651 8518 1012 2048 10176 1997 1996 17181 4275 3499 2005 3119 1011 12446 2090 10640 1998 24134 4219 1012 2005 2119 10176 1010 2057 8556 1998 3189 1996 3276 2090 2944 11619 1010 7692 8381 1010 1996 11343 1997 4651 4708 2731 2951 1010 1998 4708 2836 1012 18539 2024 2081 2007 2918 1011 3210 2008 2224 2773 2504 4651 4083 3081 3653 23654 2098 2773 7861 8270 4667 2015 2004 2092 2004 26163 2015 2079 2025 2224 2151 4651 4083 1012 2057 2424 2008 4651 4083 2478 6251 7861 8270 4667 2015 12102 2000 2041 4842 14192 2773 2504 4651 1012 2007 4651 4083 3081 6251 7861 8270 4667 2015 1010 2057 11949 10889 2204 2836 2007 10124 8310 1997 13588 2731 2951 2005 2050 4651 4708 1012 2057 6855 11434 3463 2006 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 9416 2012 25952 2944 13827 1012 2256 3653 1011 4738 6251 17181 4275 2024 2081 10350 2800 2005 8816 1998 2006 1056 2546 9594 1012 4728 1010 23760 28689 22828 2015 2024 15757 2011 2892 10175 8524 3508 2006 1996 4708 2731 2951 2043 2800 2030 1996 9312 3231 2951 2043 4445 2731 4496 16475 2951 2024 3024 2731 17993 2702 2335 2005 2169 4651 4708 2944 2007 2367 18154 3988 3550 15871 1998 2057 3189 9312 3463 2011 14985 2408 3216 2000 14358 13827 1999 2256 17181 4275 1010 2057 16157 1996 3997 1997 2536 8924 4342 2011 2256 2944 2006 2057 4017 2773 7201 2795 1015 1024 4651 4708 9312 4520 1015 1010 6445 2487 16475 3231 1015 1010 4261 2683 1007 1012 2795 1016 1024 2944 2836 2006 4651 8518 1012 2224 1056 2003 1996 5415 6251 4372 16044 2099 1006 2224 1007 2478 10938 2121 1012 2224 1040 2003 1996 5415 4372 16044 2099 4907 2944 1012 4275 26610 2007 26163 2015 2007 2053 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 1011 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 2720 7020 2102 8541 6847 6131 19062 29461 2278 4942 3501 13675 2795 1017 1024 4708 2836 2006 7020 2102 2005 9671 8310 1997 2731 2951 1012 7020 2102 6163 1012 1017 2243 5836 1996 2440 2731 2275 1012 2478 2069 1015 1010 2199 4973 2005 2731 1010 4651 4083 2013 2224 1056 2003 2583 2000 6855 2836 2008 9169 2116 1997 1996 2060 4275 4738 2006 1996 2440 6163 1012 1017 4595 2742 2731 2275 1012 26163 2015 2007 2053 4651 4083 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 2795 1018 1024 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 2005 15913 102 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0
I0620 00:07:08.389233 140250234066752 run_classifier_sci.py:543] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0
I0620 00:07:08.389397 140250234066752 run_classifier_sci.py:544] segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:label: true (id = 0)
I0620 00:07:08.389462 140250234066752 run_classifier_sci.py:545] label: true (id = 0)
INFO:tensorflow:*** Example ***
I0620 00:07:08.396143 140250234066752 run_classifier_sci.py:538] *** Example ***
INFO:tensorflow:guid: test-2
I0620 00:07:08.396243 140250234066752 run_classifier_sci.py:539] guid: test-2
INFO:tensorflow:tokens: [CLS] question answering ; squad ; f1 [SEP] universal sentence en ##code ##r we present models for encoding sentences into em ##bed ##ding vectors that specifically target transfer learning to other nl ##p tasks . the models are efficient and result in accurate performance on diverse transfer tasks . two variants of the encoding models allow for trade - offs between accuracy and compute resources . for both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance . comparisons are made with base - lines that use word level transfer learning via pre ##train ##ed word em ##bed ##ding ##s as well as baseline ##s do not use any transfer learning . we find that transfer learning using sentence em ##bed ##ding ##s tends to out ##per ##form word level transfer . with transfer learning via sentence em ##bed ##ding ##s , we observe surprisingly good performance with minimal amounts of supervised training data for ##a transfer task . we obtain encouraging results on word em ##bed ##ding association tests ( we ##at ) targeted at detecting model bias . our pre - trained sentence encoding models are made freely available for download and on t ##f hub . otherwise , hyper ##para ##meter ##s are tuned by cross ##val ##ida ##tion on the task training data when available or the evaluation test data when neither training nor dev data are provided training repeats ten times for each transfer task model with different randomly initial ##ized weights and we report evaluation results by averaging across runs to assess bias in our encoding models , we evaluate the strength of various associations learned by our model on we ##at word lists table 1 : transfer task evaluation sets 1 , 82 ##1 dev test 1 , 37 ##9 ) . table 2 : model performance on transfer tasks . use t is the universal sentence en ##code ##r ( use ) using transform ##er . use d is the universal en ##code ##r dan model . models tagged with baseline ##s with no transfer learning sentence & word em ##bed ##ding transfer learning - sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning mr ss ##t sts bench mp ##qa tre ##c sub ##j cr table 3 : task performance on ss ##t for varying amounts of training data . ss ##t 67 . 3 ##k represents the full training set . using only 1 , 000 examples for training , transfer learning from use t is able to obtain performance that rivals many of the other models trained on the full 67 . 3 thousand example training set . baseline ##s with no transfer learning sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning sentence & word em ##bed ##ding transfer learning table 4 : word em ##bed ##ding association tests ( we ##at ) for glove [SEP]
I0620 00:07:08.396442 140250234066752 run_classifier_sci.py:541] tokens: [CLS] question answering ; squad ; f1 [SEP] universal sentence en ##code ##r we present models for encoding sentences into em ##bed ##ding vectors that specifically target transfer learning to other nl ##p tasks . the models are efficient and result in accurate performance on diverse transfer tasks . two variants of the encoding models allow for trade - offs between accuracy and compute resources . for both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance . comparisons are made with base - lines that use word level transfer learning via pre ##train ##ed word em ##bed ##ding ##s as well as baseline ##s do not use any transfer learning . we find that transfer learning using sentence em ##bed ##ding ##s tends to out ##per ##form word level transfer . with transfer learning via sentence em ##bed ##ding ##s , we observe surprisingly good performance with minimal amounts of supervised training data for ##a transfer task . we obtain encouraging results on word em ##bed ##ding association tests ( we ##at ) targeted at detecting model bias . our pre - trained sentence encoding models are made freely available for download and on t ##f hub . otherwise , hyper ##para ##meter ##s are tuned by cross ##val ##ida ##tion on the task training data when available or the evaluation test data when neither training nor dev data are provided training repeats ten times for each transfer task model with different randomly initial ##ized weights and we report evaluation results by averaging across runs to assess bias in our encoding models , we evaluate the strength of various associations learned by our model on we ##at word lists table 1 : transfer task evaluation sets 1 , 82 ##1 dev test 1 , 37 ##9 ) . table 2 : model performance on transfer tasks . use t is the universal sentence en ##code ##r ( use ) using transform ##er . use d is the universal en ##code ##r dan model . models tagged with baseline ##s with no transfer learning sentence & word em ##bed ##ding transfer learning - sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning mr ss ##t sts bench mp ##qa tre ##c sub ##j cr table 3 : task performance on ss ##t for varying amounts of training data . ss ##t 67 . 3 ##k represents the full training set . using only 1 , 000 examples for training , transfer learning from use t is able to obtain performance that rivals many of the other models trained on the full 67 . 3 thousand example training set . baseline ##s with no transfer learning sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning sentence & word em ##bed ##ding transfer learning table 4 : word em ##bed ##ding association tests ( we ##at ) for glove [SEP]
INFO:tensorflow:input_ids: 101 3160 10739 1025 4686 1025 20069 102 5415 6251 4372 16044 2099 2057 2556 4275 2005 17181 11746 2046 7861 8270 4667 19019 2008 4919 4539 4651 4083 2000 2060 17953 2361 8518 1012 1996 4275 2024 8114 1998 2765 1999 8321 2836 2006 7578 4651 8518 1012 2048 10176 1997 1996 17181 4275 3499 2005 3119 1011 12446 2090 10640 1998 24134 4219 1012 2005 2119 10176 1010 2057 8556 1998 3189 1996 3276 2090 2944 11619 1010 7692 8381 1010 1996 11343 1997 4651 4708 2731 2951 1010 1998 4708 2836 1012 18539 2024 2081 2007 2918 1011 3210 2008 2224 2773 2504 4651 4083 3081 3653 23654 2098 2773 7861 8270 4667 2015 2004 2092 2004 26163 2015 2079 2025 2224 2151 4651 4083 1012 2057 2424 2008 4651 4083 2478 6251 7861 8270 4667 2015 12102 2000 2041 4842 14192 2773 2504 4651 1012 2007 4651 4083 3081 6251 7861 8270 4667 2015 1010 2057 11949 10889 2204 2836 2007 10124 8310 1997 13588 2731 2951 2005 2050 4651 4708 1012 2057 6855 11434 3463 2006 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 9416 2012 25952 2944 13827 1012 2256 3653 1011 4738 6251 17181 4275 2024 2081 10350 2800 2005 8816 1998 2006 1056 2546 9594 1012 4728 1010 23760 28689 22828 2015 2024 15757 2011 2892 10175 8524 3508 2006 1996 4708 2731 2951 2043 2800 2030 1996 9312 3231 2951 2043 4445 2731 4496 16475 2951 2024 3024 2731 17993 2702 2335 2005 2169 4651 4708 2944 2007 2367 18154 3988 3550 15871 1998 2057 3189 9312 3463 2011 14985 2408 3216 2000 14358 13827 1999 2256 17181 4275 1010 2057 16157 1996 3997 1997 2536 8924 4342 2011 2256 2944 2006 2057 4017 2773 7201 2795 1015 1024 4651 4708 9312 4520 1015 1010 6445 2487 16475 3231 1015 1010 4261 2683 1007 1012 2795 1016 1024 2944 2836 2006 4651 8518 1012 2224 1056 2003 1996 5415 6251 4372 16044 2099 1006 2224 1007 2478 10938 2121 1012 2224 1040 2003 1996 5415 4372 16044 2099 4907 2944 1012 4275 26610 2007 26163 2015 2007 2053 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 1011 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 2720 7020 2102 8541 6847 6131 19062 29461 2278 4942 3501 13675 2795 1017 1024 4708 2836 2006 7020 2102 2005 9671 8310 1997 2731 2951 1012 7020 2102 6163 1012 1017 2243 5836 1996 2440 2731 2275 1012 2478 2069 1015 1010 2199 4973 2005 2731 1010 4651 4083 2013 2224 1056 2003 2583 2000 6855 2836 2008 9169 2116 1997 1996 2060 4275 4738 2006 1996 2440 6163 1012 1017 4595 2742 2731 2275 1012 26163 2015 2007 2053 4651 4083 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 2795 1018 1024 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 2005 15913 102 0 0 0 0 0 0 0 0 0 0 0
I0620 00:07:08.396660 140250234066752 run_classifier_sci.py:542] input_ids: 101 3160 10739 1025 4686 1025 20069 102 5415 6251 4372 16044 2099 2057 2556 4275 2005 17181 11746 2046 7861 8270 4667 19019 2008 4919 4539 4651 4083 2000 2060 17953 2361 8518 1012 1996 4275 2024 8114 1998 2765 1999 8321 2836 2006 7578 4651 8518 1012 2048 10176 1997 1996 17181 4275 3499 2005 3119 1011 12446 2090 10640 1998 24134 4219 1012 2005 2119 10176 1010 2057 8556 1998 3189 1996 3276 2090 2944 11619 1010 7692 8381 1010 1996 11343 1997 4651 4708 2731 2951 1010 1998 4708 2836 1012 18539 2024 2081 2007 2918 1011 3210 2008 2224 2773 2504 4651 4083 3081 3653 23654 2098 2773 7861 8270 4667 2015 2004 2092 2004 26163 2015 2079 2025 2224 2151 4651 4083 1012 2057 2424 2008 4651 4083 2478 6251 7861 8270 4667 2015 12102 2000 2041 4842 14192 2773 2504 4651 1012 2007 4651 4083 3081 6251 7861 8270 4667 2015 1010 2057 11949 10889 2204 2836 2007 10124 8310 1997 13588 2731 2951 2005 2050 4651 4708 1012 2057 6855 11434 3463 2006 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 9416 2012 25952 2944 13827 1012 2256 3653 1011 4738 6251 17181 4275 2024 2081 10350 2800 2005 8816 1998 2006 1056 2546 9594 1012 4728 1010 23760 28689 22828 2015 2024 15757 2011 2892 10175 8524 3508 2006 1996 4708 2731 2951 2043 2800 2030 1996 9312 3231 2951 2043 4445 2731 4496 16475 2951 2024 3024 2731 17993 2702 2335 2005 2169 4651 4708 2944 2007 2367 18154 3988 3550 15871 1998 2057 3189 9312 3463 2011 14985 2408 3216 2000 14358 13827 1999 2256 17181 4275 1010 2057 16157 1996 3997 1997 2536 8924 4342 2011 2256 2944 2006 2057 4017 2773 7201 2795 1015 1024 4651 4708 9312 4520 1015 1010 6445 2487 16475 3231 1015 1010 4261 2683 1007 1012 2795 1016 1024 2944 2836 2006 4651 8518 1012 2224 1056 2003 1996 5415 6251 4372 16044 2099 1006 2224 1007 2478 10938 2121 1012 2224 1040 2003 1996 5415 4372 16044 2099 4907 2944 1012 4275 26610 2007 26163 2015 2007 2053 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 1011 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 2720 7020 2102 8541 6847 6131 19062 29461 2278 4942 3501 13675 2795 1017 1024 4708 2836 2006 7020 2102 2005 9671 8310 1997 2731 2951 1012 7020 2102 6163 1012 1017 2243 5836 1996 2440 2731 2275 1012 2478 2069 1015 1010 2199 4973 2005 2731 1010 4651 4083 2013 2224 1056 2003 2583 2000 6855 2836 2008 9169 2116 1997 1996 2060 4275 4738 2006 1996 2440 6163 1012 1017 4595 2742 2731 2275 1012 26163 2015 2007 2053 4651 4083 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 2795 1018 1024 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 2005 15913 102 0 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0
I0620 00:07:08.396877 140250234066752 run_classifier_sci.py:543] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0
I0620 00:07:08.397241 140250234066752 run_classifier_sci.py:544] segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0
INFO:tensorflow:label: true (id = 0)
I0620 00:07:08.397389 140250234066752 run_classifier_sci.py:545] label: true (id = 0)
INFO:tensorflow:*** Example ***
I0620 00:07:08.404088 140250234066752 run_classifier_sci.py:538] *** Example ***
INFO:tensorflow:guid: test-3
I0620 00:07:08.404234 140250234066752 run_classifier_sci.py:539] guid: test-3
INFO:tensorflow:tokens: [CLS] relation prediction ; f ##b ##15 ##k - 237 ; h @ 1 [SEP] universal sentence en ##code ##r we present models for encoding sentences into em ##bed ##ding vectors that specifically target transfer learning to other nl ##p tasks . the models are efficient and result in accurate performance on diverse transfer tasks . two variants of the encoding models allow for trade - offs between accuracy and compute resources . for both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance . comparisons are made with base - lines that use word level transfer learning via pre ##train ##ed word em ##bed ##ding ##s as well as baseline ##s do not use any transfer learning . we find that transfer learning using sentence em ##bed ##ding ##s tends to out ##per ##form word level transfer . with transfer learning via sentence em ##bed ##ding ##s , we observe surprisingly good performance with minimal amounts of supervised training data for ##a transfer task . we obtain encouraging results on word em ##bed ##ding association tests ( we ##at ) targeted at detecting model bias . our pre - trained sentence encoding models are made freely available for download and on t ##f hub . otherwise , hyper ##para ##meter ##s are tuned by cross ##val ##ida ##tion on the task training data when available or the evaluation test data when neither training nor dev data are provided training repeats ten times for each transfer task model with different randomly initial ##ized weights and we report evaluation results by averaging across runs to assess bias in our encoding models , we evaluate the strength of various associations learned by our model on we ##at word lists table 1 : transfer task evaluation sets 1 , 82 ##1 dev test 1 , 37 ##9 ) . table 2 : model performance on transfer tasks . use t is the universal sentence en ##code ##r ( use ) using transform ##er . use d is the universal en ##code ##r dan model . models tagged with baseline ##s with no transfer learning sentence & word em ##bed ##ding transfer learning - sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning mr ss ##t sts bench mp ##qa tre ##c sub ##j cr table 3 : task performance on ss ##t for varying amounts of training data . ss ##t 67 . 3 ##k represents the full training set . using only 1 , 000 examples for training , transfer learning from use t is able to obtain performance that rivals many of the other models trained on the full 67 . 3 thousand example training set . baseline ##s with no transfer learning sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning sentence & word em ##bed ##ding transfer learning table 4 : word em ##bed ##ding association tests ( we ##at ) for glove [SEP]
I0620 00:07:08.404481 140250234066752 run_classifier_sci.py:541] tokens: [CLS] relation prediction ; f ##b ##15 ##k - 237 ; h @ 1 [SEP] universal sentence en ##code ##r we present models for encoding sentences into em ##bed ##ding vectors that specifically target transfer learning to other nl ##p tasks . the models are efficient and result in accurate performance on diverse transfer tasks . two variants of the encoding models allow for trade - offs between accuracy and compute resources . for both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance . comparisons are made with base - lines that use word level transfer learning via pre ##train ##ed word em ##bed ##ding ##s as well as baseline ##s do not use any transfer learning . we find that transfer learning using sentence em ##bed ##ding ##s tends to out ##per ##form word level transfer . with transfer learning via sentence em ##bed ##ding ##s , we observe surprisingly good performance with minimal amounts of supervised training data for ##a transfer task . we obtain encouraging results on word em ##bed ##ding association tests ( we ##at ) targeted at detecting model bias . our pre - trained sentence encoding models are made freely available for download and on t ##f hub . otherwise , hyper ##para ##meter ##s are tuned by cross ##val ##ida ##tion on the task training data when available or the evaluation test data when neither training nor dev data are provided training repeats ten times for each transfer task model with different randomly initial ##ized weights and we report evaluation results by averaging across runs to assess bias in our encoding models , we evaluate the strength of various associations learned by our model on we ##at word lists table 1 : transfer task evaluation sets 1 , 82 ##1 dev test 1 , 37 ##9 ) . table 2 : model performance on transfer tasks . use t is the universal sentence en ##code ##r ( use ) using transform ##er . use d is the universal en ##code ##r dan model . models tagged with baseline ##s with no transfer learning sentence & word em ##bed ##ding transfer learning - sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning mr ss ##t sts bench mp ##qa tre ##c sub ##j cr table 3 : task performance on ss ##t for varying amounts of training data . ss ##t 67 . 3 ##k represents the full training set . using only 1 , 000 examples for training , transfer learning from use t is able to obtain performance that rivals many of the other models trained on the full 67 . 3 thousand example training set . baseline ##s with no transfer learning sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning sentence & word em ##bed ##ding transfer learning table 4 : word em ##bed ##ding association tests ( we ##at ) for glove [SEP]
INFO:tensorflow:input_ids: 101 7189 17547 1025 1042 2497 16068 2243 1011 23297 1025 1044 1030 1015 102 5415 6251 4372 16044 2099 2057 2556 4275 2005 17181 11746 2046 7861 8270 4667 19019 2008 4919 4539 4651 4083 2000 2060 17953 2361 8518 1012 1996 4275 2024 8114 1998 2765 1999 8321 2836 2006 7578 4651 8518 1012 2048 10176 1997 1996 17181 4275 3499 2005 3119 1011 12446 2090 10640 1998 24134 4219 1012 2005 2119 10176 1010 2057 8556 1998 3189 1996 3276 2090 2944 11619 1010 7692 8381 1010 1996 11343 1997 4651 4708 2731 2951 1010 1998 4708 2836 1012 18539 2024 2081 2007 2918 1011 3210 2008 2224 2773 2504 4651 4083 3081 3653 23654 2098 2773 7861 8270 4667 2015 2004 2092 2004 26163 2015 2079 2025 2224 2151 4651 4083 1012 2057 2424 2008 4651 4083 2478 6251 7861 8270 4667 2015 12102 2000 2041 4842 14192 2773 2504 4651 1012 2007 4651 4083 3081 6251 7861 8270 4667 2015 1010 2057 11949 10889 2204 2836 2007 10124 8310 1997 13588 2731 2951 2005 2050 4651 4708 1012 2057 6855 11434 3463 2006 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 9416 2012 25952 2944 13827 1012 2256 3653 1011 4738 6251 17181 4275 2024 2081 10350 2800 2005 8816 1998 2006 1056 2546 9594 1012 4728 1010 23760 28689 22828 2015 2024 15757 2011 2892 10175 8524 3508 2006 1996 4708 2731 2951 2043 2800 2030 1996 9312 3231 2951 2043 4445 2731 4496 16475 2951 2024 3024 2731 17993 2702 2335 2005 2169 4651 4708 2944 2007 2367 18154 3988 3550 15871 1998 2057 3189 9312 3463 2011 14985 2408 3216 2000 14358 13827 1999 2256 17181 4275 1010 2057 16157 1996 3997 1997 2536 8924 4342 2011 2256 2944 2006 2057 4017 2773 7201 2795 1015 1024 4651 4708 9312 4520 1015 1010 6445 2487 16475 3231 1015 1010 4261 2683 1007 1012 2795 1016 1024 2944 2836 2006 4651 8518 1012 2224 1056 2003 1996 5415 6251 4372 16044 2099 1006 2224 1007 2478 10938 2121 1012 2224 1040 2003 1996 5415 4372 16044 2099 4907 2944 1012 4275 26610 2007 26163 2015 2007 2053 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 1011 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 2720 7020 2102 8541 6847 6131 19062 29461 2278 4942 3501 13675 2795 1017 1024 4708 2836 2006 7020 2102 2005 9671 8310 1997 2731 2951 1012 7020 2102 6163 1012 1017 2243 5836 1996 2440 2731 2275 1012 2478 2069 1015 1010 2199 4973 2005 2731 1010 4651 4083 2013 2224 1056 2003 2583 2000 6855 2836 2008 9169 2116 1997 1996 2060 4275 4738 2006 1996 2440 6163 1012 1017 4595 2742 2731 2275 1012 26163 2015 2007 2053 4651 4083 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 2795 1018 1024 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 2005 15913 102 0 0 0 0
I0620 00:07:08.404725 140250234066752 run_classifier_sci.py:542] input_ids: 101 7189 17547 1025 1042 2497 16068 2243 1011 23297 1025 1044 1030 1015 102 5415 6251 4372 16044 2099 2057 2556 4275 2005 17181 11746 2046 7861 8270 4667 19019 2008 4919 4539 4651 4083 2000 2060 17953 2361 8518 1012 1996 4275 2024 8114 1998 2765 1999 8321 2836 2006 7578 4651 8518 1012 2048 10176 1997 1996 17181 4275 3499 2005 3119 1011 12446 2090 10640 1998 24134 4219 1012 2005 2119 10176 1010 2057 8556 1998 3189 1996 3276 2090 2944 11619 1010 7692 8381 1010 1996 11343 1997 4651 4708 2731 2951 1010 1998 4708 2836 1012 18539 2024 2081 2007 2918 1011 3210 2008 2224 2773 2504 4651 4083 3081 3653 23654 2098 2773 7861 8270 4667 2015 2004 2092 2004 26163 2015 2079 2025 2224 2151 4651 4083 1012 2057 2424 2008 4651 4083 2478 6251 7861 8270 4667 2015 12102 2000 2041 4842 14192 2773 2504 4651 1012 2007 4651 4083 3081 6251 7861 8270 4667 2015 1010 2057 11949 10889 2204 2836 2007 10124 8310 1997 13588 2731 2951 2005 2050 4651 4708 1012 2057 6855 11434 3463 2006 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 9416 2012 25952 2944 13827 1012 2256 3653 1011 4738 6251 17181 4275 2024 2081 10350 2800 2005 8816 1998 2006 1056 2546 9594 1012 4728 1010 23760 28689 22828 2015 2024 15757 2011 2892 10175 8524 3508 2006 1996 4708 2731 2951 2043 2800 2030 1996 9312 3231 2951 2043 4445 2731 4496 16475 2951 2024 3024 2731 17993 2702 2335 2005 2169 4651 4708 2944 2007 2367 18154 3988 3550 15871 1998 2057 3189 9312 3463 2011 14985 2408 3216 2000 14358 13827 1999 2256 17181 4275 1010 2057 16157 1996 3997 1997 2536 8924 4342 2011 2256 2944 2006 2057 4017 2773 7201 2795 1015 1024 4651 4708 9312 4520 1015 1010 6445 2487 16475 3231 1015 1010 4261 2683 1007 1012 2795 1016 1024 2944 2836 2006 4651 8518 1012 2224 1056 2003 1996 5415 6251 4372 16044 2099 1006 2224 1007 2478 10938 2121 1012 2224 1040 2003 1996 5415 4372 16044 2099 4907 2944 1012 4275 26610 2007 26163 2015 2007 2053 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 1011 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 2720 7020 2102 8541 6847 6131 19062 29461 2278 4942 3501 13675 2795 1017 1024 4708 2836 2006 7020 2102 2005 9671 8310 1997 2731 2951 1012 7020 2102 6163 1012 1017 2243 5836 1996 2440 2731 2275 1012 2478 2069 1015 1010 2199 4973 2005 2731 1010 4651 4083 2013 2224 1056 2003 2583 2000 6855 2836 2008 9169 2116 1997 1996 2060 4275 4738 2006 1996 2440 6163 1012 1017 4595 2742 2731 2275 1012 26163 2015 2007 2053 4651 4083 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 2795 1018 1024 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 2005 15913 102 0 0 0 0
INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0
I0620 00:07:08.405048 140250234066752 run_classifier_sci.py:543] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0
INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0
I0620 00:07:08.405241 140250234066752 run_classifier_sci.py:544] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0
INFO:tensorflow:label: true (id = 0)
I0620 00:07:08.405312 140250234066752 run_classifier_sci.py:545] label: true (id = 0)
INFO:tensorflow:*** Example ***
I0620 00:07:08.412264 140250234066752 run_classifier_sci.py:538] *** Example ***
INFO:tensorflow:guid: test-4
I0620 00:07:08.412429 140250234066752 run_classifier_sci.py:539] guid: test-4
INFO:tensorflow:tokens: [CLS] word sense di ##sam ##bi ##gua ##tion ; se ##me ##val 2013 ; f1 [SEP] universal sentence en ##code ##r we present models for encoding sentences into em ##bed ##ding vectors that specifically target transfer learning to other nl ##p tasks . the models are efficient and result in accurate performance on diverse transfer tasks . two variants of the encoding models allow for trade - offs between accuracy and compute resources . for both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance . comparisons are made with base - lines that use word level transfer learning via pre ##train ##ed word em ##bed ##ding ##s as well as baseline ##s do not use any transfer learning . we find that transfer learning using sentence em ##bed ##ding ##s tends to out ##per ##form word level transfer . with transfer learning via sentence em ##bed ##ding ##s , we observe surprisingly good performance with minimal amounts of supervised training data for ##a transfer task . we obtain encouraging results on word em ##bed ##ding association tests ( we ##at ) targeted at detecting model bias . our pre - trained sentence encoding models are made freely available for download and on t ##f hub . otherwise , hyper ##para ##meter ##s are tuned by cross ##val ##ida ##tion on the task training data when available or the evaluation test data when neither training nor dev data are provided training repeats ten times for each transfer task model with different randomly initial ##ized weights and we report evaluation results by averaging across runs to assess bias in our encoding models , we evaluate the strength of various associations learned by our model on we ##at word lists table 1 : transfer task evaluation sets 1 , 82 ##1 dev test 1 , 37 ##9 ) . table 2 : model performance on transfer tasks . use t is the universal sentence en ##code ##r ( use ) using transform ##er . use d is the universal en ##code ##r dan model . models tagged with baseline ##s with no transfer learning sentence & word em ##bed ##ding transfer learning - sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning mr ss ##t sts bench mp ##qa tre ##c sub ##j cr table 3 : task performance on ss ##t for varying amounts of training data . ss ##t 67 . 3 ##k represents the full training set . using only 1 , 000 examples for training , transfer learning from use t is able to obtain performance that rivals many of the other models trained on the full 67 . 3 thousand example training set . baseline ##s with no transfer learning sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning sentence & word em ##bed ##ding transfer learning table 4 : word em ##bed ##ding association tests ( we ##at ) for glove [SEP]
I0620 00:07:08.412707 140250234066752 run_classifier_sci.py:541] tokens: [CLS] word sense di ##sam ##bi ##gua ##tion ; se ##me ##val 2013 ; f1 [SEP] universal sentence en ##code ##r we present models for encoding sentences into em ##bed ##ding vectors that specifically target transfer learning to other nl ##p tasks . the models are efficient and result in accurate performance on diverse transfer tasks . two variants of the encoding models allow for trade - offs between accuracy and compute resources . for both variants , we investigate and report the relationship between model complexity , resource consumption , the availability of transfer task training data , and task performance . comparisons are made with base - lines that use word level transfer learning via pre ##train ##ed word em ##bed ##ding ##s as well as baseline ##s do not use any transfer learning . we find that transfer learning using sentence em ##bed ##ding ##s tends to out ##per ##form word level transfer . with transfer learning via sentence em ##bed ##ding ##s , we observe surprisingly good performance with minimal amounts of supervised training data for ##a transfer task . we obtain encouraging results on word em ##bed ##ding association tests ( we ##at ) targeted at detecting model bias . our pre - trained sentence encoding models are made freely available for download and on t ##f hub . otherwise , hyper ##para ##meter ##s are tuned by cross ##val ##ida ##tion on the task training data when available or the evaluation test data when neither training nor dev data are provided training repeats ten times for each transfer task model with different randomly initial ##ized weights and we report evaluation results by averaging across runs to assess bias in our encoding models , we evaluate the strength of various associations learned by our model on we ##at word lists table 1 : transfer task evaluation sets 1 , 82 ##1 dev test 1 , 37 ##9 ) . table 2 : model performance on transfer tasks . use t is the universal sentence en ##code ##r ( use ) using transform ##er . use d is the universal en ##code ##r dan model . models tagged with baseline ##s with no transfer learning sentence & word em ##bed ##ding transfer learning - sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning mr ss ##t sts bench mp ##qa tre ##c sub ##j cr table 3 : task performance on ss ##t for varying amounts of training data . ss ##t 67 . 3 ##k represents the full training set . using only 1 , 000 examples for training , transfer learning from use t is able to obtain performance that rivals many of the other models trained on the full 67 . 3 thousand example training set . baseline ##s with no transfer learning sentence em ##bed ##ding transfer learning word em ##bed ##ding transfer learning sentence & word em ##bed ##ding transfer learning table 4 : word em ##bed ##ding association tests ( we ##at ) for glove [SEP]
INFO:tensorflow:input_ids: 101 2773 3168 4487 21559 5638 19696 3508 1025 7367 4168 10175 2286 1025 20069 102 5415 6251 4372 16044 2099 2057 2556 4275 2005 17181 11746 2046 7861 8270 4667 19019 2008 4919 4539 4651 4083 2000 2060 17953 2361 8518 1012 1996 4275 2024 8114 1998 2765 1999 8321 2836 2006 7578 4651 8518 1012 2048 10176 1997 1996 17181 4275 3499 2005 3119 1011 12446 2090 10640 1998 24134 4219 1012 2005 2119 10176 1010 2057 8556 1998 3189 1996 3276 2090 2944 11619 1010 7692 8381 1010 1996 11343 1997 4651 4708 2731 2951 1010 1998 4708 2836 1012 18539 2024 2081 2007 2918 1011 3210 2008 2224 2773 2504 4651 4083 3081 3653 23654 2098 2773 7861 8270 4667 2015 2004 2092 2004 26163 2015 2079 2025 2224 2151 4651 4083 1012 2057 2424 2008 4651 4083 2478 6251 7861 8270 4667 2015 12102 2000 2041 4842 14192 2773 2504 4651 1012 2007 4651 4083 3081 6251 7861 8270 4667 2015 1010 2057 11949 10889 2204 2836 2007 10124 8310 1997 13588 2731 2951 2005 2050 4651 4708 1012 2057 6855 11434 3463 2006 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 9416 2012 25952 2944 13827 1012 2256 3653 1011 4738 6251 17181 4275 2024 2081 10350 2800 2005 8816 1998 2006 1056 2546 9594 1012 4728 1010 23760 28689 22828 2015 2024 15757 2011 2892 10175 8524 3508 2006 1996 4708 2731 2951 2043 2800 2030 1996 9312 3231 2951 2043 4445 2731 4496 16475 2951 2024 3024 2731 17993 2702 2335 2005 2169 4651 4708 2944 2007 2367 18154 3988 3550 15871 1998 2057 3189 9312 3463 2011 14985 2408 3216 2000 14358 13827 1999 2256 17181 4275 1010 2057 16157 1996 3997 1997 2536 8924 4342 2011 2256 2944 2006 2057 4017 2773 7201 2795 1015 1024 4651 4708 9312 4520 1015 1010 6445 2487 16475 3231 1015 1010 4261 2683 1007 1012 2795 1016 1024 2944 2836 2006 4651 8518 1012 2224 1056 2003 1996 5415 6251 4372 16044 2099 1006 2224 1007 2478 10938 2121 1012 2224 1040 2003 1996 5415 4372 16044 2099 4907 2944 1012 4275 26610 2007 26163 2015 2007 2053 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 1011 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 2720 7020 2102 8541 6847 6131 19062 29461 2278 4942 3501 13675 2795 1017 1024 4708 2836 2006 7020 2102 2005 9671 8310 1997 2731 2951 1012 7020 2102 6163 1012 1017 2243 5836 1996 2440 2731 2275 1012 2478 2069 1015 1010 2199 4973 2005 2731 1010 4651 4083 2013 2224 1056 2003 2583 2000 6855 2836 2008 9169 2116 1997 1996 2060 4275 4738 2006 1996 2440 6163 1012 1017 4595 2742 2731 2275 1012 26163 2015 2007 2053 4651 4083 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 2795 1018 1024 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 2005 15913 102 0 0 0
I0620 00:07:08.412981 140250234066752 run_classifier_sci.py:542] input_ids: 101 2773 3168 4487 21559 5638 19696 3508 1025 7367 4168 10175 2286 1025 20069 102 5415 6251 4372 16044 2099 2057 2556 4275 2005 17181 11746 2046 7861 8270 4667 19019 2008 4919 4539 4651 4083 2000 2060 17953 2361 8518 1012 1996 4275 2024 8114 1998 2765 1999 8321 2836 2006 7578 4651 8518 1012 2048 10176 1997 1996 17181 4275 3499 2005 3119 1011 12446 2090 10640 1998 24134 4219 1012 2005 2119 10176 1010 2057 8556 1998 3189 1996 3276 2090 2944 11619 1010 7692 8381 1010 1996 11343 1997 4651 4708 2731 2951 1010 1998 4708 2836 1012 18539 2024 2081 2007 2918 1011 3210 2008 2224 2773 2504 4651 4083 3081 3653 23654 2098 2773 7861 8270 4667 2015 2004 2092 2004 26163 2015 2079 2025 2224 2151 4651 4083 1012 2057 2424 2008 4651 4083 2478 6251 7861 8270 4667 2015 12102 2000 2041 4842 14192 2773 2504 4651 1012 2007 4651 4083 3081 6251 7861 8270 4667 2015 1010 2057 11949 10889 2204 2836 2007 10124 8310 1997 13588 2731 2951 2005 2050 4651 4708 1012 2057 6855 11434 3463 2006 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 9416 2012 25952 2944 13827 1012 2256 3653 1011 4738 6251 17181 4275 2024 2081 10350 2800 2005 8816 1998 2006 1056 2546 9594 1012 4728 1010 23760 28689 22828 2015 2024 15757 2011 2892 10175 8524 3508 2006 1996 4708 2731 2951 2043 2800 2030 1996 9312 3231 2951 2043 4445 2731 4496 16475 2951 2024 3024 2731 17993 2702 2335 2005 2169 4651 4708 2944 2007 2367 18154 3988 3550 15871 1998 2057 3189 9312 3463 2011 14985 2408 3216 2000 14358 13827 1999 2256 17181 4275 1010 2057 16157 1996 3997 1997 2536 8924 4342 2011 2256 2944 2006 2057 4017 2773 7201 2795 1015 1024 4651 4708 9312 4520 1015 1010 6445 2487 16475 3231 1015 1010 4261 2683 1007 1012 2795 1016 1024 2944 2836 2006 4651 8518 1012 2224 1056 2003 1996 5415 6251 4372 16044 2099 1006 2224 1007 2478 10938 2121 1012 2224 1040 2003 1996 5415 4372 16044 2099 4907 2944 1012 4275 26610 2007 26163 2015 2007 2053 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 1011 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 2720 7020 2102 8541 6847 6131 19062 29461 2278 4942 3501 13675 2795 1017 1024 4708 2836 2006 7020 2102 2005 9671 8310 1997 2731 2951 1012 7020 2102 6163 1012 1017 2243 5836 1996 2440 2731 2275 1012 2478 2069 1015 1010 2199 4973 2005 2731 1010 4651 4083 2013 2224 1056 2003 2583 2000 6855 2836 2008 9169 2116 1997 1996 2060 4275 4738 2006 1996 2440 6163 1012 1017 4595 2742 2731 2275 1012 26163 2015 2007 2053 4651 4083 6251 7861 8270 4667 4651 4083 2773 7861 8270 4667 4651 4083 6251 1004 2773 7861 8270 4667 4651 4083 2795 1018 1024 2773 7861 8270 4667 2523 5852 1006 2057 4017 1007 2005 15913 102 0 0 0
INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0
I0620 00:07:08.413334 140250234066752 run_classifier_sci.py:543] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0
INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0
I0620 00:07:08.413540 140250234066752 run_classifier_sci.py:544] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0
INFO:tensorflow:label: true (id = 0)
I0620 00:07:08.413613 140250234066752 run_classifier_sci.py:545] label: true (id = 0)
INFO:tensorflow:Writing example 10000 of 13071
I0620 00:08:04.180170 140250234066752 run_classifier_sci.py:565] Writing example 10000 of 13071
INFO:tensorflow:***** Running prediction*****
I0620 00:08:20.861666 140250234066752 run_classifier_sci.py:1040] ***** Running prediction*****
INFO:tensorflow:  Num examples = 13071 (13071 actual, 0 padding)
I0620 00:08:20.862021 140250234066752 run_classifier_sci.py:1043]   Num examples = 13071 (13071 actual, 0 padding)
INFO:tensorflow:  Batch size = 6
I0620 00:08:20.862240 140250234066752 run_classifier_sci.py:1044]   Batch size = 6
WARNING:tensorflow:From run_classifier_sci.py:592: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.

W0620 00:08:20.863243 140250234066752 module_wrapper.py:139] From run_classifier_sci.py:592: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.

INFO:tensorflow:***** Predict results *****
I0620 00:08:20.863574 140250234066752 run_classifier_sci.py:1059] ***** Predict results *****
WARNING:tensorflow:From /opt/conda/envs/python3/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
W0620 00:08:20.944119 140250234066752 deprecation.py:506] From /opt/conda/envs/python3/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From run_classifier_sci.py:628: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.experimental.map_and_batch(...)`.
W0620 00:08:20.970303 140250234066752 deprecation.py:323] From run_classifier_sci.py:628: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.experimental.map_and_batch(...)`.
WARNING:tensorflow:From /opt/conda/envs/python3/lib/python3.6/site-packages/tensorflow_core/contrib/data/python/ops/batching.py:276: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.
W0620 00:08:20.970516 140250234066752 deprecation.py:323] From /opt/conda/envs/python3/lib/python3.6/site-packages/tensorflow_core/contrib/data/python/ops/batching.py:276: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.
WARNING:tensorflow:From /opt/conda/envs/python3/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.

W0620 00:08:21.031409 140250234066752 module_wrapper.py:139] From /opt/conda/envs/python3/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.

WARNING:tensorflow:From run_classifier_sci.py:608: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
W0620 00:08:21.125184 140250234066752 deprecation.py:323] From run_classifier_sci.py:608: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
INFO:tensorflow:Calling model_fn.
I0620 00:08:21.139762 140250234066752 estimator.py:1148] Calling model_fn.
INFO:tensorflow:Running infer on CPU
I0620 00:08:21.139998 140250234066752 tpu_estimator.py:3124] Running infer on CPU
INFO:tensorflow:*** Features ***
I0620 00:08:21.140350 140250234066752 run_classifier_sci.py:708] *** Features ***
INFO:tensorflow:  name = input_ids, shape = (?, 512)
I0620 00:08:21.140520 140250234066752 run_classifier_sci.py:710]   name = input_ids, shape = (?, 512)
INFO:tensorflow:  name = input_mask, shape = (?, 512)
I0620 00:08:21.140665 140250234066752 run_classifier_sci.py:710]   name = input_mask, shape = (?, 512)
INFO:tensorflow:  name = is_real_example, shape = (?,)
I0620 00:08:21.140797 140250234066752 run_classifier_sci.py:710]   name = is_real_example, shape = (?,)
INFO:tensorflow:  name = label_ids, shape = (?,)
I0620 00:08:21.140921 140250234066752 run_classifier_sci.py:710]   name = label_ids, shape = (?,)
INFO:tensorflow:  name = segment_ids, shape = (?, 512)
I0620 00:08:21.141051 140250234066752 run_classifier_sci.py:710]   name = segment_ids, shape = (?, 512)
WARNING:tensorflow:From /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/BERT/modeling.py:172: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

W0620 00:08:21.147312 140250234066752 module_wrapper.py:139] From /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/BERT/modeling.py:172: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/BERT/modeling.py:410: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

W0620 00:08:21.148734 140250234066752 module_wrapper.py:139] From /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/BERT/modeling.py:410: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/BERT/modeling.py:491: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.

W0620 00:08:21.173003 140250234066752 module_wrapper.py:139] From /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/BERT/modeling.py:491: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.

WARNING:tensorflow:From /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/BERT/modeling.py:672: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
W0620 00:08:21.216951 140250234066752 deprecation.py:323] From /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/BERT/modeling.py:672: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.Dense instead.
WARNING:tensorflow:From /opt/conda/envs/python3/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
W0620 00:08:21.218242 140250234066752 deprecation.py:323] From /opt/conda/envs/python3/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From run_classifier_sci.py:728: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.

W0620 00:08:22.701711 140250234066752 module_wrapper.py:139] From run_classifier_sci.py:728: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.

WARNING:tensorflow:From run_classifier_sci.py:742: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.

W0620 00:08:22.709006 140250234066752 module_wrapper.py:139] From run_classifier_sci.py:742: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.

INFO:tensorflow:**** Trainable Variables ****
I0620 00:08:23.182518 140250234066752 run_classifier_sci.py:744] **** Trainable Variables ****
INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (30522, 768), *INIT_FROM_CKPT*
I0620 00:08:23.182717 140250234066752 run_classifier_sci.py:750]   name = bert/embeddings/word_embeddings:0, shape = (30522, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*
I0620 00:08:23.182896 140250234066752 run_classifier_sci.py:750]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*
I0620 00:08:23.183040 140250234066752 run_classifier_sci.py:750]   name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.183165 140250234066752 run_classifier_sci.py:750]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.183278 140250234066752 run_classifier_sci.py:750]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.183387 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.183502 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.183610 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.183721 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.183837 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.183978 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.184093 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.184212 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.184326 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.184437 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
I0620 00:08:23.184548 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
I0620 00:08:23.184665 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
I0620 00:08:23.184782 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.184900 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.185018 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.185122 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.185226 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.185337 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.185441 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.185551 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.185656 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.185766 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.185870 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.185981 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.186086 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.186189 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
I0620 00:08:23.186292 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
I0620 00:08:23.186405 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
I0620 00:08:23.186511 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.186620 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.186723 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.186827 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.186930 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.187041 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.187146 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.187255 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.187359 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.187468 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.187573 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.187682 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.187787 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.187901 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
I0620 00:08:23.188008 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
I0620 00:08:23.188122 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
I0620 00:08:23.188227 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.188336 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.188440 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.188544 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.188648 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.188758 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.188863 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.188972 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.189075 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.189186 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.189291 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.189399 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.189503 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.189608 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
I0620 00:08:23.189714 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
I0620 00:08:23.189822 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
I0620 00:08:23.189927 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.190036 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.190140 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.190244 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.190347 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.190456 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.190560 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.190670 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.190774 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.190883 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.190986 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.191094 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.191198 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.191304 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
I0620 00:08:23.191409 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
I0620 00:08:23.191517 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
I0620 00:08:23.191621 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.191730 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.191842 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.191977 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.192089 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.192205 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.192316 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.192432 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.192545 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.192661 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.192771 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.192887 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.193006 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.193111 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
I0620 00:08:23.193215 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
I0620 00:08:23.193325 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
I0620 00:08:23.193430 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.193543 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.193647 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.193750 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.193853 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.193962 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.194066 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.194175 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.194280 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.194400 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.194505 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.194616 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.194721 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.194847 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
I0620 00:08:23.194958 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
I0620 00:08:23.195075 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
I0620 00:08:23.195186 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.195303 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.195413 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.195524 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.195634 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.195769 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.195899 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.196026 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.196140 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.196262 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.196377 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.196502 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.196618 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.196732 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
I0620 00:08:23.196856 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
I0620 00:08:23.196972 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
I0620 00:08:23.197083 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.197199 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.197309 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.197420 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.197530 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.197646 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.197757 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.197890 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.198004 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.198122 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.198239 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.198359 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.198474 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.198588 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
I0620 00:08:23.198714 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
I0620 00:08:23.198830 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
I0620 00:08:23.198941 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.199062 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.199166 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.199269 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.199372 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.199479 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.199581 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.199690 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.199792 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.199934 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.200051 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.200170 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.200284 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.200396 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
I0620 00:08:23.200510 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
I0620 00:08:23.200629 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
I0620 00:08:23.200744 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.200834 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.200892 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.200966 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.201014 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.201066 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.201114 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.201164 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.201212 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.201263 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.201310 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.201361 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.201409 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.201456 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
I0620 00:08:23.201503 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
I0620 00:08:23.201553 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
I0620 00:08:23.201604 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.201654 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.201701 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.201748 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.201795 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.201845 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.201892 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.201942 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.201989 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.202039 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.202087 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.202137 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.202184 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.202231 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
I0620 00:08:23.202277 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
I0620 00:08:23.202330 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
I0620 00:08:23.202378 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.202428 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.202475 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.202522 140250234066752 run_classifier_sci.py:750]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
I0620 00:08:23.202569 140250234066752 run_classifier_sci.py:750]   name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*
INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
I0620 00:08:23.202618 140250234066752 run_classifier_sci.py:750]   name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*
INFO:tensorflow:  name = output_weights:0, shape = (2, 768)
I0620 00:08:23.202684 140250234066752 run_classifier_sci.py:750]   name = output_weights:0, shape = (2, 768)
INFO:tensorflow:  name = output_bias:0, shape = (2,)
I0620 00:08:23.202754 140250234066752 run_classifier_sci.py:750]   name = output_bias:0, shape = (2,)
INFO:tensorflow:Done calling model_fn.
I0620 00:08:23.203126 140250234066752 estimator.py:1150] Done calling model_fn.
WARNING:tensorflow:From /opt/conda/envs/python3/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
W0620 00:08:23.345185 140250234066752 deprecation.py:323] From /opt/conda/envs/python3/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
INFO:tensorflow:Graph was finalized.
I0620 00:08:23.605347 140250234066752 monitored_session.py:240] Graph was finalized.
2021-06-20 00:08:23.605753: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2021-06-20 00:08:23.644080: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2021-06-20 00:08:23.648725: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561ec97817e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-06-20 00:08:23.648760: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-06-20 00:08:23.654220: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2021-06-20 00:08:24.544251: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561ec7711f60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-06-20 00:08:24.544279: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
2021-06-20 00:08:24.545115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:3d:00.0
2021-06-20 00:08:24.549409: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-20 00:08:24.591568: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2021-06-20 00:08:24.614669: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2021-06-20 00:08:24.627285: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2021-06-20 00:08:24.675331: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2021-06-20 00:08:24.706138: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2021-06-20 00:08:24.798516: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-06-20 00:08:24.800601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2021-06-20 00:08:24.803205: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2021-06-20 00:08:24.804608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-06-20 00:08:24.804630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2021-06-20 00:08:24.804638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2021-06-20 00:08:24.807416: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10320 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:3d:00.0, compute capability: 7.5)
INFO:tensorflow:Restoring parameters from ../data/paperwithcode/60Neg800unk/twofoldwithunk/fold2/models/uncased_L-12_H-768_A-12_BERT/model.ckpt-13473
I0620 00:08:24.810283 140250234066752 saver.py:1284] Restoring parameters from ../data/paperwithcode/60Neg800unk/twofoldwithunk/fold2/models/uncased_L-12_H-768_A-12_BERT/model.ckpt-13473
INFO:tensorflow:Running local_init_op.
I0620 00:08:42.044302 140250234066752 session_manager.py:500] Running local_init_op.
INFO:tensorflow:Done running local_init_op.
I0620 00:08:42.107937 140250234066752 session_manager.py:502] Done running local_init_op.
2021-06-20 00:08:42.911646: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
INFO:tensorflow:prediction_loop marked as finished
I0620 00:12:25.971555 140250234066752 error_handling.py:101] prediction_loop marked as finished
INFO:tensorflow:prediction_loop marked as finished
I0620 00:12:25.972141 140250234066752 error_handling.py:101] prediction_loop marked as finished
