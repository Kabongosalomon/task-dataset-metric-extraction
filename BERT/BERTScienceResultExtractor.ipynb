{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XLNetScienceResultExtractor",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kabongosalomon/task-dataset-metric-extraction/blob/dataleakage/BERT/BERTScienceResultExtractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ak-LLmPucLm"
      },
      "source": [
        "## Run experiments based on textual entailment system\r\n",
        "\r\n",
        "We release the training/testing datasets for all experiments described in the paper. You can find them under the data/exp directory. The results reported in the paper are based on the datasets under the [data/exp/few-shot-setup/NLP-TDMS/paperVersion](data/exp/few-shot-setup/NLP-TDMS/paperVersion) directory. We later further clean the datasets (e.g., remove five pdf files from the testing datasets which appear in the training datasets with a different name) and the clean version is under the [data/exp/few-shot-setup/NLP-TDMS](data/exp/few-shot-setup/NLP-TDMS) folder. Below we illustrate how to run experiments on the NLP-TDSM dataset in the few-shot setup to extract TDM pairs. \r\n",
        "\r\n",
        "\r\n",
        "1) Fork and clone this repository.\r\n",
        "\r\n",
        "2) Download or clone [BERT](https://github.com/google-research/bert).\r\n",
        "\r\n",
        "3) Run this command `pip install -r requirements.txt` from `./bert_tdms/` folder. \r\n",
        "\r\n",
        "4) Copy [run_classifier_sci.py](./bert_tdms/run_classifier_sci.py) into the BERT directory.\r\n",
        "\r\n",
        "5) Download BERT embeddings.  We use the [base uncased models](https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip).\r\n",
        "\r\n",
        "6) If we use `BERT_DIR` to point to the directory with the embeddings and `DATA_DIR` to point to the [directory with our train and test data](./data/exp/few-shot-setup/NLP-TDMS/), we can run the textual entailment system with  [run_classifier_sci.py](./bert_tdms/run_classifier_sci.py). For example:\r\n",
        "\r\n",
        "```\r\n",
        "> DATA_DIR=../data/exp/few-shot-setup/NLP-TDMS/\r\n",
        "> BERT_DIR=./model/uncased_L-12_H-768_A-12\r\n",
        "> python run_classifier_sci.py --do_train=true --do_eval=false --do_predict=true --data_dir=${DATA_DIR} --task_name=sci --vocab_file=${BERT_DIR}/vocab.txt --bert_config_file=${BERT_DIR}/bert_config.json --init_checkpoint=${BERT_DIR}/bert_model.ckpt --output_dir=bert_tdms --max_seq_length=512 --train_batch_size=6 --predict_batch_size=6\r\n",
        "```\r\n",
        "\r\n",
        "5) [TEModelEvalOnNLPTDMS](nlpLeaderboard/src/main/java/com/ibm/sre/tdmsie/TEModelEvalOnNLPTDMS.java) provides methods to evaluate TDMS tuples extraction.\r\n",
        "\r\n",
        "6) [GenerateTestDataOnPDFPapers](nlpLeaderboard/src/main/java/com/ibm/sre/tdmsie/GenerateTestDataOnPDFPapers.java) provides methods to generate testing dataset for any PDF papers.\r\n",
        "\r\n",
        "\r\n",
        "### Read NLP-TDMS and ARC-PDN corpora ###\r\n",
        "\r\n",
        "1) Follow the instructions in the [README](data/NLP-TDMS/downloader/README.md) in [data/NLP-TDMS/downloader/](data/NLP-TDMS/downloader/) to download the entire collection of raw PDFs of the NLP-TDMS dataset.  The downloaded PDFs can be moved to [data/NLP-TDMS/pdfFile](./data/NLP-TDMS/pdfFile) (i.e., `mv *.pdf ../pdfFile/.`).\r\n",
        "\r\n",
        "2) For the ARC-PDN corpus, the original pdf files can be downloaded from the [ACL Anthology Reference Corpus (Version 20160301)](https://acl-arc.comp.nus.edu.sg/). We use papers from ACL(P)/EMNLP(D)/NAACL(N) between 2010 and 2015. After uncompressing the downloaded PDF files, put the PDF files into the corresponding directories under the /data/ARC-PDN/ folder, e.g., copy D10 to /data/ARC-PDN/D/D10.\r\n",
        "\r\n",
        "3) We release the parsed NLP-TDMS and ARC-PDN corpora. [NlpTDMSReader](nlpLeaderboard/src/main/java/com/ibm/sre/data/corpus/NlpTDMSReader.java) and [ArcPDNReader](nlpLeaderboard/src/main/java/com/ibm/sre/data/corpus/ArcPDNReader.java) in the corpus package illustrate how to read section and table contents from PDF files in these two corpora. \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtUt74kcBhQn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d9da521-9b8e-4eb2-e1dc-17403ee844a3"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk0HNXMQtlJV",
        "outputId": "8322b1ac-26da-4ee0-b39f-f9d3a487e1d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! git clone https://github.com/google-research/bert"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'bert'...\n",
            "remote: Enumerating objects: 340, done.\u001b[K\n",
            "remote: Total 340 (delta 0), reused 0 (delta 0), pack-reused 340\u001b[K\n",
            "Receiving objects: 100% (340/340), 315.49 KiB | 4.26 MiB/s, done.\n",
            "Resolving deltas: 100% (185/185), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeQnIECOt5yP",
        "outputId": "e786c722-9dab-4d92-d040-dda343dc652c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# !wget https://raw.githubusercontent.com/Kabongosalomon/task-dataset-metric-extraction/dataleakage/bert_tdms/requirements.txt\r\n",
        "!pip install tensorflow==1.11.0\r\n",
        "!pip install tensorflow-gpu==1.11.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.11.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/d5/38cd4543401708e64c9ee6afa664b936860f4630dd93a49ab863f9998cd2/tensorflow-1.11.0-cp36-cp36m-manylinux1_x86_64.whl (63.0MB)\n",
            "\u001b[K     |████████████████████████████████| 63.0MB 70kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0) (1.33.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0) (0.35.1)\n",
            "Collecting setuptools<=39.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/10/79282747f9169f21c053c562a0baa21815a8c7879be97abd930dbcf862e8/setuptools-39.1.0-py2.py3-none-any.whl (566kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 45.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0) (0.3.3)\n",
            "Collecting tensorboard<1.12.0,>=1.11.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/2f/4d788919b1feef04624d63ed6ea45a49d1d1c834199ec50716edb5d310f4/tensorboard-1.11.0-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 43.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0) (0.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0) (1.15.0)\n",
            "Collecting keras-applications>=1.0.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0) (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.11.0) (3.12.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow==1.11.0) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow==1.11.0) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.5->tensorflow==1.11.0) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.12.0,>=1.11.0->tensorflow==1.11.0) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.12.0,>=1.11.0->tensorflow==1.11.0) (3.4.0)\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement setuptools>=41.2, but you'll have setuptools 39.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-auth 1.17.2 has requirement setuptools>=40.3.0, but you'll have setuptools 39.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: setuptools, tensorboard, keras-applications, tensorflow\n",
            "  Found existing installation: setuptools 50.3.2\n",
            "    Uninstalling setuptools-50.3.2:\n",
            "      Successfully uninstalled setuptools-50.3.2\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed keras-applications-1.0.8 setuptools-39.1.0 tensorboard-1.11.0 tensorflow-1.11.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.11.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/52/01438b81806765936eee690709edc2a975472c4e9d8d465a01840869c691/tensorflow_gpu-1.11.0-cp36-cp36m-manylinux1_x86_64.whl (258.8MB)\n",
            "\u001b[K     |████████████████████████████████| 258.8MB 54kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0) (0.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0) (3.12.4)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0) (0.3.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0) (1.18.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0) (0.35.1)\n",
            "Requirement already satisfied: setuptools<=39.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0) (39.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0) (1.33.2)\n",
            "Requirement already satisfied: tensorboard<1.12.0,>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0) (1.11.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.11.0) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.5->tensorflow-gpu==1.11.0) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow-gpu==1.11.0) (3.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.12.0,>=1.11.0->tensorflow-gpu==1.11.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.12.0,>=1.11.0->tensorflow-gpu==1.11.0) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.12.0,>=1.11.0->tensorflow-gpu==1.11.0) (3.4.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2RDlFkpujXG"
      },
      "source": [
        "5) Download BERT embeddings.  We use the [base uncased models](https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJMH5FBSBzJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "343662b3-8ed6-4c35-ea48-3158e39c5519"
      },
      "source": [
        "# !wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-09 13:38:42--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.212.128, 172.217.214.128, 108.177.111.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.212.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M   153MB/s    in 2.5s    \n",
            "\n",
            "2020-12-09 13:38:45 (153 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RM1dC_rWB4eI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9434f26-9d64-4eed-ae9c-29b130d12271"
      },
      "source": [
        "# !unzip uncased_L-12_H-768_A-12.zip"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  uncased_L-12_H-768_A-12.zip\n",
            "   creating: uncased_L-12_H-768_A-12/\n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RFio_s7u7MS"
      },
      "source": [
        "6) If we use `BERT_DIR` to point to the directory with the embeddings and `DATA_DIR` to point to the [directory with our train and test data](./data/exp/few-shot-setup/NLP-TDMS/), we can run the textual entailment system with  [run_classifier_sci.py](./bert_tdms/run_classifier_sci.py). For example:\r\n",
        "\r\n",
        "```\r\n",
        "> DATA_DIR=../data/exp/few-shot-setup/NLP-TDMS/\r\n",
        "> BERT_DIR=./model/uncased_L-12_H-768_A-12\r\n",
        "> python run_classifier_sci.py --do_train=true --do_eval=false --do_predict=true --data_dir=${DATA_DIR} --task_name=sci --vocab_file=${BERT_DIR}/vocab.txt --bert_config_file=${BERT_DIR}/bert_config.json --init_checkpoint=${BERT_DIR}/bert_model.ckpt --output_dir=bert_tdms --max_seq_length=512 --train_batch_size=6 --predict_batch_size=6\r\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_ypqulNu2oJ"
      },
      "source": [
        "!DATA_DIR=../content/gdrive/MyDrive/colab-ssh/science-result-extractor/data/exp/few-shot-setup/NLP-TDMS/\r\n",
        "!BERT_DIR=../content/gdrive/MyDrive/colab-ssh/uncased_L-12_H-768_A-12"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rARWoKAIu2dS"
      },
      "source": [
        "# !cp -r uncased_L-12_H-768_A-12/ ../content/gdrive/MyDrive/colab-ssh/"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RnwUmr00ij4"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/Kabongosalomon/task-dataset-metric-extraction/dataleakage/bert_tdms/run_classifier_sci.py\r\n",
        "!cp run_classifier_sci.py bert/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bFEuYQOw-bS",
        "outputId": "08bb575d-07b3-4393-d144-cb06dc9456ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd bert/\r\n",
        "# %cd ../"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/bert\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUaehekTCC94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dfdf52c-79da-4afc-8912-a7cff67c45d2"
      },
      "source": [
        "!python run_classifier_sci.py --do_train=true --do_eval=false --do_predict=true --data_dir=${DATA_DIR} --task_name=sci --vocab_file=${BERT_DIR}/vocab.txt --bert_config_file=${BERT_DIR}/bert_config.json --init_checkpoint=${BERT_DIR}/bert_model.ckpt --output_dir=bert_tdms --max_seq_length=512 --train_batch_size=6 --predict_batch_size=6"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n",
            "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n",
            "    _pywrap_tensorflow_internal = swig_import_helper()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
            "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
            "  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\n",
            "    return load_dynamic(name, filename, file)\n",
            "  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\n",
            "    return _load(spec)\n",
            "ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"run_classifier_sci.py\", line 24, in <module>\n",
            "    import modeling\n",
            "  File \"/content/bert/modeling.py\", line 28, in <module>\n",
            "    import tensorflow as tf\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py\", line 22, in <module>\n",
            "    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/__init__.py\", line 49, in <module>\n",
            "    from tensorflow.python import pywrap_tensorflow\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 74, in <module>\n",
            "    raise ImportError(msg)\n",
            "ImportError: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n",
            "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n",
            "    _pywrap_tensorflow_internal = swig_import_helper()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
            "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
            "  File \"/usr/lib/python3.6/imp.py\", line 243, in load_module\n",
            "    return load_dynamic(name, filename, file)\n",
            "  File \"/usr/lib/python3.6/imp.py\", line 343, in load_dynamic\n",
            "    return _load(spec)\n",
            "ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory\n",
            "\n",
            "\n",
            "Failed to load the native TensorFlow runtime.\n",
            "\n",
            "See https://www.tensorflow.org/install/install_sources#common_installation_problems\n",
            "\n",
            "for some common reasons and solutions.  Include the entire stack trace\n",
            "above this error message when asking for help.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MbGIy-y0z1U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}